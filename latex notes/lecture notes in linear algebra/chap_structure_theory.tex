\chapter{Structure Theory of a Linear Transformation}

This chapter aims to find the canonical form of linear transformations. We reduce a linear space to its indecomposable subspaces, then show that these indecomposable subspaces are in fact cyclic. \par
To fully understand this procedure, we provide several extra viewpoints, including $\lambda$-matrix theory and module theory. We will show that module theory interprets all these results most adequately.
\section{$\lambda$-matrix}
When dealing with the question about if a matrix can be diagonalized or reduced to some other simple form, we often use the theory of $\lambda$-matrix, the concept of invariant factors, determinant factors, elementary factors,  minimal polynomials, characteristic polynomials, and so on. Remember, the theoretical method is very powerful in solving relevant questions. \underline{So do not compute complicated and}
\underline{unsturctural calculation! Use your structural observation!} So normally, an insightful observation can simplified the proof to a few sentences.
\subsection{Smith normal form}
Every $\lambda$-matrix can be reduced to a $\lambda$-matrix with only diagonal elements by elementary operations on $\lambda$-matrix. \par
The elementary operations of $\lambda$-matrix is essentially the same as that of ordinary matrix, but note that the elements of the matrix are polynomials, which forms a integral domain$F[\lambda]$, so the invertible elements of $F[\lambda]$ consist of $F\setminus\left\{0\right\}$. Therefore, to make the elementary $\lambda$-matrix with respect to an elementary operation invertible, we must restrict the second kind of elementary operations on $F\setminus\left\{0\right\}$ instead of $F[\lambda]$.


\begin{example}
Let $f(x)$ be relatively prime to $g(x)$ and $h(x)$, we have already know that 
\[\begin{pmatrix}f(x)g(x) & \\&h(x)\end{pmatrix}\sim \begin{pmatrix}
 g(x) & \\&f(x)h(x)\end{pmatrix}\] Now, as an exercise, lets find out $U(x)$ and $V(x)$ that make 
 \[U(x)\begin{pmatrix}f(x)g(x) & \\&h(x)\end{pmatrix}V(x)=\begin{pmatrix}
 g(x) & \\&f(x)h(x)\end{pmatrix}\]
That is, finding expicit operations that transform the first matrix to the second. Because $f$ is relatively prime to $h$ and $g$, $\exists u,v\text{ s.t. }fu+hv=1$,$\exists p,q\text{ s.t. }fp+gq=1$.
\begin{align*}
    \begin{pmatrix}f(x)g(x) & \\&h(x)\end{pmatrix}&\sim \begin{pmatrix}fg& hgv\\&h\end{pmatrix}\\
    &\sim \begin{pmatrix}fg& g\\&h\end{pmatrix}\\
    &\sim \begin{pmatrix}g& fg\\h&\end{pmatrix}\\
    &\sim \begin{pmatrix}g& \\h&-fh\end{pmatrix}\\
    &\sim \begin{pmatrix}g& \\qgh&-fh\end{pmatrix}\\
    &\sim \begin{pmatrix}g& \\&fh\end{pmatrix}\\
\end{align*}
\end{example}
\begin{remark}
This result can be used to compute the Smith normal form of a diagonal $\lambda$-matrix.
\end{remark}



\begin{theorem}[Calculation of Minimal Polynomials]
The last invariant factor of $\lambda I_{n\times n}-A$ is the minimal polynomial of $A$, $A\in M_n(F),\text{ where }F\text{ is a field}$ .
\end{theorem}


\subsection{Jordan normal form: alternative treatment}
Except for the $\lambda$-matrix treatment above, there is another observation that leads to a new way to compute the Jordan normal form, and we exhibit the treatment below.\par
The key observation: $\text{rank }J_n(0)^i=\max (n-i,0)$\par 
Given $A\in M_{n\times n}(F)$,We first calculate the eigenvalues of $\lambda$ of $A$. Then, for every $\lambda$, compute the rank of $(\lambda I-A)^i$ until it becomes a constant. Denote the rank correspond to $i$ by $r_i$, and let $r_0=n$.
Suppose $d_i=r_{i-1}-r_i$, then $d_i$ means that we have at least $d_i$ $J_i(\lambda)$ in the Jordan normal form, so we have $c_i=d_i-d_{i+1}$ $J_i(\lambda)$ Jordan blocks in the Jordan normal form.
\begin{example}
Define
\begin{align*}
    D: \mathbb{C}[x]_n&\longrightarrow \mathbb{C}[x]_n \\
    f&\longmapsto f'
\end{align*}
We choose a basis for $\mathbb{C}[x]_n$: $(1,x,\cdots,x^n)$. Then the matrix of $D$ is \[\begin{pmatrix}
 0 & 1 &  &  & \\
  &  & 2 &  & \\
  &  &  & \ddots & \\
  &  &  &  &n \\
  &  &  &  &0
\end{pmatrix}\]
\[\det (\lambda I-A)=0 \Longrightarrow \lambda=0\]
Due to $\rk D=n$, there is only one Jordan block, therefore the Jordan normal form of $D$ is immediately obtained.\par
Now let's find the transition matrix. The key observation is: there is only one cyclic subspace, so we only need to find a generator for this space, and take the set of elements generated by the generator to be the basis.
\[P=\begin{pmatrix}
 \frac{n!}{0!} &  &  &  & \\
  & \frac{n!}{1!} &  &  & \\
  &  & \frac{n!}{2!} &  & \\
  &  &  &\ddots  & \\
  &  &  &  &\frac{n!}{n!}
\end{pmatrix}\]
\end{example}
From the example we can see, for certain structures, we prefer to use the treatment above from the perspective of rank instead of computing with $\lambda$-matrix. Readers should choose the best treatment according to different situations.

\subsection{The Elementary factors of $f(A)$}
We assume $A\in M_{n\times n}(\mathbb{C})$.
\begin{theorem}
The elementary factors of $f(A)$ can be obtained by the following procedure:\par
Given an elementary factor of $A$ \[(\lambda-\lambda_0)^p\]
it corresponds to elementary factors of $f(A)$ through the following way:
\begin{align*}
    (\lambda-f(\lambda_0))^p &\text{,when } p\ge 1\text{ and }f'(\lambda_0)\ne 0\\
    \underbrace{(\lambda-f(\lambda_0))^{q+1}}_h \quad \underbrace{(\lambda-f(\lambda_0))^q}_{k-h}&\text{,when } p>1\text{ and }f'(\lambda_0)=\cdots=f^{(k-1)}(\lambda_0)=0,f^{(k)}(\lambda_0)\ne 0(k<p)\\
    &\text{ where }p=qk+h(q\ge0,k>h\ge0)\\
    \underbrace{\lambda-f(\lambda_0)} _{ p} &\text{,when }p>1\text{ and }f'(\lambda_0)=\cdots=f^{(p-1)}(\lambda_0)=0\\
\end{align*}
\end{theorem}
\begin{proof}
 Suppose 
\[(\lambda-\lambda_1)^{p_1},(\lambda-\lambda_2)^{p_2},\cdots,(\lambda-\lambda_u)^{p_u}\]
are the elementary factors of $A$. Then $A=TJT^{-1}$, where $J$ is a Jordan matrix. Therefore, $f(A)=Tf(J)T^{-1}$
\end{proof}



\subsection{Intermezzo: Centers}
In this section, we are interested in the study of the centers of a matrix. We begin by a well-known result.
\begin{theorem}

\end{theorem}
\begin{proof}

\end{proof}

In the following discussion, we base on a simple but useful theorem which eliminate a lot of possible situations.
\begin{theorem}
\label{relatively prime zero solution}
Let $A$ and $B$ be $n\times n$ and $m \times m$ matrix respectively. Suppose the minimal polynomials of $A$ and $B$ are relatively prime, then the euqation $XA=BX$ only have zero solution.
\end{theorem}
\begin{proof}
\[Xm_A(A)=m_A(B)X=0\text{ and }m_B(B)X=0\]
\[\exists u,v \text{ s.t. } um_A+vm_B=1\Longrightarrow 0=u(B)m_A(B)X+v(B)m_B(B)X=X\]
\end{proof}
\begin{remark}
Actually, this is a special case of the general discussion about matrix equation. We may take a deeper look at it in \ref{Matrix Equations}.
\end{remark}
And we need another theorem to reduce a general matrix to some simple cases
\begin{theorem}
If two matrices are similar, than there centers are isomorphic.
\end{theorem}
\begin{proof}
Let $A,B$ be two matrices.Suppose $P$ is an invertible matrix s.t. $P^{-1}AP=B$. If $X\in C(A)$, then $P^{-1}XP\in B$, this gives a homomorphism. By symmetry this is a isomorphism.
\end{proof}

Now let's discuss the center of a companion matrix.
\begin{theorem}
\label{center of a companion matrix}
Let $A\in M_{(n\times n)}(F)$ be a companion matrix of a polynomial $p(x)$. Then $C(A)=F[A]$, $\dim C[A]=n$ .
\end{theorem}
\begin{proof}
Let $B\in C(A) $, then $BA=AB$. Let $p(x)=x^n+\sum_{i=0}^{n-1}a_ix^i$. Let $\vec{\alpha}_1,\dots,\vec{\alpha}_n$ be the basis, then 
\begin{align*}
    A\vec{\alpha}_1&=\vec{\alpha}_2\\
    &\vdots\\
    A\vec{\alpha}_{n-1}&=\vec{\alpha}_n\\
    A\vec{\alpha}_n&=-\sum_0^{n-1}a_{i}\vec{\alpha}_{i+1}
\end{align*}
Denote $B\vec{\alpha}_i$ by $\vec{\beta}_i$. If $AB=BA$, then $AB\vec{\alpha}_i=BA\vec{\alpha}_i$ for all i. Therefore, 
\begin{align*}
    A\vec{\beta}_1&=\vec{\beta}_2\\
    &\vdots\\
    A\vec{\beta}_{n-1}&=\vec{\beta}_n\\
    A\vec{\beta}_n&=-\sum_0^{n-1}a_{i}\vec{\beta}_{i+1}
\end{align*}
So given $\beta_1$, we can calculate other $\beta_i$'s and the last equality is trivially satisfied by the property of $A$. We get $\dim C(A)=n$. On the other hand, $\dim F[A]=n$ and $F[A]\subset C(A)$, so $C(A)=F(A)$.
\end{proof}
\begin{remark}
To deal with companion matrix, or something with a similar cyclic structure, it is insightful to consider its action on the basis.
\end{remark}

Now let's discuss the center of a Jordan block.
\begin{theorem}
\label{center of a Jordan block}
Given $a\in F$, let $J_n(a)$ denote the $n\times n$ Jordan block with diagonal elements $a$. Then $C(J_n(a))=F[J_n(0)]$ and $\dim C(J_n(a))=n$.
\end{theorem}
\begin{proof}
Let $B\in C(J_n(a))$. Note that $J_n(a)=aI+J_n(0)$, so $J_n(a)B=BJ_n(a)\Longleftrightarrow J_n(0)B=BJ_n(0)$. Similarly, let $\vec{\alpha}_1,\dots,\vec{\alpha}_n$ be the basis, then
\begin{align*}
    A\vec{\alpha}_1&=\vec{\alpha}_2\\
    &\vdots\\
    A\vec{\alpha}_{n-1}&=\vec{\alpha}_n\\
    A\vec{\alpha}_n&=0
\end{align*}
for the rest of the proof, we can argue exactly as in the proof of \ref{center of a companion matrix}
\end{proof}

We can use \ref{relatively prime zero solution} to generalize \ref{center of a Jordan block} to a Jordan canonical form.
\begin{theorem}
Let $A\in M_{n\times n}(F)$ be a Jordan canonical form,i.e.\[A=\text{diag}\left \{J_{n_!}(\lambda_1),J_{n_2}(\lambda_2),\cdots,J_{n_s}(\lambda_s)\right\}\]
with the restriction that none of the $\lambda_i$'s are equal. Then $C(A)=\left\{B=\text{diag}\left\{B_1,B_2,\cdots,B_n\right\}|B_i\in F[J_{n_i}(0)]\right\}$, $\dim C(A)=n, C(A)=F[A]$.
\end{theorem}
Actually, there is a general theorem stated as below:

\section{$\lambda$-matrix Revisited: Modules over PID}
\subsection{Modules}
\begin{definition}
Let $M$ be an $R$-module. A submodule of the form\[\left \langle v \right \rangle =Rv=\left \{ rv|r\in R \right \} \]for $v\in M$ is called a cyclic submodule generated by $v$.
\end{definition}
\begin{definition}
Let $M$ be an $R$-module. The annihilator of an element $v\in M$ is \[\text{ann}(v)=\left\{r\in R|rv=0\right\}\]and the annihilator of a submodule $N$ of $M$ is \[\text{ann}(N)=\left\{r\in R|rN=\left\{0\right\}\right\}\]
\end{definition}
It is easy to see that ann($v$) and ann($N$) are ideals of $R$. For $x\in M$ we define \[\zeta_x: R\longrightarrow Rx, a\mapsto ax\]then this is a module homomorphism. ker$(\zeta_x)$=ann$(x)$, and $Rx\simeq R/\text{ann}(x)$.
\begin{definition}
Let M be an r-module. A subset of M is a basis if it is linearly independent and spans M. An R-module M is said to be free if M has a basis.
\end{definition}
\begin{theorem}

\end{theorem}
\subsection{Modules over PID}
In this section, we consider modules over principle ideal domains.\par
\begin{lemma}
Let $R$ be a PID, $M$ be a free module with rank $n$. Then every submodule of $M$ is a free $R$-module with rank $\le n$.
\end{lemma}
Let $M'$ be a finitely generated module over a PID $R$, $x_1,\cdots,x_m$ be a set of generators. Construct a free $R$-module $M$ of rank $m$, $e_1,\cdots,e_m$ be a set of basis, then the homomorphism from $M$ to $M'$
\[\eta : \sum_{i=1}^ma_ie_i\twoheadrightarrow \sum_{i=1}^ma_ix_i\]
is surjective. Denote $N=\ker (\eta)$, then $M/N\simeq M'$, where $N$ can be interpreted as the collection of the relationship between $M'$'s generators $x_1,\cdots,x_m$. Conversely, given a submodule $N$ of $M$, we can construct a finitely generated $R-$module $M'$. \par
Let $f_1,\cdots,f_n$ be a set of generators that generate $N$. Let 
\[f_i=\sum_{j=1}^ma_{ji}e_j,i=1,\cdots,n\]
i.e.\[(f_1,\cdots,f_n)=(e_1,\cdots,e_m)A\]
where $A=(a_{ij})$ is a $m\times n$ matrix. Make a basis transformation, let $e_1',\cdots,e_m'$ be another basis of $M$. Let
\[(e_1,\cdots,e_m)=(e_1',\cdots,e_m')P\]where $P=(p_{ij})\in M_{m\times m}(R)$ is a invertible matrix. Conversely, if $P$ is invertible, then $e_1',\cdots,e_m'$ is a set of basis of $M$. Similarly, utilize an invertible matrix $Q\in M_{n\times n}(R)$ to be a transformation of $N$'s generators\[(f_1,\cdots,f_n)=(f_1',\cdots,f_n')Q\]
where the restriction that $Q$ is invertible guarantees that $f_1',\cdots,f_n'$ still generates $N$. Therefore\[(f_1,\cdots,f_n)=(e_1',\cdots,e_m')PAQ^{-1}\]
If we are given a free module $M$ of rank $m$ and its basis $e_1,\cdots,e_m$, the matrix $A$ portraits the submodule $N$, as well as the quotient module $M/N$. The discussion above explains the relationship between the transformation of basis as well as generators and the transformation of matrix. This leads to our following definition.
\begin{definition}
Let $A,B\in M_{m\times n}(R)$. If $\exists$ $P\in M_{m\times m}^{\times}(R)$ and $Q\in M_{n\times n}^{\times}(R)$ s.t. \[B=PAQ\] then we call $A$ is equivalent to $B$ on $R$.
\end{definition}
\begin{theorem}
Let $M$ be a free module on $R$ with rank $m$, $N$ be its submodule. The there exists a basis $e_1,\cdots,e_n$ of $M$ s.t. $d_1e_1,\cdots,d_re_r$ be a basis of $N$, satisfying \[d_i|d_{i+1},i=1,\cdots,r-1\]$d_1,\cdots,d_r$ are uniquely determined by $N$ except for units, and they are called the invariant of $N$ or the invarinat factors of $M/N$. $r$ is the rank of $N$, $m-r$ is the rank of $M/N$.
\end{theorem}
Actually, by using the matrix representation, this theorem is essentially the same to the theorem of Smith normal form of $\lambda$-matrix. The only difference is that $F[\lambda]$ is not only a PID, but also a Euclid's ring. Therefore, we need to generalize the Smith normal form to PID.
\begin{theorem}

\end{theorem}





\section{Decomposition of Linear Operators}
In this section, we focus on how to decompose a vector space(module) with respect to a linear operator into several simple subspaces(submodules).
\par First, we need to introduce some important concepts that will be used extensively



\subsection{Cyclic Spaces}


\subsection{Irreducible Spaces}

\subsection{Centers}
In this part, we consider the center of a linear transformation $\varphi$.\par
Let $f$ be any polynomial. Then $\ker(f)$ is invariant under every $\phi\in C(\varphi)$. In fact if $v\in \ker(f)$ is any vector, then 
\[f(\varphi)\phi v=\phi f(\varphi)v=0\]\par
Next consider the decomposition of $V$ into generalized eigenspaces of $\varphi$ and of $\phi$, 
\[V=V_1\oplus\cdots V_r\quad \text{(for }\varphi\text{)}\]
and
\[V=W_1\oplus\cdots W_s\quad \text{(for }\phi\text{)}\]
and the corresponding projection operators in $V$, $\pi_i$ and $\rho_j$. Since the mappings $\pi_i$ and $\rho_j$ are respectively polynomials in $\varphi$ and $\phi$, it follows that 
\[\pi_i\circ \rho_j=\rho_j\circ\pi_i\quad \forall i,j\]\par
Now define linear transformations $\tau_{ij}$ in $V$ by \[\tau_{ij}=\pi_i\circ\rho_j\]
Then we obtain that \[\tau_{ij}^2=\pi_i\circ \rho_j\circ\pi_i\circ \rho_j=\pi_i^2\circ\rho_j^2=\pi_i\circ\rho_j=\tau_{ij}\]
and hence the $\tau_{ij}$ are again projection operators in $V$.\par
Since \[\operatorname{Im} \tau_{ij}\subset V_i\cap W_j\]
and \[\sum_{i,j}\tau_{ij}=(\sum_i \pi_i)\circ(\sum_j\rho_j)=Id_V\]
it follows that \[ \operatorname{Im}\tau_{ij}=V_i\cap W_j\]
and \[V=\sum_{i,j} V_i\cap W_j\]
\begin{theorem}
Let $V=V_1\oplus\cdots V_s$ be any decomposition of $V$ as a direct sum of subspaces. Then the subspaces $V_j$ are invariant under $\varphi$ if and only if the projection operators $\sigma_j$ are contained in $C(\varphi)$.
\end{theorem}
\begin{proof}
If $\sigma_j\in C(\varphi)$, then $V_j$ are invariant under $\varphi$. Conversely, if the $V_j$ are invariant under $\varphi$, we have for each $v\in V_j$ that $\varphi v\in V_j$, and hence $\sigma_j \varphi v=\varphi v=\varphi \sigma_j v$ while $\sigma_l\varphi v=0=\varphi \sigma_l v$ for $l\ne j$. Thus $\sigma_l$ commute with $\varphi$.
\end{proof}
\begin{theorem}[Cecioni-Frobenius Theorem]

\end{theorem}



\begin{theorem}[bicommutant]
\[C^2(\varphi)=F[\varphi]\]
\end{theorem}
\begin{proof}
Clearly $C^2(\varphi)\supset F[\varphi]$. Conversely, suppose $\phi\in C^2(\varphi)$ is any linear transformation and let \[ V=V_1\oplus \cdots\oplus V_s\] be a decomposition of $V$ into cyclic subspaces w.r.t. $\varphi$. Let $a_i$ be any fixed generator of the space $V_i$.\par
Denote by $\varphi_i$ the linear transformation in $V_i$ induced by $\varphi$ and let $\mu_i$ be the minimum polynomial of $\varphi_i$. Then $\mu_i|\mu$ so we can write $\mu=\mu_i v_i$. We may assume that $\mu_1=\mu$.\par
Now the $V_i$ are invariant under $\varphi$, so the projection operators commute with $\varphi$. Hence they commute with $\phi$ as well, and $V_i$ are invariant under $\phi$. In particular $\phi a_i\in V_i$, therefore $\phi a_i=g_i(\varphi)a_i$. Thus if $h(\varphi)a_i\in V_i$ is an arbitrary vector in $V_i$ we obtain \[\phi h(\varphi)a_i=h(\varphi)\phi a_i=h(\varphi)g_i(\varphi)a_i=g_i(\varphi)h(\varphi)a_i\]
so $\phi_i=g_i(\varphi)$ where $\phi_i$ denotes the restriction of $\phi$ to $V_i$.\par
Our goal is to show $\phi=g_1(\varphi)$.\par
Consider now linear transformations $\chi_i(i\ne 1)$ in $V$ defined by 
\begin{align*}
\chi_i &x=x\quad x\in F_j\,\, j\ne i\\
\chi_i &f(\varphi)a_i=v_i(\varphi)f(\varphi)a_1
\end{align*}
To show that $\chi_i$ is well-defined it is sufficient to prove that 
\[f(\varphi)a_i=0\Longrightarrow v_i(\varphi)f(\varphi)a_1=0\]
This is because $f(\varphi)a_i=0$, then $\mu_i|f$ and so $\mu |v_if$.\par
$\chi_i$ commutes with $\varphi$, and hence with $\phi$. On the other hand, 
\begin{align*}
\chi_i&\phi a_i=\chi_i g_i(\varphi)a_i=v_i(\varphi)g_i(\varphi)a_1\\
\phi&\chi_i a_i=\phi v_i(\varphi)a_1=v_i(\varphi)\phi a_1=v_i(\varphi)g_1(\varphi)a_1
\end{align*}
whence \[v_i(\varphi)[g_i(\varphi)-g_1(\varphi)]a_1=0\]
This relation implies $\mu|v_i(g_i-g_1)$, so that $\mu_i|g_i-g_1$. This last relation yields that for any vector $x\in V_i$, \[\phi x=g_i(\varphi)x=g_1(\varphi)x\]that is $\phi=g_1(\varphi)$.
\end{proof}

\section{Decomposition of Linear Operators Revisited: Modules over PID}
\begin{theorem}[primary decomposition]
Let $V$ be finite-dimensional and let $\tau\in\mathcal{L}(V)$  have minimal polynomial  \[m_{\tau}(x)=p_1^{e_1}(x)\cdots p_n^{e_n}(x)\]where the polynomials $p_i(x)$ are distinct monic primes. Then the $F[x]$-module $V_{\tau}$ is the direct sum \[V_{\tau}=\oplus_{i=1}^{n}V_{p_i}\]where \[V_{p_i}=\frac{m_{\tau}(x)}{p_i^{e_i}(x)}V=\left\{v\in V|p_i^{e_i}(\tau)(v)=0\right\}\]is a primary submodule of $V_\tau$ of order $p_i^{e_i}(x)$. In vector space terms, $V_{p_i}$ is a $\tau$-invariant subspace of $V$ and the minimal polynomial of $\tau|_{V_{p_i}}$ is $p_i^{e_i}(x)$
\end{theorem}




\section{???Linear Homomorphisms}
To study abstract mathematics well, the key is to transfer freely between the abstract world and the computational world. For example, semisimple homomorphism can correspond to block matrix. One aspect is that we can apply abstract structures in real world problem, another aspect is that we can subtract abstract structures from concrete examples.

\subsection{???Semisimple Operators}
\begin{definition}[simple,irreducible]
A linear operator $T$ on a vector space $V$ is simple if  $V$ has no nontrivial $T$-invariant subspace.
\end{definition}
\begin{definition}[semisimple,completely reducible]
A linear operator $T$ on a vector space is semisimple if every $T$-invariant subspace has a complementary $T$-invariant subspace.
\end{definition}
\begin{remark}
This definition origins from semisimple representation in representation theory.
A semisimple representation is a linear representation of a group or an algebra that is a direct sum of simple representations( a nonzero representation that has no proper nontrivial subrepresentation).
\end{remark}


