
\chapter{Lie Theory}
In this chapter, we exhibit the very beautiful thoery of Sophus Lie, emphasising on the application of linear algebra.

\section{BCH Formula}
\subsection{Matrix Exponetial}
The exponential is the mechanism for passing information from the Lie algebra to the Lie group.

Let $X$ be an $n\times n$ real or complex matrix. We wish to define the exponential of $X$, denoted $e^X$, by the usual power series
\[e^X=\sum_{m=0}^\infty \frac{X^m}{m!}\] 

Actually, regardless of the norm, this series converge. For simplicity, we use the Frobenius norm. This norm satisfies the inequalities 
\[\left\|X+Y\right\|\le\left\|X\right\|+\left\|Y\right\|\]
\[\left\|XY\right\|\le \left\|X\right\|\left\|Y\right\|\] 
\begin{proof}
    The first of these is the trianglular inequality, and the second follows from the Schwarz inequality.
\end{proof}

\subsection{Matrix Logarithm}
\begin{lemma}
    The function $f(z)=\sum_{m=1}^\infty (-1)^{m+1}\frac{(z-1)^m}{m}$ is defined and analytic in a circle of radius $1$ about $z=1$.

    For all $z$ with $\left|z-1\right|<1$,\[e^{\log z}=z\] 

    For all $u$ with $\left|u\right|<\log 2$, and $\left|e^u-1\right|<1$, \[\log e^u=u\]
\end{lemma}
\begin{definition}
    For any $n\times n$ matrix $A$, define $\log A$ by \[\log A=\sum_{m=1}^\infty (-1)^{m+1}\frac{(A-1)^m}{m}\]
\end{definition}

\subsection{BCH Formula for Heisenberg Group}


\subsection{General BCH Formula}


\section{The Representations of SU(2) and SU(3)}
\subsection{The Irreducible Representations of SU(2)}
We will use the following basis for $\mathfrak{sl}(2;\mathbb{C})$:
\[H=\begin{pmatrix}
    1 & 0 \\
    0 & -1
   \end{pmatrix};X=\begin{pmatrix}
    0 & 1\\
    0 & 0
   \end{pmatrix};Y=\begin{pmatrix}
    0 & 0\\
     1 & 0
   \end{pmatrix}\]

\begin{lemma}
    Let $u$ be an eigenvector of $\pi(H)$ with eigenvalue $\alpha\in\mathbb{C}$. Then 
    \[\pi(H)\pi(X)u=(\alpha+2)\pi(X)u\] 
    \[\pi(H)\pi(Y)u=(\alpha-2)\pi(Y)u\]
\end{lemma}
\begin{proof}
    
\end{proof}

\section{Root Systems}



\subsection{Root}
\begin{definition}[Root System]
    A root system is a finite-dimensional real vector space $V$ with an inner product $\langle\cdot,\cdot\rangle$,
    together with a finite collection $R$ of nonzero vectors in $V$ satisfying the following properties:\newline 
    (i) The vectors in $R$ span $V$.\newline 
    (ii) $\alpha\in R\Longrightarrow -\alpha\in R$.\newline
    (iii) If $\alpha\in R$, then the only multiples of $\alpha$ in $R$ are $\alpha$ and $-\alpha$.\newline 
    (iv) $\alpha,\beta\in R\Longrightarrow w_\alpha\cdot \beta\in R$, where 
    \[w_\alpha\cdot \beta=\beta-2\frac{\langle\beta,\alpha\rangle}{\langle\alpha,\alpha\rangle}\alpha\]
    (v) $\forall \alpha,\beta\in R$, $2\frac{\langle\beta,\alpha\rangle}{\langle\alpha,\alpha\rangle}$ is an integer.

    $\dim V$ is called the rank of the root system and the elements of $R$ is called roots.
\end{definition}
There are some redundancy in this definition since $w_\alpha(\alpha)=-\alpha$, but our definition is more intuitive.

\begin{theorem}[Weyl Group]
    Let $(V,R)$ be a root system, then the Weyl group $W$ of $R$ is the subgroup of the orthogonal group of $V$, 
    generated by $w_\alpha$ where $\alpha\in R$.
\end{theorem}

\begin{theorem}[Direct Sum]
    Suppose $(V,R)$ and $(W,S)$ are root systems. Consider the vector space $V\oplus W$, with the natural inner product.
    Then $R\cup S$ is a root system in $V\oplus W$, called the direct sum of $V$ and $W$. Here we assume the natural identification.
\end{theorem}

\begin{definition}[Irreduciblility]
    A root system $(V,R)$ is called reducible if there exists an orthogonal decomposition $V=V_1\oplus V_2$ with $\dim V_1>0$ and $\dim V_2>0$
    s.t. every element of $R$ is either in $V_1$ or in $V_2$. If no such decomposition exists, $(V,R)$ is called irreducible.
\end{definition}

\begin{definition}[Equivalence]
    Two root systems $(V_1,R_1)$ and $(V_2,R_2)$ are said to be equivalent if $\exists T:V_1\to V_2$ which is an invertible linear transformation
    s.t. $T$ maps $R_1$ onto $R_2$ and s.t. $\forall \alpha\in R_1,\beta\in V_1$, we have 
    \[ T(w_\alpha\cdot \beta)=w_{T\alpha}\cdot T\beta.\] 
    A map $T$ with this property is called an equivalence.
\end{definition}

What are the root systems?
\begin{theorem}\label{4 cases}
    Suppose $\alpha,\beta$ are noncollinear roots and $\langle\alpha\rangle\geq\langle\beta\rangle$. Then one of the following holds:\newline 
    (i) $\langle \alpha,\beta\rangle=0$ \newline 
    (ii) $\langle \alpha,\alpha\rangle=\langle \beta,\beta\rangle$ and the angle between $\alpha$ and $\beta$ is $\frac{\pi}{3}$ or $\frac{2\pi}{3}$\newline 
    (iii) $\langle \alpha,\alpha\rangle=2\langle \beta,\beta\rangle$ and the angle between $\alpha$ and $\beta$ is $\frac{\pi}{4}$ or $\frac{3\pi}{4}$\newline 
    (iv) $\langle \alpha,\alpha\rangle=3\langle \beta,\beta\rangle$ and the angle between $\alpha$ and $\beta$ is $\frac{\pi}{6}$ or $\frac{5\pi}{6}$
\end{theorem}
\begin{proof}
    Let $\theta$ be the angle between $\alpha$ and $\beta$.
    \[4\frac{\langle\beta,\alpha\rangle}{\langle\alpha,\alpha\rangle}\frac{\langle\beta,\alpha\rangle}{\langle\beta,\beta\rangle}=4\cos^2\theta\in \mathbb{Z}\]
\end{proof}
\begin{corollary}\label{a+b}
    Suppose $\alpha,\beta$ are roots. \newline
    (i) If the angle between $\alpha$ and $\beta$ is strictly obtuse, then $\alpha+\beta$ is a root.\newline 
    (ii) If the angle between $\alpha$ and $\beta$ is strictly acute, then $\alpha-\beta$ is a root.
\end{corollary}
\begin{proof}
    Discuss case by case.
\end{proof}

Now we discuss the dual of a root system.
\begin{definition}[Co-Root]
    If $(V,R)$ is a root system, then for each root $\alpha\in R$, define the co-root $H_\alpha$ by \[H_\alpha=2\frac{\alpha}{\langle\alpha,\alpha\rangle}\] 
    The set of all co-roots is denoted $R^\vee$ and is called the dual root system to $R$.
\end{definition}
This is actually an inversion with respect to the ball centered at the origin with radius $\sqrt{2}$. 
\begin{theorem}[Duality]
    $(R^\vee)^\vee=R$
\end{theorem}
\begin{proof}
    The dual system can be regarded as an inversion with respect to the ball centered at the origin with radius $\sqrt{2}$,
    and inversion is a kind of involution.
\end{proof}
\begin{theorem}[Dual Root System]
    $R^\vee$ is a root system and its Weyl group is the same as the that of $R$. 
\end{theorem}

Next we construct the base if a root system.
\begin{definition}[Base]
    A subset $\Delta$ of $R$ is called a base for $R$ if the following conditions hold:\newline 
    (i) $\Delta$ is a basis for $V$ as a vector space.\newline 
    (ii) Each root can be expressed as a linear combination of elements of $\Delta$ with integer coefficients and 
    in such a way that the coefficients are either all non-negative or all nonpositive.

    The roots for which the coefficients are non-negative are called positive roots and the others are called negative roots.
    The set of positive roots relative to a fixed base $\Delta$ is denoted $R^+$. The elements of $\Delta$ are called simple positive roots.
\end{definition}
\begin{lemma}
    If $\alpha,\beta$ are distinct elements of a base, then $\langle\alpha,\beta\rangle\le 0$. 
\end{lemma}
\begin{proof}
    Use Corollary \ref{a+b}.
\end{proof}
\begin{lemma}
    If $V$ is a finite-dimensional real vector space and $R$ is a finite subset of $V$ not containing $0$,
    then there exists a hyperplane $M$ that does not contain any element of $R$.
\end{lemma}
\begin{proof}
    The union of finite collection of hyperplanes can not be all of $V$.
\end{proof}
\begin{definition}[Indecomposability]
    
\end{definition}
\begin{theorem}\label{indecomposable base}
    Suppose $(R,V)$ is a root system, $W$ is a hyperplane not containing any element of $R$, and $R^+$ is the set of roots lying on the
    fixed side of $V$. Then the set of indecomposable elements of $R^+$ is a base for $R$.
\end{theorem}

\begin{theorem}
    Given any base $\Delta$ for $R$, there exists a hyperplane $V$ s.t. $\Delta$ arises as in Theorem \ref{indecomposable base}.
\end{theorem}

\begin{theorem}[Base for Dual Root System]\label{Base for Dual Root System}
    If $\Delta$ is a base for $R$, then the set of all co-roots $H_\alpha$, $\alpha\in\Delta$, is a base for the dual root system $R^\vee$.
\end{theorem}

Finally we explore the integral elements.
\begin{definition}[Integral Element]
    An element $v\in V$ is called an integral element if for all $\alpha\in R$, the quantity 
    \[2\frac{\langle\mu,\alpha\rangle}{\langle\alpha,\alpha\rangle}\] 
    is an integer.
\end{definition}
\begin{definition}
    If $\Delta$ is a base for $R$, then an integral element $\mu$ is called dominant integral if 
    \[ 2\frac{\langle\mu,\alpha\rangle}{\langle\alpha,\alpha\rangle}\geq 0,\quad\forall \alpha\in\Delta\] 
    It is called strictly dominant if the inequality is strict.
\end{definition}
\begin{remark}
    An integral element is dominant if and only if it is contained in closed fundamental Weyl chamber,
    and strictly dominant if and only if contained in the open fundamental Weyl chamber.
\end{remark}
\begin{lemma}
    If $v\in V$ has the property that \[2\frac{\langle\mu,\alpha\rangle}{\langle\alpha,\alpha\rangle}\] 
    is an integer for all $\alpha\in \Delta$, then $v$ us an integral element.
\end{lemma}
\begin{proof}
    Use Theorem \ref{Base for Dual Root System}.
\end{proof}

\begin{definition}[Fundamental Weights]
    
\end{definition}


\begin{definition}[Higher and Lower]
    
\end{definition}
\subsection{Example: Rank 2}





\subsection{Example: Rank 3}





\subsection{Additional Properties}




\subsection{Application: Classical Lie Algebras}



\subsection{Dynkin Diagrams}
The classification of root systems is given in terms of an object called the Dynkin diagram.

Suppose $\Delta=\{\alpha_1,\cdots,\alpha_r\}$ is a base for a root system $R$.
Then the Dynkin diagram for $R$ is a graph having vertices $v_1,\cdots,v_r$.
Between any two vertices, we place either no edge, one edge, two edge, or three edge corresponding to the four cases in Theorem \ref{4 cases}.


\begin{theorem}
    Every irreducible root system is isomorphic to precisely one root system from the following list:\newline 
    (i) $A_n$, $n\geq 1$ \newline 
    (ii) $B_n$, $n\geq 2$\newline 
    (iii) $C_n$, $n\geq 3$\newline 
    (iv) $D_n$, $n\geq 4$\newline 
    (v) $G_2$, $F_4$, $E_6$, $E_7$, $E_8$
\end{theorem}

\subsection{The Root Lattice}



\section{Lie Algebra: an Algebraic Viewpoint}
\subsection{Basic Definition and Examples}
\begin{definition}
Let $\mathfrak{g}$ be a vector space on $\mathbb{F}$, call $\mathfrak{g}$ a Lie algebra on $\mathbb{F}$ if there is a bineary operation [,] satisfying:
\begin{align*}
&(1)[x,y]=-[y,x]\\
&(2)[k_1x_1+k_2x_2,y]=k_2[x_1,y]+k_2[x_2,y]\\
&(3)[[x,y],z]+[[y,z],x]+[[z,x],y]=0
\end{align*}
\end{definition}
\begin{remark}
If $[x,y]=0\forall x,y\in\mathfrak{g}$, then we call $\mathfrak{g}$ a trivial Lie algebra
\end{remark}
\begin{example}
If dim $\mathfrak{g}=1$, then $\mathfrak{g}$ is trivial.
\end{example}
\subsection{Lie Theorem}

\subsection{Cartan's Criterion}
\begin{lemma}
    \,
    \begin{enumerate}[label=(\roman*)]
        \item If $s$ is diagonalizable, then $ad_s$ is diagonalizable
        \item If $n$ is nilpotent, the $ad_n$ is nilpotent
        \item If $z=s+n$ is the Jordan-Chevalley decomposition of $z$, then $ad_z=ad_s+ad_n$ is the Jordan-Chevalley decomposition of $ad_z$
    \end{enumerate}
\end{lemma}
\begin{proof}
(i)(iii) can be proved by direct computation. For (ii), $ad_n$ acts on $x$ by adding $n$ on either side of x. Acting enough times can make it vanish.
\end{proof}
\subsection{Killing form}
We begin with a lemma.
\begin{lemma}
Let $B(X,Y)$ be a symmetric nondegenerate bilinear form on $\mathbb{F}^{n\times n}$ with the associative property \[B(XY,Z)=B(X,YZ)\]
Then $\exists c\ne 0\in \mathbb{F} \text{ s.t. }B=c\tr(XY)$
\end{lemma}
This lemma can be easily proved by direct computation, but here we make use of the general theory of bilinear form to exihibit an elegant proof.
\begin{proof}
Note that $\tr(XY)$ is another nondegenerate bilinear form on $\mathbb{F}^{n\times n}$, so there exists an isomorphism $\varphi$ of $\mathbb{F}^{n\times n}$, such that\[B(X,Y)=\tr(\varphi(X)Y)\]
By the property of $B(X,Y)$,\[\tr(\varphi(X)Y)=\tr
(\varphi(XY))=\tr(X\varphi(Y))\]
Thus $\varphi(X)Y=\varphi(XY)=X\varphi(Y)$, so $\varphi$ is multiplication by a constant.
\end{proof}
\begin{remark}
An ideal in $\mathbb{F}^{n\times n}$ is either $\left \{ 0 \right \} $ or  $\mathbb{F}^{n\times n}$ itself, that is,  $\mathbb{F}^{n\times n}$ is a simple algebra.
\end{remark}
Now we can define the killing form on a finite-dimensional Lie algebra


\section{Discussion}

Section root system follows \citet{HallBrianC2004LgLa}.