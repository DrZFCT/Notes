\documentclass{book}
\usepackage[utf8]{inputenc}
\usepackage[a4paper,total={6in,10in}]{geometry}
\usepackage{amsmath}
\usepackage[all,cmtip]{xy}

\usepackage{amsthm}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{example}{Example}[section]
\newtheorem*{remark}{Remark}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]


\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{hyperref}



\title{Lecture Notes in Linear Algebra}
\author{Kaizhao Liu}
\date{March 2022}

\begin{document}
\maketitle
\tableofcontents
\section{Prologue}
In my opinion, \textbf{the goal of mathematics} is to develop theories to solve particular questions. \par
These are the criterion for good theories. The theories should be understandable, that is, if one novice follow the thoery from the beginning to the end, he or she should find all definitions, examples, theorems, proofs, and other mathematical objects as natural as possible. Tricks are not allowed, but integrated in the theory as a natural consequence of the definition. \par
The questions are from two sources. One is real world, the other is the theory. Some questions are natural and have pratical usage. Other questions can be developed to torture students, can be asked for fun or novelty, being more palyful and having no practical use. For studying purpose, I suggest the secular questions, that is, the first kind of questions. The second kind of questions can be omitted.\par
Remember the theory is developed for real world questions. In the process of developing the theory, theoritical questions emerge and produce more theories. New thoeries can also be put into real-world use. Therefore, the \textbf{relationship between theories and questions} is that they support each other, and can not be divided. When reading this book, you need to keep in mind what are the questions we care about. In this way, you can have a better understanding of the theory. However note that, for a certain problem, there exists other approaches unpresented here. You should compare and contrast them with the approach presented in this book.\par
Different theories have close relationship. As stated above, one theory can be developed from another theory. One theory can provide method for another theory. All these theories form a mathematician's toolbox.\par
Please take great care of the process of developing the theory. The ideas and insight can not be stated completely because the fallacy of language. These are left to the readers to feel with hearts. Take special care of the history of the theory. \par

\[\spadesuit\spadesuit\spadesuit\]
\textbf{WARNING:THIS IS AN UNFINISHED MANUSCRIPT}\par
\textbf{DO NOT TAKE IT SERIOUSLY}
\[\spadesuit\spadesuit\spadesuit\]
\begin{itemize}
\item \textbf{Cayley-Hamilton Theorem}
\item \textbf{Perron-Frobenius Theorem}
\item \textbf{Algorithm: division with remainders}
\item \textbf{Algorithm: Euclid's algorithm}
\item \textbf{Euclid's ring}
\item \textbf{prime ideal}
\item \textbf{principle ideal domain}
\item \textbf{Unique Factor Domain}
\item \textbf{Mason-Stothers Theorem} 
\item \textbf{symmetric polynomials}
\begin{enumerate}
    \item elementary symmetric polynomials
    \item fundamental theorem of symmetric polynomials: (representation of symmetric polynomials by elementary symmetric polynomials) proof and computation
\end{enumerate}
\item \textbf{Newton's formula} 
\begin{enumerate}
    \item derivation
    \item memorization
    \item application
\end{enumerate}
\item \textbf{Smith normal form} 
\begin{enumerate}
    \item algorithm \textit{from easy to complex then to easy}
    \item proof
\end{enumerate}
\end{itemize}
\textbf{Relationships between similarities of matrix and }
\chapter{Basic Concepts: Lost Properties}
This chapter collects some definitions and theorems out of the scope of linear algebra, but is of its own importance.
\section{Noncommutative-Rings}
This section aims to study the algebra of matrices from the perspective of noncommutative rings. \par
The goal of next theorem is to express the invert of $x^{-1}+y^{-1}$ by $x$ and $y$. 
\begin{theorem}
If $x,y,(x+y)$ is invertible, then $x^{-1}+y^{-1}$ is invertible with 
\[(x^{-1}+y^{-1})^{-1}=x(x+y)^{-1}y=y(x+y)^{-1}x\]
\end{theorem}
\begin{proof}
We seek inspiration from commutative rings which we are more familar with. If the ring is commutative, then \[(x^{-1}+y^{-1})^{-1}=\frac{xy}{x+y}\] The formula we seek must reduce to the formual above when the ring is commutative. And the formula we seek must be invariant when we interchange $x$ and $y$. $xy(x+y)^{-1}$ or something like this can not satisfy the symmetry. So $x(x+y)^{-1}y$ is a good candidate.\par
We first show that $x(x+y)^{-1}y=y(x+y)^{-1}x$. 
\begin{align*}
x(x+y)^{-1}y&=(x+y-y)(x+y)^{-1}y\\&=y-y(x+y)^{-1}y\\&=y-y(x+y)^{-1}(x+y-x)\\&=y(x+y)^{-1}x
\end{align*}
Now we prove that it's the inverse.
\begin{align*}
x^{-1}+y^{-1}&=x^{-1}yy^{-1}+x^{-1}xy^{-1}\\&=x^{-1}(x+y)y^{-1}
\end{align*}
The result follows immediately.
\begin{align*}
(x^{-1}+y^{-1})(y(x+y)^{-1}x)&=x^{-1}(x+y)y^{-1}(y(x+y)^{-1}x)\\&=1
\end{align*}
\end{proof}
\begin{remark}
Note how we do the tricks. These are typical in ring theory.
\end{remark}

\begin{theorem}
If $1-yx$ is invertible, then $1-xy$ is invertible with
\[(1-xy)^{-1}=1+x(1-yx)^{-1}y\]
\end{theorem}
\begin{proof}
We seek inspiration from geometric series. Formally,
\begin{align*}
(1-xy)^{-1}&=1+xy+xyxy+xyxyxy+\cdots\\
&=1+x(1+yx+yxyx+\cdots)y\\&=1+x(1-yx)^{-1}y
\end{align*}
The rest is direct computation.
\end{proof}
\section{Polynomials}
\subsection{Euclid's algorithm}
We list the two fundamental theorems on polynomial ring. These algorithms are the basic tools to portrait the polynomial ring
\begin{theorem}[division with remainder]
$f,g\in F[x],\exists q,r\text{ with } \deg r<\deg g$ s.t. $f=qg+r$
\end{theorem}
\begin{theorem}[BÃ©zout's theorem]
$f\text{ coprime with }g\in F[x],\exists u,v$ s.t. $uf+vg=1$
\end{theorem}


\subsection{}
\begin{lemma}[Eisenstein's Criterion]
\end{lemma}
\begin{proof}
let $f(x)=a_nx^n+...+a_0,(a_0,...,a_n)=1$
\[p\mid a_0,...,p\mid a_{s-1},p\nmid a_s\]

\end{proof}

\begin{theorem}[Gauss's Lemma]
 Let $f(x)\in \mathbb{Z}[x],\text{ and }f(x)\text{ is reducible in }\mathbb{Q}[x]$. Then $f(x) \text{ is reducible in }\mathbb{Z}[x].$
\end{theorem}
\begin{proof} Let $f(x)=f_1(x)f_2(x), f_i(x)\in \mathbb{Q}[x] \text{ and } degf_i(x)<degf(x)$
\[f(x)=c(f)\]
\end{proof}

\subsection{symmetric polynomials}


\begin{theorem}
Let $A$ be a commutative ring and let $t_1,...,t_n$ be algebraically independent elements over $A$. Let $f(t)\in A[t_1,...,t_n] $ be symmetric of degree $d$. Then there exists a polynomial $g(X_1,...,X_n)$ of weight $\le d$ such that\[f(t)=g(s_1,...,s_n)\] where each $s_i=s_i(t_1,...,t_n)$ is a polynomial in $t_1,...,t_n$.
\end{theorem}
\begin{proof}
By induction on $n$. The theorem is obvious if $n=1$, because $s_1=t_1$. Assume the theorem is proved for polynomials in n-1 variables.\par
If we substitute $t_n=0$ in the expression for $F(X)$, we find \[(X-t_1)\cdot \cdot \cdot (X-t_{n-1})X=X^n-(s_1)_0X^{n-1}+\cdot \cdot \cdot +(-1)^{n-1}(s_{n-1})_0X\] where $(s_i)_0$ is the expression obtained by substituting $t_n=0$ in $s_i$. We see that $(s_i)_0$ are precisely the elementary symmetric polynomials in $t_1,...,t_{n-1}$.\par
We now carry out induction on $d$. If $d=0$, our assertion is trivial. Assume $d>0$, and assume our assertion proved for polynomials of degree $<d$. Let $f(t_1,...,t_n)$ have degree $d$. There exists a polynomial $g_1(X_1,...,X_{n-1}$ of weight $\le d$ such that \[f(t_1,...,t_{n-1},0)=g_1((s_1)_0,...,(s_{n-1})_0)\]
We note that $g_1(s_1,...,s_{n-1})$ has degree $\le d$ in $t_1,...,t_n$. The polynomial \[f_1(t_1,...,t_n)=f(t_1,...,t_n)-g_1(s_1,...,s_{n-1})\]
has degree $\le d$ in $t_1,...,t_n$ and is symmetric. We have \[f_1(t_1,...,t_{n-1},0)=0\]
Hence $f_1$ has a root $t_n$, and by symmetry, \[f_1=s_nf_2(t_1,...,t_n)\]
$f_2$ has degree $\le d-n < d$. By induction, there exists a polynomial $g_2$ in $n$ variables and weight $\le d-n$ such that \[f_2(t_1,...,t_n)=g_2(s_1,...,s_n)\]
We obtain \[f(t)=g_1(s_1,...,s_{n-1})+s_ng_2(s_1,...,s_n)\] and each term on the right has weight $\le d$. This completes the proof.
\end{proof}
\begin{corollary}
\end{corollary}

\subsection{}
\begin{theorem}[Mason-Stothers Theorem]
Let $a(t),b(t),c(t)$ be relatively prime polynomials over $\mathbb{C}$ such that $a+b=c$. Then \[max \text{ deg}\left \{a,b,c\right \}\le \text{deg} \text{ rad}(abc) -1\]
\end{theorem}
\begin{proof}
 Dividing by $c$ and let $f=\frac{a}{c},g=\frac{b}{c}$.Then $f+g=1$, where $f,g$ are rational functions. Differentiating, we get $f'+g'=0$, which we rewrite as \[\frac{f'}{f}f+\frac{g'}{g}g=0\]
So that \[\frac{b}{a}=\frac{g}{g}=-\frac{\frac{f'}{f}}{\frac{g'}{g}}\]
\end{proof}

\begin{theorem}[Newton's Formula]

\end{theorem}
\begin{proof}
Let $h(x)=\prod_{i=1}^{n}(x-x_i)$. Then \[h'(x)=\sum_{i=1}^{n} \frac{h(x)}{x-x_i}\]
\[x^{k+1}h'(x)=\sum_{i=1}^{n}\frac{x^{k+1}-x_i^{k+1}+x_i^{k+1}}{x-x_i}h(x)\]


\[\text{if } k\ge n, 0=s_k-\sigma_1s_{k-1}+\cdot \cdot \cdot +(-1)^n\sigma_ns_{k-n}\]
\[\text{if } k<n, (-1)^k(n-k)\sigma_k=s_k-\sigma_1s_{k-1}+\cdot \cdot \cdot +(-1)^k\sigma_ks_0\]
For example,\[D(x_1,x_2,x_3)=\begin{vmatrix}
  s_0& s_1 & s_2\\
  s_1& s_2 &s_3 \\
  s_2& s_3 &s_4
\end{vmatrix}
\]
\[s_0=3,s_1=\sigma_1,s_2=\sigma_1^2-2\sigma_2,s_3=\sigma_1^3-3\sigma_1
\sigma_2+3\sigma_3,s_4=\sigma_1^4-4\sigma_1^2\sigma_2+4\sigma_1\sigma_3
+2\sigma_2^2\]

\end{proof}


\section{Polynomials Revisited: Structure of Fields}
\subsection{Algebraic Field Extensions}
\begin{definition}[extension field]
A field $F$ is said to be an extension field of $K$ provided that $K$ is a subfield of $F$.
\end{definition}
If $F$ is an extension field of $K$, then $F$ is a vector space over $K$. The dimension of the $K$-vector space $F$ will be denoted by $[F:K]$. $F$ is said to be a finite dimensional extension or infinite dimensional extension of $K$ according as $[F:K]$ is finite or infinite.
\begin{theorem}
Let F be an extension field of E and E an extension field of K. Then [F:K]=[F:E][E:K]. Futhermore [F:K] is finite if and only if [F:E] and [E:K] are finite.
\end{theorem}
In the situation $K\in E\in F$ of the above theorem, $E$ is said to be an intermediate field of $K$ and $F$.


\subsection{The Fundamental Theorem of Galois Theory}
\begin{definition}
Let $E$ and $F$ be extension fields of a field $K$. A nonzero map $\sigma:E\longrightarrow F$ which is both a field and a $K$-module homomorphism is called a $K$-homomorphism. Similarly if a field automorphism $\sigma\in \text{Aut}F$ is a $K$-homomorphism, then $\sigma$ is called a $K$-automorphism of $F$. The group of all $K$-automorphism of $F$ is called the Galios group if $F$ over $K$ and is denoted $\text{Aut}_KF$.
\end{definition}
\begin{theorem}
Let F be an extension field of K and $f\in K[x]$. If $u\in F$ is a root of f and $\sigma\in\text{Aut}_KF$, then $\sigma(u)\in F$ is also a root of f.
\end{theorem}

\begin{example}
If $F=\mathbb{Q}(\sqrt2,\sqrt3)=\mathbb{Q}(\sqrt2)(\sqrt3)$, then since $x^2-3$ is irreducible over $\mathbb{Q(\sqrt2)}$

\end{example}

\begin{theorem}
Let F be an extension field of K, E an intermediate field and H a sbugroup of $Aut_KF$. Then
\begin{align*}
&(i)H'=\left\{ v\in F|\sigma(v)=v \forall \sigma\in H \right\}\text{ is an intermediate field of the extension}\\
&(ii)E'=\left\{ \sigma\in Aut_KF|\sigma(u)=u \forall u\in E \right\}=Aut_EF\text{ is a subgroup of }Aut_KF\\
\end{align*}

\end{theorem}
The field $H'$ is called the fix field of $H$ in $F$.
\begin{definition}
Let $F$ be an extension field of $K$ s.t. the fixed field of the galios group  $Aut_KF$ is K itself. Then $F$ is said to be a Galios extension of $K$.
\end{definition}

\begin{theorem}[Fundamental Theorem of Galios Theory]
If F is a finite dimensional Galios extension of K, then there is a one to one correspond between the set of all intermediate fields of the extension and the set of all subgroups of the Galios group $Aut_KF$ such that:
\end{theorem}


\section{Matrices}

\subsection{Schur Complement}
\begin{definition}
    Suppose $A=\begin{pmatrix}
        A_{11} & A_{12}\\
         A_{21}&A_{22}
       \end{pmatrix}$. If $A_{11}$ is invertible, define the Schur complement of $A$ to be
        $A\setminus A_{11}=A_{22}-A_{21}A_{11}^{-1}A_{12}$.

\end{definition}
\begin{theorem}
    $\begin{pmatrix}
        I & O\\
        -A_{21}A_{11}^{-1} &I
       \end{pmatrix}
       \begin{pmatrix}
        A_{11} & A_{12}\\
        A_{21} & A_{22}
       \end{pmatrix}=\begin{pmatrix}
         A_{11}& A_{12}\\
        O &A\setminus A_{11}
       \end{pmatrix}$\par
       $\begin{pmatrix}
       A_{11} &A_{12} \\
       A_{21} &A_{22}
      \end{pmatrix}\begin{pmatrix}
       I & -A_{11}^{-1}A_{12}\\
       O & I
      \end{pmatrix}=\begin{pmatrix}
        A_{11}& O\\
        A_{21}& A\setminus A_{11}
      \end{pmatrix}$\par
      $\begin{pmatrix}
        I & O\\
        -A_{21}A_{11}^{-1} &I
       \end{pmatrix}\begin{pmatrix}
        A_{11} & A_{12}\\
        A_{21} & A_{22}
       \end{pmatrix}\begin{pmatrix}
        I & -A_{11}^{-1}A_{12}\\
        O & I
       \end{pmatrix}
      =\begin{pmatrix}
       A_{11} &O \\
       O &A\setminus A_{11}
      \end{pmatrix}$
\end{theorem}
If $A$ is Hermitian
\begin{theorem}[Haynsworth]
    
\end{theorem}


\section{Linear Homomorphism}
\subsection{exact sequence}
Exact sequence provide a sophisticated method for describing elementary properties of linear mappings. 
\begin{definition}
Suppose that 
\begin{equation*}
\xymatrix{0 \ar[r]& F \ar[r]^\varphi & E \ar[r]^\psi &G \ar[r] &0}
\end{equation*}
is a short exact sequence, and assume that $\chi : E\longleftarrow G$ is a linear mapping such that \[\psi \circ \chi =\iota \]
Then $\chi$ is said to split the sequence and the sequence 
\[\xymatrix{0 \ar[r]& F \ar[r]^\varphi & E \ar@/^/[r]^\psi  &G \ar[r] \ar@/^/[l]^\chi &0}\]
is called a split short exact sequence.
\end{definition}
\begin{theorem}
Consider the short exact sequence 
\[\xymatrix{0 \ar[r] & E_1 \ar[r]^i & E \ar[r]^\pi & E/E_1 \ar[r] & 0}\]
Then the relation \xymatrix{\chi \ar@/^/[r] & \text{Im} \chi \ar@/^/[l] } defines a bijection between linear mappings $\chi : E\longleftarrow E/E_1$ which split the sequence, and complementary subspaces of $E_1$ in $E$.
\end{theorem}
\begin{proof}

\end{proof}
\begin{theorem}
A short exact sequence \xymatrix{} is split if and only if there exist a linear mapping $\omega:F\longleftarrow E$ such that $\omega\circ\varphi=\iota$
\end{theorem}
\begin{theorem}
Given an exact sequence 
\[\xymatrix{E \ar[r]^\varphi & F \ar[r]^\psi &G \ar[r]^\chi &H}\]
Then \[ \varphi \text{ is surjective}\Longleftrightarrow \chi \text{ is injective} \]
\end{theorem}
\begin{proof}
\begin{align*}
\chi \text{ is injective} &\Longleftrightarrow \ker \chi =\left\{0\right\}\\
&\Longleftrightarrow \text{Im } \psi =\left\{0\right\}\\
&\Longleftrightarrow \ker \psi=F\\
&\Longleftrightarrow \text{Im } \varphi =F\\
&\Longleftrightarrow \varphi \text{ is surjective}
\end{align*}
\end{proof}
\begin{theorem}[5-lemma]
Assume a commutative diagram of linear maps where both horizontal sequence are exact.
\[\xymatrix{
E_1 \ar[r]^{\alpha_1} \ar[d]^{\varphi_1} & E_2 \ar[r]^{\alpha_2} \ar[d]^{\varphi_2} & E_3 \ar[r]^{\alpha_3} \ar[d]^{\varphi_3} & E_4 \ar[r]^{\alpha_4} \ar[d]^{\varphi_4} & E_5 \ar[d]^{\varphi_5}\\
F_1 \ar[r]^{\beta_1} & F_2 \ar[r]^{\beta_2} & F_3 \ar[r]^{\beta_3} & F_4 \ar[r]^{\beta_4} &F_5}\]
Then :\par
i) If $\varphi_4$ is injective and $\varphi_1$ is surjective, then \[\ker \varphi_3= \alpha_2( \ker \varphi_2 )\]
ii) If $\varphi_5$ is injective and $\varphi_2$ is surjective, then \[ \text{Im } \varphi_3=\beta_3^{-1} (\text{Im } \varphi_4) \]
iii) If maps $\varphi_1,\varphi_2,\varphi_4,\varphi_5$ are linear isomorphisms, then so is $\varphi_3$. 
\end{theorem}
\begin{proof}

\end{proof}

\chapter{Structure Theory of a Linear Transformation}
This chapter aims to find the canonical form of linear transformations. We reduce a linear space to its indecomposable subspaces, then show that these indecomposable subspaces are in fact cyclic. \par
To fully understand this procedure, we provide several extra viewpoints, including $\lambda$-matrix theory and module theory. We will show that module theory interprets all these results most adequately.
\section{$\lambda$-matrix}
When dealing with the question about if a matrix can be diagonalized or reduced to some other simple form, we often use the theory of $\lambda$-matrix, the concept of invariant factors, determinant factors, elementary factors,  minimal polynomials, characteristic polynomials, and so on. Remember, the theoretical method is very powerful in solving relevant questions. \underline{So do not compute complicated and}
\underline{unsturctural calculation! Use your structural observation!} So normally, an insightful observation can simplified the proof to a few sentences.
\subsection{Smith normal form}
Every $\lambda$-matrix can be reduced to a $\lambda$-matrix with only diagonal elements by elementary operations on $\lambda$-matrix. \par
The elementary operations of $\lambda$-matrix is essentially the same as that of ordinary matrix, but note that the elements of the matrix are polynomials, which forms a integral domain$F[\lambda]$, so the invertible elements of $F[\lambda]$ consist of $F\setminus\left\{0\right\}$. Therefore, to make the elementary $\lambda$-matrix with respect to an elementary operation invertible, we must restrict the second kind of elementary operations on $F\setminus\left\{0\right\}$ instead of $F[\lambda]$.


\begin{example}
Let $f(x)$ be relatively prime to $g(x)$ and $h(x)$, we have already know that 
\[\begin{pmatrix}f(x)g(x) & \\&h(x)\end{pmatrix}\sim \begin{pmatrix}
 g(x) & \\&f(x)h(x)\end{pmatrix}\] Now, as an exercise, lets find out $U(x)$ and $V(x)$ that make 
 \[U(x)\begin{pmatrix}f(x)g(x) & \\&h(x)\end{pmatrix}V(x)=\begin{pmatrix}
 g(x) & \\&f(x)h(x)\end{pmatrix}\]
That is, finding expicit operations that transform the first matrix to the second. Because $f$ is relatively prime to $h$ and $g$, $\exists u,v\text{ s.t. }fu+hv=1$,$\exists p,q\text{ s.t. }fp+gq=1$.
\begin{align*}
    \begin{pmatrix}f(x)g(x) & \\&h(x)\end{pmatrix}&\sim \begin{pmatrix}fg& hgv\\&h\end{pmatrix}\\
    &\sim \begin{pmatrix}fg& g\\&h\end{pmatrix}\\
    &\sim \begin{pmatrix}g& fg\\h&\end{pmatrix}\\
    &\sim \begin{pmatrix}g& \\h&-fh\end{pmatrix}\\
    &\sim \begin{pmatrix}g& \\qgh&-fh\end{pmatrix}\\
    &\sim \begin{pmatrix}g& \\&fh\end{pmatrix}\\
\end{align*}
\end{example}
\begin{remark}
This result can be used to compute the Smith normal form of a diagonal $\lambda$-matrix.
\end{remark}



\begin{theorem}[Calculation of Minimal Polynomials]
The last invariant factor of $\lambda I_{n\times n}-A$ is the minimal polynomial of $A$, $A\in M_n(F),\text{ where }F\text{ is a field}$ .
\end{theorem}


\subsection{Jordan normal form: alternative treatment}
Except for the $\lambda$-matrix treatment above, there is another observation that leads to a new way to compute the Jordan normal form, and we exhibit the treatment below.\par
The key observation: $\text{rank }J_n(0)^i=\max (n-i,0)$\par 
Given $A\in M_{n\times n}(F)$,We first calculate the eigenvalues of $\lambda$ of $A$. Then, for every $\lambda$, compute the rank of $(\lambda I-A)^i$ until it becomes a constant. Denote the rank correspond to $i$ by $r_i$, and let $r_0=n$.
Suppose $d_i=r_{i-1}-r_i$, then $d_i$ means that we have at least $d_i$ $J_i(\lambda)$ in the Jordan normal form, so we have $c_i=d_i-d_{i+1}$ $J_i(\lambda)$ Jordan blocks in the Jordan normal form.
\begin{example}
Define
\begin{align*}
    D: \mathbb{C}[x]_n&\longrightarrow \mathbb{C}[x]_n \\
    f&\longmapsto f'
\end{align*}
We choose a basis for $\mathbb{C}[x]_n$: $(1,x,\cdots,x^n)$. Then the matrix of $D$ is \[\begin{pmatrix}
 0 & 1 &  &  & \\
  &  & 2 &  & \\
  &  &  & \ddots & \\
  &  &  &  &n \\
  &  &  &  &0
\end{pmatrix}\]
\[\det (\lambda I-A)=0 \Longrightarrow \lambda=0\]
Due to $\text{rank} D=n$, there is only one Jordan block, therefore the Jordan normal form of $D$ is immediately obtained.\par
Now let's find the transition matrix. The key observation is: there is only one cyclic subspace, so we only need to find a generator for this space, and take the set of elements generated by the generator to be the basis.
\[P=\begin{pmatrix}
 \frac{n!}{0!} &  &  &  & \\
  & \frac{n!}{1!} &  &  & \\
  &  & \frac{n!}{2!} &  & \\
  &  &  &\ddots  & \\
  &  &  &  &\frac{n!}{n!}
\end{pmatrix}\]
\end{example}
From the example we can see, for certain structures, we prefer to use the treatment above from the perspective of rank instead of computing with $\lambda$-matrix. Readers should choose the best treatment according to different situations.

\subsection{The Elementary factors of $f(A)$}
We assume $A\in M_{n\times n}(\mathbb{C})$.
\begin{theorem}
The elementary factors of $f(A)$ can be obtained by the following procedure:\par
Given an elementary factor of $A$ \[(\lambda-\lambda_0)^p\]
it corresponds to elementary factors of $f(A)$ through the following way:
\begin{align*}
    (\lambda-f(\lambda_0))^p &\text{,when } p\ge 1\text{ and }f'(\lambda_0)\ne 0\\
    \underbrace{(\lambda-f(\lambda_0))^{q+1}}_h \quad \underbrace{(\lambda-f(\lambda_0))^q}_{k-h}&\text{,when } p>1\text{ and }f'(\lambda_0)=\cdots=f^{(k-1)}(\lambda_0)=0,f^{(k)}(\lambda_0)\ne 0(k<p)\\
    &\text{ where }p=qk+h(q\ge0,k>h\ge0)\\
    \underbrace{\lambda-f(\lambda_0)} _{ p} &\text{,when }p>1\text{ and }f'(\lambda_0)=\cdots=f^{(p-1)}(\lambda_0)=0\\
\end{align*}
\end{theorem}
\begin{proof}
 Suppose 
\[(\lambda-\lambda_1)^{p_1},(\lambda-\lambda_2)^{p_2},\cdots,(\lambda-\lambda_u)^{p_u}\]
are the elementary factors of $A$. Then $A=TJT^{-1}$, where $J$ is a Jordan matrix. Therefore, $f(A)=Tf(J)T^{-1}$
\end{proof}



\subsection{Intermezzo: Centers}
In this section, we are interested in the study of the centers of a matrix. We begin by a well-known result.
\begin{theorem}

\end{theorem}
\begin{proof}

\end{proof}

In the following discussion, we base on a simple but useful theorem which eliminate a lot of possible situations.
\begin{theorem}
\label{relatively prime zero solution}
Let $A$ and $B$ be $n\times n$ and $m \times m$ matrix respectively. Suppose the minimal polynomials of $A$ and $B$ are relatively prime, then the euqation $XA=BX$ only have zero solution.
\end{theorem}
\begin{proof}
\[Xm_A(A)=m_A(B)X=0\text{ and }m_B(B)X=0\]
\[\exists u,v \text{ s.t. } um_A+vm_B=1\Longrightarrow 0=u(B)m_A(B)X+v(B)m_B(B)X=X\]
\end{proof}
\begin{remark}
Actually, this is a special case of the general discussion about matrix equation. We may take a deeper look at it in \ref{Matrix Equations}.
\end{remark}
And we need another theorem to reduce a general matrix to some simple cases
\begin{theorem}
If two matrices are similar, than there centers are isomorphic.
\end{theorem}
\begin{proof}
Let $A,B$ be two matrices.Suppose $P$ is an invertible matrix s.t. $P^{-1}AP=B$. If $X\in C(A)$, then $P^{-1}XP\in B$, this gives a homomorphism. By symmetry this is a isomorphism.
\end{proof}

Now let's discuss the center of a companion matrix.
\begin{theorem}
\label{center of a companion matrix}
Let $A\in M_{(n\times n)}(F)$ be a companion matrix of a polynomial $p(x)$. Then $C(A)=F[A]$, $\dim C[A]=n$ .
\end{theorem}
\begin{proof}
Let $B\in C(A) $, then $BA=AB$. Let $p(x)=x^n+\sum_{i=0}^{n-1}a_ix^i$. Let $\vec{\alpha}_1,\dots,\vec{\alpha}_n$ be the basis, then 
\begin{align*}
    A\vec{\alpha}_1&=\vec{\alpha}_2\\
    &\vdots\\
    A\vec{\alpha}_{n-1}&=\vec{\alpha}_n\\
    A\vec{\alpha}_n&=-\sum_0^{n-1}a_{i}\vec{\alpha}_{i+1}
\end{align*}
Denote $B\vec{\alpha}_i$ by $\vec{\beta}_i$. If $AB=BA$, then $AB\vec{\alpha}_i=BA\vec{\alpha}_i$ for all i. Therefore, 
\begin{align*}
    A\vec{\beta}_1&=\vec{\beta}_2\\
    &\vdots\\
    A\vec{\beta}_{n-1}&=\vec{\beta}_n\\
    A\vec{\beta}_n&=-\sum_0^{n-1}a_{i}\vec{\beta}_{i+1}
\end{align*}
So given $\beta_1$, we can calculate other $\beta_i$'s and the last equality is trivially satisfied by the property of $A$. We get $\dim C(A)=n$. On the other hand, $\dim F[A]=n$ and $F[A]\subset C(A)$, so $C(A)=F(A)$.
\end{proof}
\begin{remark}
To deal with companion matrix, or something with a similar cyclic structure, it is insightful to consider its action on the basis.
\end{remark}

Now let's discuss the center of a Jordan block.
\begin{theorem}
\label{center of a Jordan block}
Given $a\in F$, let $J_n(a)$ denote the $n\times n$ Jordan block with diagonal elements $a$. Then $C(J_n(a))=F[J_n(0)]$ and $\dim C(J_n(a))=n$.
\end{theorem}
\begin{proof}
Let $B\in C(J_n(a))$. Note that $J_n(a)=aI+J_n(0)$, so $J_n(a)B=BJ_n(a)\Longleftrightarrow J_n(0)B=BJ_n(0)$. Similarly, let $\vec{\alpha}_1,\dots,\vec{\alpha}_n$ be the basis, then
\begin{align*}
    A\vec{\alpha}_1&=\vec{\alpha}_2\\
    &\vdots\\
    A\vec{\alpha}_{n-1}&=\vec{\alpha}_n\\
    A\vec{\alpha}_n&=0
\end{align*}
for the rest of the proof, we can argue exactly as in the proof of \ref{center of a companion matrix}
\end{proof}

We can use \ref{relatively prime zero solution} to generalize \ref{center of a Jordan block} to a Jordan canonical form.
\begin{theorem}
Let $A\in M_{n\times n}(F)$ be a Jordan canonical form,i.e.\[A=\text{diag}\left \{J_{n_!}(\lambda_1),J_{n_2}(\lambda_2),\cdots,J_{n_s}(\lambda_s)\right\}\]
with the restriction that none of the $\lambda_i$'s are equal. Then $C(A)=\left\{B=\text{diag}\left\{B_1,B_2,\cdots,B_n\right\}|B_i\in F[J_{n_i}(0)]\right\}$, $\dim C(A)=n, C(A)=F[A]$.
\end{theorem}
Actually, there is a general theorem stated as below:

\section{$\lambda$-matrix Revisited: Modules over PID}
\subsection{Modules}
\begin{definition}
Let $M$ be an $R$-module. A submodule of the form\[\left \langle v \right \rangle =Rv=\left \{ rv|r\in R \right \} \]for $v\in M$ is called a cyclic submodule generated by $v$.
\end{definition}
\begin{definition}
Let $M$ be an $R$-module. The annihilator of an element $v\in M$ is \[\text{ann}(v)=\left\{r\in R|rv=0\right\}\]and the annihilator of a submodule $N$ of $M$ is \[\text{ann}(N)=\left\{r\in R|rN=\left\{0\right\}\right\}\]
\end{definition}
It is easy to see that ann($v$) and ann($N$) are ideals of $R$. For $x\in M$ we define \[\zeta_x: R\longrightarrow Rx, a\mapsto ax\]then this is a module homomorphism. ker$(\zeta_x)$=ann$(x)$, and $Rx\simeq R/\text{ann}(x)$.
\begin{definition}
Let M be an r-module. A subset of M is a basis if it is linearly independent and spans M. An R-module M is said to be free if M has a basis.
\end{definition}
\begin{theorem}

\end{theorem}
\subsection{Modules over PID}
In this section, we consider modules over principle ideal domains.\par
\begin{lemma}
Let $R$ be a PID, $M$ be a free module with rank $n$. Then every submodule of $M$ is a free $R$-module with rank $\le n$.
\end{lemma}
Let $M'$ be a finitely generated module over a PID $R$, $x_1,\cdots,x_m$ be a set of generators. Construct a free $R$-module $M$ of rank $m$, $e_1,\cdots,e_m$ be a set of basis, then the homomorphism from $M$ to $M'$
\[\eta : \sum_{i=1}^ma_ie_i\twoheadrightarrow \sum_{i=1}^ma_ix_i\]
is surjective. Denote $N=\ker (\eta)$, then $M/N\simeq M'$, where $N$ can be interpreted as the collection of the relationship between $M'$'s generators $x_1,\cdots,x_m$. Conversely, given a submodule $N$ of $M$, we can construct a finitely generated $R-$module $M'$. \par
Let $f_1,\cdots,f_n$ be a set of generators that generate $N$. Let 
\[f_i=\sum_{j=1}^ma_{ji}e_j,i=1,\cdots,n\]
i.e.\[(f_1,\cdots,f_n)=(e_1,\cdots,e_m)A\]
where $A=(a_{ij})$ is a $m\times n$ matrix. Make a basis transformation, let $e_1',\cdots,e_m'$ be another basis of $M$. Let
\[(e_1,\cdots,e_m)=(e_1',\cdots,e_m')P\]where $P=(p_{ij})\in M_{m\times m}(R)$ is a invertible matrix. Conversely, if $P$ is invertible, then $e_1',\cdots,e_m'$ is a set of basis of $M$. Similarly, utilize an invertible matrix $Q\in M_{n\times n}(R)$ to be a transformation of $N$'s generators\[(f_1,\cdots,f_n)=(f_1',\cdots,f_n')Q\]
where the restriction that $Q$ is invertible guarantees that $f_1',\cdots,f_n'$ still generates $N$. Therefore\[(f_1,\cdots,f_n)=(e_1',\cdots,e_m')PAQ^{-1}\]
If we are given a free module $M$ of rank $m$ and its basis $e_1,\cdots,e_m$, the matrix $A$ portraits the submodule $N$, as well as the quotient module $M/N$. The discussion above explains the relationship between the transformation of basis as well as generators and the transformation of matrix. This leads to our following definition.
\begin{definition}
Let $A,B\in M_{m\times n}(R)$. If $\exists$ $P\in M_{m\times m}^{\times}(R)$ and $Q\in M_{n\times n}^{\times}(R)$ s.t. \[B=PAQ\] then we call $A$ is equivalent to $B$ on $R$.
\end{definition}
\begin{theorem}
Let $M$ be a free module on $R$ with rank $m$, $N$ be its submodule. The there exists a basis $e_1,\cdots,e_n$ of $M$ s.t. $d_1e_1,\cdots,d_re_r$ be a basis of $N$, satisfying \[d_i|d_{i+1},i=1,\cdots,r-1\]$d_1,\cdots,d_r$ are uniquely determined by $N$ except for units, and they are called the invariant of $N$ or the invarinat factors of $M/N$. $r$ is the rank of $N$, $m-r$ is the rank of $M/N$.
\end{theorem}
Actually, by using the matrix representation, this theorem is essentially the same to the theorem of Smith normal form of $\lambda$-matrix. The only difference is that $F[\lambda]$ is not only a PID, but also a Euclid's ring. Therefore, we need to generalize the Smith normal form to PID.
\begin{theorem}

\end{theorem}





\section{Decomposition of Linear Operators}
In this section, we focus on how to decompose a vector space(module) with respect to a linear operator into several simple subspaces(submodules).
\par First, we need to introduce some important concepts that will be used extensively



\subsection{Cyclic Spaces}


\subsection{Irreducible Spaces}

\subsection{Centers}
In this part, we consider the center of a linear transformation $\varphi$.\par
Let $f$ be any polynomial. Then $\ker(f)$ is invariant under every $\phi\in C(\varphi)$. In fact if $v\in \ker(f)$ is any vector, then 
\[f(\varphi)\phi v=\phi f(\varphi)v=0\]\par
Next consider the decomposition of $V$ into generalized eigenspaces of $\varphi$ and of $\phi$, 
\[V=V_1\oplus\cdots V_r\quad \text{(for }\varphi\text{)}\]
and
\[V=W_1\oplus\cdots W_s\quad \text{(for }\phi\text{)}\]
and the corresponding projection operators in $V$, $\pi_i$ and $\rho_j$. Since the mappings $\pi_i$ and $\rho_j$ are respectively polynomials in $\varphi$ and $\phi$, it follows that 
\[\pi_i\circ \rho_j=\rho_j\circ\pi_i\quad \forall i,j\]\par
Now define linear transformations $\tau_{ij}$ in $V$ by \[\tau_{ij}=\pi_i\circ\rho_j\]
Then we obtain that \[\tau_{ij}^2=\pi_i\circ \rho_j\circ\pi_i\circ \rho_j=\pi_i^2\circ\rho_j^2=\pi_i\circ\rho_j=\tau_{ij}\]
and hence the $\tau_{ij}$ are again projection operators in $V$.\par
Since \[\text{Im} \tau_{ij}\subset V_i\cap W_j\]
and \[\sum_{i,j}\tau_{ij}=(\sum_i \pi_i)\circ(\sum_j\rho_j)=Id_V\]
it follows that \[ \text{Im}\tau_{ij}=V_i\cap W_j\]
and \[V=\sum_{i,j} V_i\cap W_j\]
\begin{theorem}
Let $V=V_1\oplus\cdots V_s$ be any decomposition of $V$ as a direct sum of subspaces. Then the subspaces $V_j$ are invariant under $\varphi$ if and only if the projection operators $\sigma_j$ are contained in $C(\varphi)$.
\end{theorem}
\begin{proof}
If $\sigma_j\in C(\varphi)$, then $V_j$ are invariant under $\varphi$. Conversely, if the $V_j$ are invariant under $\varphi$, we have for each $v\in V_j$ that $\varphi v\in V_j$, and hence $\sigma_j \varphi v=\varphi v=\varphi \sigma_j v$ while $\sigma_l\varphi v=0=\varphi \sigma_l v$ for $l\ne j$. Thus $\sigma_l$ commute with $\varphi$.
\end{proof}
\begin{theorem}[Cecioni-Frobenius Theorem]

\end{theorem}



\begin{theorem}[bicommutant]
\[C^2(\varphi)=F[\varphi]\]
\end{theorem}
\begin{proof}
Clearly $C^2(\varphi)\supset F[\varphi]$. Conversely, suppose $\phi\in C^2(\varphi)$ is any linear transformation and let \[ V=V_1\oplus \cdots\oplus V_s\] be a decomposition of $V$ into cyclic subspaces w.r.t. $\varphi$. Let $a_i$ be any fixed generator of the space $V_i$.\par
Denote by $\varphi_i$ the linear transformation in $V_i$ induced by $\varphi$ and let $\mu_i$ be the minimum polynomial of $\varphi_i$. Then $\mu_i|\mu$ so we can write $\mu=\mu_i v_i$. We may assume that $\mu_1=\mu$.\par
Now the $V_i$ are invariant under $\varphi$, so the projection operators commute with $\varphi$. Hence they commute with $\phi$ as well, and $V_i$ are invariant under $\phi$. In particular $\phi a_i\in V_i$, therefore $\phi a_i=g_i(\varphi)a_i$. Thus if $h(\varphi)a_i\in V_i$ is an arbitrary vector in $V_i$ we obtain \[\phi h(\varphi)a_i=h(\varphi)\phi a_i=h(\varphi)g_i(\varphi)a_i=g_i(\varphi)h(\varphi)a_i\]
so $\phi_i=g_i(\varphi)$ where $\phi_i$ denotes the restriction of $\phi$ to $V_i$.\par
Our goal is to show $\phi=g_1(\varphi)$.\par
Consider now linear transformations $\chi_i(i\ne 1)$ in $V$ defined by 
\begin{align*}
\chi_i &x=x\quad x\in F_j\,\, j\ne i\\
\chi_i &f(\varphi)a_i=v_i(\varphi)f(\varphi)a_1
\end{align*}
To show that $\chi_i$ is well-defined it is sufficient to prove that 
\[f(\varphi)a_i=0\Longrightarrow v_i(\varphi)f(\varphi)a_1=0\]
This is because $f(\varphi)a_i=0$, then $\mu_i|f$ and so $\mu |v_if$.\par
$\chi_i$ commutes with $\varphi$, and hence with $\phi$. On the other hand, 
\begin{align*}
\chi_i&\phi a_i=\chi_i g_i(\varphi)a_i=v_i(\varphi)g_i(\varphi)a_1\\
\phi&\chi_i a_i=\phi v_i(\varphi)a_1=v_i(\varphi)\phi a_1=v_i(\varphi)g_1(\varphi)a_1
\end{align*}
whence \[v_i(\varphi)[g_i(\varphi)-g_1(\varphi)]a_1=0\]
This relation implies $\mu|v_i(g_i-g_1)$, so that $\mu_i|g_i-g_1$. This last relation yields that for any vector $x\in V_i$, \[\phi x=g_i(\varphi)x=g_1(\varphi)x\]that is $\phi=g_1(\varphi)$.
\end{proof}

\section{Decomposition of Linear Operators Revisited: Modules over PID}
\begin{theorem}[primary decomposition]
Let $V$ be finite-dimensional and let $\tau\in\mathcal{L}(V)$  have minimal polynomial  \[m_{\tau}(x)=p_1^{e_1}(x)\cdots p_n^{e_n}(x)\]where the polynomials $p_i(x)$ are distinct monic primes. Then the $F[x]$-module $V_{\tau}$ is the direct sum \[V_{\tau}=\oplus_{i=1}^{n}V_{p_i}\]where \[V_{p_i}=\frac{m_{\tau}(x)}{p_i^{e_i}(x)}V=\left\{v\in V|p_i^{e_i}(\tau)(v)=0\right\}\]is a primary submodule of $V_\tau$ of order $p_i^{e_i}(x)$. In vector space terms, $V_{p_i}$ is a $\tau$-invariant subspace of $V$ and the minimal polynomial of $\tau|_{V_{p_i}}$ is $p_i^{e_i}(x)$
\end{theorem}




\section{???Linear Homomorphisms}
To study abstract mathematics well, the key is to transfer freely between the abstract world and the computational world. For example, semisimple homomorphism can correspond to block matrix. One aspect is that we can apply abstract structures in real world problem, another aspect is that we can subtract abstract structures from concrete examples.

\subsection{???Semisimple Operators}
\begin{definition}[simple,irreducible]
A linear operator $T$ on a vector space $V$ is simple if  $V$ has no nontrivial $T$-invariant subspace.
\end{definition}
\begin{definition}[semisimple,completely reducible]
A linear operator $T$ on a vector space is semisimple if every $T$-invariant subspace has a complementary $T$-invariant subspace.
\end{definition}
\begin{remark}
This definition origins from semisimple representation in representation theory.
A semisimple representation is a linear representation of a group or an algebra that is a direct sum of simple representations( a nonzero representation that has no proper nontrivial subrepresentation).
\end{remark}







\chapter{Theory of Bilinear Functions}
Now we study the theory of bilinear functions. This chapter begins with a general definition of bilinear functions, then studies a more special case where the bilinear function is nondegenerate and induces a kind of duality. If we impose some other restrictions, then we would obtain metric vector spaces, and the more familiar inner product spaces.\par
The duality provides us a powerful language to deal with the canonical form of normal operators, which is very difficult in the language of matrices. The bridge between these two viewpoints is that the transposition of a matrix is the adjoint of the operator corresponding to that matrix in the standard inner product space.\par
We also know that the transposition of a matrix corresponds to the dual mapping of that matrix. And it is exactly the duality identify the dual mapping from the space of linear functions to the original space. These two different approaches lead to the same end.
\section{Bilinear Functions}
\begin{definition}[bilinear functions]
Let $V$ and $W$ be vector spaces and $F$ be a field. Then a mapping $B:V\times W\to F$ satisfying 
\begin{align*}
B(\lambda v_1+\mu v_2,w)&=\lambda B(v_1,w)+\mu B(v_2,w)\quad \forall v_1,v_2\in V\,\,w\in W\\
B(v,\lambda w_1+\mu w_2)&=\lambda B(v,w_1)+\mu B(v,w_2)\quad \forall v\in V\,\,w_1,w_2\in W
\end{align*}
\end{definition}
\begin{remark}
The space of all bilinear functions is denoted by $\text{Bil}(V,W;F)$.
\end{remark}

\begin{definition}[radicals]
A bilinear function $B$ in $V\times W$ determines two subspaces $N_V\subset V$ and $N_W\subset W$ defined by 
\begin{align*}
N_V&=\left \{ v|B(v,W)=0 \right \} \\
N_W&=\left \{ w|B(V,w)=0 \right \} 
\end{align*}
They are called the left radical and the right radical respectively.
\end{definition}
\begin{definition}[non-degenerate]
If $N_V=N_W=\left \{ 0 \right \} $, then $B$ is called non-degenerate.
\end{definition}
\begin{definition}[orthogonality]
Two vectors $v\in V$ and $w\in W$ are called orthogonal if $B(v,w)=0$, written $v\perp w$. Orthogonality for subspaces is defined similarly.
\end{definition}
\begin{definition}[orthogonal complement]
Let $V_1$ be a subspace of $V$, then the vectors of $W$ which are orthogonal to $V_1$ forms a subspace $V_1^\perp$ of $W$. $V_1^\perp$ is called the orthogonal complement of $V_1$. In the same way $W_1\subset W$ determines an orthogonal complement $W_1^\perp\subset V$.
\end{definition}

\begin{theorem}
$\text{Hom}(W,V^\vee)\simeq \text{Bil}(V,W;F)\simeq \text{Hom}(V,W^\vee)$
\end{theorem}
\begin{remark}
Notice that for finite dimensional spaces $V,W$, a necessary condition for non-degenerate bilinear functions to exist is that $\dim V=\dim W$.
\end{remark}
\begin{corollary}
Let $V,W$ be finite dimensional vector spaces, then \[\dim V-\dim N_V=\dim W-\dim N_W\]
\end{corollary}
\begin{theorem}
Let $V,W$ be finite dimensional vector spaces with $\dim V=\dim W$, then $\forall B\in \text{Bil}(V,W;F)$, TFAE:\par
(1) $B$ is non-degenerate.\par
(2) $N_V=\left\{0\right\}$\par
(3) $N_W=\left\{0\right\}$
\end{theorem}



\section{Dual Vector Spaces}
In this section, we focus on non-degenerate bilinear functions.
\subsection{Duality}
\begin{definition}[dual spaces]
Suppose $V,W$ is a pair of vector spaces, and assume that a fixed non-degenerate bilinear function, $\left\langle,\right\rangle$, in $V\times W$ is defined. Then $V$ and $W$ will be called \textbf{dual} with respect to the billinear function $\left\langle,\right\rangle$. 
\end{definition}
\begin{theorem}\label{general dual v.s. special dual}
Let $V,W$ be a pair of vector spaces which are dual with respect to a scalar product $\left\langle,\right\rangle$. Then an injective linear map $\Phi : V \longrightarrow W^\vee$ is defined by \[\Phi(v)(w)=\left\langle v,w\right\rangle\]
\end{theorem}
\begin{remark}
If $E$ has finite dimension, then $\Phi$ is also surjective, hence an isomorphism.
\end{remark}

\begin{definition}[dual mapping]
Suppose that $V,V'$ and $W,W'$ are two pairs of dual spaces and $\varphi:V\to W$ and $\varphi^*:W'\to V'$ are linear mappings. The mappings $\varphi$ and $\varphi^*$ are called dual if \[\left\langle \varphi v,w'\right\rangle=\left\langle v,\varphi^* w'\right\rangle \quad \forall v\in V,w'\in W\]
\end{definition}
Uniqueness comes immediately.
\begin{theorem}
There exists at most one dual mapping to a given linear mapping $\varphi:V\to W$.
\end{theorem}
Let us discuss the operations of dual mappings.
\begin{theorem}
Let $\sigma:V\to W$ and $\tau:V\to W$ and $r\in F$\par
(1) $(\sigma+\tau)^*=\sigma^*+\tau^*$\par
(2) $(r\tau)^*=r\tau^*$
\end{theorem}
Now we look at the kernel and image spaces.
\begin{theorem}
Let $\sigma:V\to W$,\par
(1) $\ker \varphi^*=(\text{Im}\varphi)^\perp$ \par
(2) $\ker \varphi=(\text{Im}\varphi^*)^\perp $
\end{theorem}
\begin{theorem}
Consider two subapsces $V_1$ and $V_2$ of $V$, then \[ (V_1+V_2)^\perp=V_1^\perp\cap V_2^\perp\]
\end{theorem}
\subsection{Space of Linear Functions}
Let $V$ be a vector space and $V^\vee $ be the space of linear functions in $V$, then they are dual w.r.t. the natural evaluation.
The space of linear functions have three important results, which are not valid for arbitrary pairs of dual spaces.\par
The first result is the existence of dual mapping.
\begin{theorem}
Let $W,W'$ be arbitrary dual spaces and $\varphi:V\to W$ be a linear mapping. Then a dual mapping $\varphi^*:W'\to V^\vee  $ exists and is given by \[(\varphi^*w')(v)=\left\langle \varphi x,w'\right\rangle\quad\forall w'\in W', v\in V\]
\end{theorem}

\begin{theorem}
Suppose $\varphi:V\to W$ is a linear mapping, and consider the dual mapping $\varphi^*:W^\vee\to V^\vee$. Then \[ \text{Im}\varphi^*=(\ker \varphi)^\perp\]
\end{theorem}

\begin{theorem}
If $W\subset V$ is any subspace, then \[ W^{\perp\perp}=W\]
\end{theorem}
Let us consider direct decompositions.
\begin{theorem}
Suppose $V=V_1\oplus V_2$. Then $V^\vee=V_1^\perp\oplus V_2^\perp $ and the pairs $V_1^\perp,V_2$ and $V_2^\perp,V_1$ are dual w.r.t. the induced evaluation.
\end{theorem}
\begin{remark}
Thus the induced injections $V_1^\perp \to V_2^\vee$ and $V_2^\perp\to V_1^\vee$ are surjective, and hence $V^\vee=V_1^\vee\oplus V_2^\vee$. Finally $(V_1^\perp)^{\perp\perp}=V_1^\perp$ and $(V_2^\perp)^{\perp\perp}=V_2^\perp$.
\end{remark}


\subsection{Finite Dimensional Vector Spaces}
For finite dimensional vector spaces, the dual spaces have some nice property, and the most important is remark\ref{general dual v.s. special dual}, which has established the relationship between a general dual space and the specific dual space of linear functions. Thus, we can transport the properties from the space of linear functions to any general dual space.



\section{Metric Vector Spaces}
In this section, all vector spaces are assumed finite-dimensional. We restrict our attention to two special kinds of bilinear form, namely the symmetic ones and the alternate ones. Note that we do not assume non-degeneration, because actually that is what we are going to deal with.
\begin{definition}
A bilinear form $V\times V\to F$ is \par
(1) \textbf{symmetric} if $\left\langle x,y\right\rangle = \left\langle y,x \right\rangle\quad\forall x,y\in V$\par
(2) \textbf{skew-symmetric} if $\left\langle x,y\right\rangle =- \left\langle y,x \right\rangle\quad\forall x,y\in V$\par
(3) \textbf{alternate} if $\left\langle x,y\right\rangle = 0\quad\forall x\in V$
\end{definition}
\begin{definition}
The pair $(V,\left\langle ,\right\rangle)$ is called a \textbf{metric vetor space} if $\left\langle ,\right\rangle$ is symmetric, skew-symmetric, or alternate.
\end{definition}
\begin{remark}
A metric vector space with a symmetric/alternate form is called an orthogonal/symplectic geometry. 
\end{remark}
We exhibit the relationship between alternate and skew symmetric bilinear forms.
\begin{theorem}
Let $V$ be a vector space over a field $F$.\par
(1) If $\text{char}(F)=2$, then \[alternate\Longrightarrow symmetric \Longleftrightarrow skew\text{-}symmetric\]
(2) If $\text{char}(F)\ne2$, then \[alternate\Longleftrightarrow skew\text{-}symmetric\]
\end{theorem}
The reason we focus on symmetric or alternate bilinear forms is that only these kinds of bilinear forms guarantee the symmetric of orthogonality.
\begin{theorem}
Let $V$ be a vector space with a bilinear form. TFAE:\par
(1) Orthogonality is a symmetric relation, i.e. $x\perp y\Longrightarrow y\perp x$\par
(2) $V$ is a metric vector space, i.e. the form is symmetric or alternate.
\end{theorem}
Now we study two types of degenerate behaviors that a vector may possess. The first case is that a vector may be orthogonal to itself, and the more severe case is that it may be orthogonal to every vector in $V$.
\begin{definition}[isotropic]
Let $V$ be a metric vector space. A nonzero $v\in V$ is isotropic if $\left\langle v,v \right\rangle$=0. $V$ is isotropic if it contains at least one isotropic vector. 
\end{definition}
\begin{definition}[degenerate]
A vector $v\in V$ is degenerate if $v\perp V$.
\end{definition}
\begin{remark}
In a metric vetor space $V$, the left radical is equal to the right radical, called the radical of $V$ and denoted by $\text{rad}(V)$. It is exactly the set of all degenerate vectors.
\end{remark}
\begin{definition}
If $S$ is a subspace of a metric vector space $V$, then $\text{rad}(S)$ denotes the set of vectors in $S$ that are degenerate in $S$.
\end{definition}
\begin{remark}
$\text{rad}(S)=S\cap S^\perp$
\end{remark}
\subsection{Orthogonal Direct Sum}
\begin{definition}
A metric vector space $V$ is the orthogonal direct sum of the subspace $S$ and $T$ if $V=S\oplus T$ and $S\perp T$, denoted by $V=S\odot T$.
\end{definition}

\begin{theorem}
$V=\text{rad}(V)\odot S$, where $S$ is non-degenerate.
\end{theorem}
\begin{proof}
$S$ is any vector space complement of $\text{rad}(V)$.
\end{proof}

\begin{lemma}
Let $S$ be a subspace of a metric vector space $V$. If either $V$ or $S$ is non-degenerate, the linear map $\tau:V\to S^\vee $ defined by \[\tau x=\left\langle \cdot,x\right\rangle|_S\]
is surjective and has kernel $S^\perp$.
\end{lemma}
\begin{proof}
Use the Riesz representation theorem.
\end{proof}
\begin{theorem}
Let $S$ be a subspace of $V$. If $S$ is non-degenerate, then \[\dim(S)+\dim(S^\perp)=\dim(V)\] Hence TFAE:\par
(1) $V=S+S^\perp$\par
(2) $S$ is non-degenerate\par
(3) $V=S\odot S^\perp$
\end{theorem}
\begin{proof}
Application of the above lemma.
\end{proof}
\begin{theorem}
Let $S$ be a subspace of $V$. If $V$ is non-degenerate, then \[\dim(S)+\dim(S^\perp)=\dim(V)\] Hence:\par
(1) $S^{\perp\perp}=S$\par
(2) $\text{rad}(S)=\text{rad}(S^\perp)$\par
(3) $S$ is non-degenerate if and only if $S^\perp$ is non-degenerate
\end{theorem}
\subsection{Isometry}
We now turn to a discussion of structure-preserving maps on metric vector spaces.
\begin{definition}
Let $V$ and $W$ be metric vector spaces. A linear isomorphism is called an isometry if it preserves the metric. If an isometry exists from $V$ to $W$, we write $V\approx W$.
\end{definition}
\begin{remark}
If $V$ is a nondegenerate orthogonal/symplectic geometry, an isometry of $V$ is called an orthogonal/symplectic transformation.
\end{remark}
\subsection{Hyperbolic Spaces}
A special type of two-dimensional metric vector space plays an important role in the structure theory of metric vector spaces, so we single out this construction in this subsection.
\begin{definition}
Let $V$ be a metric vector space. A \textbf{hyperbolic pair} is a pair of vectors $u,v\in V$ for which \[\left \langle u,u \right \rangle =\left \langle v,v \right \rangle =0,\left \langle u,v \right \rangle=1 \]
The subspace $H=\text{span}(u,v)$ is called a \textbf{hyperbolic plane}, and any space which is an orthogonal direct sum of several hyperbolic planes is called a \textbf{hyperbolic space}. 
\end{definition}


\section{Real and Complex Inner Product Spaces}
For complex inner product spaces, we are no long considering bilinear forms, but sesquilinear forms. Here we impose positive-definiteness, but the vector space need not to be finite dimensional.
\begin{definition}[inner product]
Let $V$ be a vector space over $F=\mathbb{R}$ or $F=\mathbb{C}$. An inner product on $V$ is a positive-definite sesquilinear function $\left \langle , \right \rangle :V\times V\longrightarrow F$.


\end{definition}

\begin{example}[standard inner product on $\mathbb{R}^n$]
\begin{equation*}
\left \langle (r_1,\cdots,r_n),(s_1,\cdots,s_n) \right \rangle=r_1s_1+\cdots+r_ns_n
\end{equation*}
\end{example}
\begin{example}[standard inner product on $\mathbb{R}^n$]
\begin{equation*}
\left \langle (r_1,\cdots,r_n),(s_1,\cdots,s_n) \right \rangle=r_1\bar{s}_1+\cdots+r_n\bar{s}_n
\end{equation*}
\end{example}
\begin{example}
The vectpr space $C[a,b]$ of all continous complex valued functions on the closed interval $[a,b]$ is a complex inner product space under the inner product 
\begin{equation*}
\left \langle f,g\right \rangle =\int_a^bf(x)\overline{g(x)}\mathrm{d}x
\end{equation*}
\end{example}

The following lemma is a simple conseqence of the positive definiteness.
\begin{lemma}
If $\left \langle u,x \right \rangle =\left \langle v,x \right \rangle$ for every $x\in V$, then $u=v$.
\end{lemma}
The next result points out one of the main differneces between real and complex inner product spaces and will play a key role in later work. 
\begin{theorem}
Let $\tau\in \mathcal{L}(V)$.\par
1)\begin{equation*}  \left\langle \tau v,w\right\rangle=0\,\, \forall v,w\in V\Longrightarrow \tau=0 \end{equation*}
2)If $V$ is a complex inner product space, then
\begin{equation*} \left\langle\tau v,v\right\rangle=0\,\, \forall v\in V\Longrightarrow\tau=0\end{equation*}
but this does not hold for real inner porduct spaces.
\end{theorem}
\begin{proof}
Part 1) follow directly from the above lemma. 
\end{proof}
\begin{definition}[norm]
If $V$ is an inner product space, the \textbf{norm} of $v\in V$ is defined by
\begin{equation*} \left \| v \right \| =\sqrt{\left \langle v,v \right \rangle } \end{equation*}
\end{definition}
Recalling that an isometry is an isomorphism which preserve the inner product, we have the following equivalent definition.
\begin{theorem}
An isomorphism $\tau\in \text{Hom}(V,W)$ is an isometry if and only if it preserves the norm.
\end{theorem}

Recalling the definition of an orthogonal direct sum, in an inner product space, we have the following properties.\par
By positive definiteness, we have a non-degenerate property.
\begin{theorem}
For a subspace $S\subset V$, $\text{rad}(S)=\left \{ 0 \right \} $.
\end{theorem}

We also have the uniqueness of an orthogonal direct sum decomposition.

\subsection{Gram-Schmidt Orthogonalization}
The Gram-Schmidt Orthogonalization process guarantee that for a finite dimensional inner product space, 
\begin{theorem}
If $V=S\odot T$, then $T=S^\perp$.
\end{theorem}


\subsection{Application: QR Decomposition}
\begin{theorem}[QR decomposition]
    
\end{theorem}

\begin{theorem}[Cholesky decomposition]
    
\end{theorem}

\subsection{The Projection Theorem}
\begin{theorem}
If $S$ is a finite-dimensional subspace of an inner product space $V$, then $V=S\odot S^\perp$.
\end{theorem}
We can prove it by the general theory of metric vector space, but for inner product spaces, we can approach the problem by Gram-Schmidt Orthogonalization.
\begin{theorem}

\end{theorem}




\subsection{Schur Triangularization}




\section{Structure Theory for Normal Operators}
Throughout this section, all vector spaces are assumed to be finite-dimensional unless noted. Also the field $F$ is either $\mathbb{R}$ or $\mathbb{C}$.
\subsection{The Adjoint of a Linear Operator}
We begin by limiting our attention within inner product spaces. We want to study a special type of linear operator, namely the normal operators. First we recall a definition.
\begin{definition}
Let $V$ and $W$ be finite-dimensional inner product spaces over $F$ and let $\tau\in \text{Hom}(V,W)$. Then there is a unique function $\tau^*:W\to V$ defined by \[\left \langle\tau v,w \right \rangle =\left \langle v,\tau^*w \right \rangle  \]
\end{definition}
\begin{remark}
This is a special case of the more general construction of adjoint operators, where we restrict the nondegenerate bilinear form to the inner product.
\end{remark}
Here are some basic properties of the adjoint operator. These properties essentially rely on the properties of inner product spaces.
\begin{theorem}
Let $V$ and $W$ be finite-dimensional inner product spaces over $F$ and let $\sigma,\tau\in \text{Hom}(V,W)$ and $r\in F$,\par
(1) $(\sigma+\tau)^*=\sigma^*+\tau^*$\par
(2) $(r\tau)^*=\bar{r}\tau^*$\par
(3) $\tau^{**}=\tau$, and so $\left \langle\tau^* v,w \right \rangle =\left \langle v,\tau w \right \rangle $ \par
(4) If $V=W$, then $(\sigma\tau)^*=\tau^*\sigma^*$ \par
(5) If $\tau$ is invertible, then $(\tau^{-1})^*=(\tau^*)^{-1}$\par
(6) If $V=W$ and $p(x)\in\mathbb{R}[x]$, then $p(\tau)^*=p(\tau^*)$\par
\end{theorem}
\begin{theorem}
If $\tau\in \text{End}(V)$ and $S$ is a subspace of $V$, then\par
(1) $S$ is $\tau$-invariant if and only if $S^\perp$ is $\tau^*$-invariant \par
(2) $(S,S^\perp)$ reduces $\tau$ if and only if $S$ is both $\tau$-invariant and $\tau^*$-invariant
\end{theorem}
\begin{theorem}
Let $V$ and $W$ be finite-dimensional inner product spaces over $F$ and let $\tau\in \text{Hom}(V,W)$. \par
(1) $\ker(\tau^*)=\text{im}(\tau)^\perp$ and $\text{im}(\tau^*)=\ker(\tau)^\perp$ \par
(2) $\ker(\tau^*\tau)=\ker(\tau)$ and $\ker(\tau\tau^*)=\ker(\tau^*)$ \par
(3) $\text{im}(\tau^*\tau)=\text{im}(\tau)$ and $\text{im}(\tau\tau^*)=\text{im}(\tau^*)$ \par
(4) $(\rho_{S,T})^*=\rho_{T^\perp,S^\perp}$
\end{theorem}

\subsection{Orthogonal Projections}
An operator $\rho$ is a projection operator if and only if it is idempotent. In an inner product space, we can single out some special projection operators.
\begin{definition}
A projection of the form $\rho_{S,S^\perp}$ is said to be orthogonal.
\end{definition}
\begin{remark}
Some care must be taken to avoid confusion between orthogonal projections and two projections that are orthogonal to each other, that is $\rho\sigma=\sigma\rho=0$.
\end{remark}
Here is a characterization of orthogonal projections.
\begin{theorem}
Let $V$ be a finite-dimensional inner product space. TFAE:\par
(1) $\rho$ is an orthogonal projection\par
(2) $\rho$ is idempotent and self-adjoint\par
(3) $\rho$ is idempotent and does not expand lengths
\end{theorem}


\subsection{Normal Operators}
\begin{definition}
A linear operator $\tau$ on an inner product space $V$ is normal if it commutes with its adjoint.
\end{definition}
\begin{theorem}
Let $\tau\in \text{End}(V)$ be normal \par
(1) The following are also normal:\par

\quad a. $\tau|_S$ if $\tau$ reudces $(S,S^\perp)$\par
\quad b. $\tau^*$\par
\quad c. $\tau^{-1}$ if $\tau$ is invertible\par
\quad d. $p(\tau)$, for any polynomial $p(x)\in F[x]$\par

(2) For any $v,w\in V$, $\left \langle \tau v,\tau w \right \rangle =\left \langle \tau^* v,\tau^* w \right \rangle $, and in particular $\left \| \tau v \right \| =\left \| \tau^* w \right \| $, and so \[\ker(\tau^*)=\ker(\tau)\]\par
(3) For any integer $k\ge 1$, \[\ker(\tau^k)=\ker(\tau)\]\par
(4) The minimal polynomial $m_\tau(x)$ is a product of distinct prime monic polynomials\par
(5) $\tau v=\lambda v\Longleftrightarrow \tau^*v=\bar{\lambda}v$ \par
(6) If $S$ and $T$ are submodules of $V_\tau$ with relatively prime orders, then $S\perp T$, and in particular, if $\lambda$ and $\nu$ are distinct eigenvalues of $\tau$, then $\mathcal{E}_\lambda\perp \mathcal{E}_\nu$
\end{theorem}
Using the general structure above, we can obtain the spectral theorem for normal operators.
\begin{theorem}[spectral theorem for normal operators: complex case]
Let $V$ be finite-dimensional complex inner product spaces and let $\tau\in \text{End}(V)$. TFAE:\par
(1) $\tau$ is normal\par
(2) $\tau$ is unitary diagonalizable\par
(3) $\tau$ has an orthogonal spectral resolution \[\tau=\lambda_1\rho_1+\cdots+\lambda_k\rho_k\]
\end{theorem}
\begin{theorem}[spectral theorem for normal operators: real case]

\end{theorem}
\subsection{Functional Calculus}


\subsection{Application: Positive Operators}



\subsection{Application: The Polar Decomposition}
It is well-known that any nonzero complex number $z$ can be written in the polar form $z=\rho e^{i\theta}$. We can do the same for any nonzero linear operator $\tau$ on a finite-dimensional complex inner product space.
\begin{theorem}
Let $\tau$ be a nonzero linear operator on a finite-dimensional complex inner product space $V$.\par
(1) There exist a positive operator $\rho$ and a unitary operator $\nu$ for which $\tau=\nu\rho$. Moreover, if $\tau$ is invertible, then the decomposition is unique.\par
(2) There exist a positive operator $\sigma$ and a unitary operator $\mu$ for which $\tau=\sigma\mu$. Moreover, if $\tau$ is invertible, then the decomposition is unique.
\end{theorem}
Any unitary operator $\mu$ has the form $\mu=e^{i\sigma}$, where $\sigma$ is a self adjoint operator. This gives the following corollary.
\begin{corollary}
Let $\tau$ be a nonzero linear operator on a finite-dimensional complex inner product space $V$. Then there is a positive operator $\rho$ and a self-adjoint operator $\sigma$ for which $\tau$ has the polar decomposition \[\tau=\rho e^{i\sigma}\]
Moreover, if $\tau$ is invertible, then the decomposition is unique.
\end{corollary}
Normal operators can be characterized using the polar decomposition.
\begin{theorem}
Let $\tau=\rho e^{i\sigma}$ be a polar decomposition of a nonzero operator $\tau$. Then $\tau$ is normal if and only if $\rho\sigma=\sigma\rho$.
\end{theorem}

\subsection{Normal Operators in Complex Inner Product Spaces: A Summary}
\begin{theorem}
    Let $(V,\left\langle,\right\rangle)$ be a finite-dimensional inner product space, $\tau\in \text{End}(V)$. TFAE:\par
(1) $\tau$ is normal \par
(2) $\left\|\tau \alpha \right\|=\left\|\tau^*\alpha\right\|,\forall\alpha\in V$\par
(3) $\tau=\tau_1+i\tau_2$, where $T_1,T_2$ are self-adjoint and commutative\par
(4) If $\alpha\in v,c\in \mathbb{C}$, $\tau\alpha=c\alpha$, then $\tau^*\alpha=\bar{c}\alpha$\par
(5) $\tau$ is diagonalizable\par
(6) $\exists g\in\mathbb{C}[x]$ s.t. $T^*=g(T)$\par
(7) If a subspace $W\subset V$ is $\tau$-invariant, then $W$ is $T^*$-invariant\par
(8) $\tau=\rho\mu$, $\rho$ is positive and $\mu$ is unitary with $\rho\mu=\mu\rho$\par
(9) $\tau$ has an orthogonal spectral resolution\par
(10) $\text{tr}(T^*T)=\sum_{i=1}^n \left|\lambda_i\right|^2$, where $\lambda_i$ are the eigenvalues of $T$
\end{theorem}
\begin{theorem}
    If $S,T,ST$ are normal, then $TS$ is normal.
\end{theorem}
\begin{proof}
    First, notice that the eigenvalues of $ST$ are the same as the eigenvalues of $TS$.\par
    Next, $\text{tr}((TS)^*(TS))=\text{tr}(S^*T^*TS)=\text{tr}(T^*TSS^*)=\text{tr}(TT^*S^*S)=\text{tr}(T^*S^*ST)=\text{tr}((ST)^*ST)=\sum_{i=1}^n\left|\lambda_i\right|^2$.
So $TS$ is normal.
\end{proof}



\chapter{Multilinear Algebra}
\section{Tensor Product}
\subsection{Tensor Product}
The tensor product is a universal object in the category, whose objects are multilinear maps of a fixed set of modules $E_1,\cdots,E_n$. The morphism from $f$ to $g$ is defined by $h$ which makes the following diagram commutative: 
\xymatrix{&E_1\times\cdots\times E_n \ar[ld]|f \ar[rd]|g  \\
F\ar@{-}[r]   &h\ar[r] &G }
\par
\begin{theorem}
A tensor product exists and is uniquely determined up to a unique isomorphism.\
\end{theorem}
\begin{proof}
By abstract nonsense, we know of course a tensorproduct is uniquely determined. \par
Let $M$ be the free module generated by the set of all $n$-tuples $(x_1,\cdots.x_n)$,$(x_i\in E_i)$, i.e. generated by the set $E_1\times\cdots\times E_n$. Let $N$ be the submodule generated by all the elements of the following type:
\[(x_1,\cdots,x_i+x'_i,\cdots,x_n)-(x_1,\cdots,x_i,\cdots,x_n)-(x_1,\cdots,x'_i,\cdots,x_n)\]
\[(x_1,a\cdots,x_i,\cdots,x_n)-a(x_1,\cdots,x_i,\cdots,x_n)\]
for all $x_i\in E_i,x'_i\in E_i,a\in R$. We have the canonical injection \[E_1\times\cdots\times E_n\longrightarrow M\]of our set into the free module generated by it. We compose this map with the canonical map $M\longrightarrow M/N$ on the factor module, to get a map \[\varphi : E_1\times\cdots\times E_n\longrightarrow M/N\]
We contend that $\varphi$ is multilinear and is a tensor product.\par
It is obvious that $\varphi$ is multilinear. Our definition was adjusted to this purpose. Let \[f:E_1\times\cdots\times E_n\longrightarrow G\] be a multilinear map. 
\end{proof}
The module $M/N$ will be denoted by \[E_1\otimes\cdots\otimes E_n\]


\subsection{Kronecker Prodcut: Application to Matrix}
\begin{definition}

\end{definition}

\begin{definition}[vectorization]
vec: $\mathbb{R}^{m}\otimes\mathbb{R}^{n}\cong\mathbb{R}^{mn}$ by converting a matrix to a vector is an isomorphism.
\end{definition}
\begin{theorem}
Suppose $A:k\times l, X:l\times m, B:m\times n$. Then
\[\text{vec}(AXB)=B^{t}\otimes A\text{ vec}(X)\]
\end{theorem}
\begin{proof}
Write $X=(x_1,\cdots,x_l)$, where $x_i$ is a $m\times 1$ vector.
\[
RHS=\begin{pmatrix}
 b_{11}A & \cdots & b_{m1}A\\
 \vdots &  & \vdots\\
  b_{1n}A& \cdots & b_{mn}A
\end{pmatrix}
\begin{pmatrix}
 v_1\\
 \vdots\\
v_l
\end{pmatrix}=\begin{pmatrix}
\sum_{i=1}^{m} b_{i1}Av_i \\
 \vdots \\
\sum_{i=1}^{m} b_{in}Av_i
\end{pmatrix}\\\]
\[LHS=\text{vec}((Av_1,\cdots,Av_l)B)=\text{vec}(\sum_{i=1}^{m} b_{i1}Av_i ,\cdots,\sum_{i=1}^{m} b_{in}Av_i)\]
\end{proof}
\begin{lemma}
$\text{tr}(BCD)=(\text{vec}(B^T))^T(I\otimes C)\text{vec}(D)$
\end{lemma}
\begin{proof}
$\text{tr}(BCD)=(\text{vec}(B^T))^T\text{vec}(CD)=(\text{vec}(B^T))^T(I\otimes C)\text{vec}(D)$
\end{proof}




\begin{theorem}
Let $A\in M_{n\times n}(\mathbb{F})$, $B\in M_{q\times q}(\mathbb{F})$. Then the eigenvalues of $A\otimes I_q+I_n\otimes B$ are $\lambda_i(A)+\lambda_j(B)$, $1\le i\le n, 1\le j\le q$.
\end{theorem}
\begin{proof}
Take the Jordan canonical form.
\end{proof}

\begin{theorem}

\end{theorem}

\chapter{Lie Theory}
In this chapter, we exhibit the very beautiful thoery of Sophus Lie, emphasising on the application of linear algebra.

\section{BCH Formula}
\subsection{Matrix Exponetial}
The exponential is the mechanism for passing information from the Lie algebra to the Lie group.

Let $X$ be an $n\times n$ real or complex matrix. We wish to define the exponential of $X$, denoted $e^X$, by the usual power series
\[e^X=\sum_{m=0}^\infty \frac{X^m}{m!}\] 

Actually, regardless of the norm, this series converge. For simplicity, we use the Frobenius norm. This norm satisfies the inequalities 
\[\left\|X+Y\right\|\le\left\|X\right\|+\left\|Y\right\|\]
\[\left\|XY\right\|\le \left\|X\right\|\left\|Y\right\|\] 
\begin{proof}
    The first of these is the trianglular inequality, and the second follows from the Schwarz inequality.
\end{proof}

\subsection{Matrix Logarithm}
\begin{lemma}
    The function $f(z)=\sum_{m=1}^\infty (-1)^{m+1}\frac{(z-1)^m}{m}$ is defined and analytic in a circle of radius $1$ about $z=1$.

    For all $z$ with $\left|z-1\right|<1$,\[e^{\log z}=z\] 

    For all $u$ with $\left|u\right|<\log 2$, and $\left|e^u-1\right|<1$, \[\log e^u=u\]
\end{lemma}
\begin{definition}
    For any $n\times n$ matrix $A$, define $\log A$ by \[\log A=\sum_{m=1}^\infty (-1)^{m+1}\frac{(A-1)^m}{m}\]
\end{definition}

\subsection{BCH Formula for Heisenberg Group}


\subsection{General BCH Formula}


\section{The Representations of SU(2) and SU(3)}
\subsection{The Irreducible Representations of SU(2)}
We will use the following basis for $\mathfrak{sl}(2;\mathbb{C})$:
\[H=\begin{pmatrix}
    1 & 0 \\
    0 & -1
   \end{pmatrix};X=\begin{pmatrix}
    0 & 1\\
    0 & 0
   \end{pmatrix};Y=\begin{pmatrix}
    0 & 0\\
     1 & 0
   \end{pmatrix}\]

\begin{lemma}
    Let $u$ be an eigenvector of $\pi(H)$ with eigenvalue $\alpha\in\mathbb{C}$. Then 
    \[\pi(H)\pi(X)u=(\alpha+2)\pi(X)u\] 
    \[\pi(H)\pi(Y)u=(\alpha-2)\pi(Y)u\]
\end{lemma}
\begin{proof}
    
\end{proof}

\section{Abstract Root Systems}
\begin{definition}
    A root system is a finite-dimensional real vector space $E$ with an inner product,
    together with a finite collection $R$ of nonzero vectors in $E$ satisfying the following properties:
\end{definition}

\section{Lie Algebra: an Algebraic Viewpoint}
\subsection{Basic Definition and Examples}
\begin{definition}
Let $\mathfrak{g}$ be a vector space on $\mathbb{F}$, call $\mathfrak{g}$ a Lie algebra on $\mathbb{F}$ if there is a bineary operation [,] satisfying:
\begin{align*}
&(1)[x,y]=-[y,x]\\
&(2)[k_1x_1+k_2x_2,y]=k_2[x_1,y]+k_2[x_2,y]\\
&(3)[[x,y],z]+[[y,z],x]+[[z,x],y]=0
\end{align*}
\end{definition}
\begin{remark}
If $[x,y]=0\forall x,y\in\mathfrak{g}$, then we call $\mathfrak{g}$ a trivial Lie algebra
\end{remark}
\begin{example}
If dim $\mathfrak{g}=1$, then $\mathfrak{g}$ is trivial.
\end{example}
\subsection{Lie Theorem}

\subsection{Cartan's Criterion}
\begin{lemma}
(1)If $s$ is diagonalizable, then $ad_s$ is diagonalizable\par
(2)If $n$ is nilpotent, the $ad_n$ is nilpotent\par
(3)If $z=s+n$ is the Jordan-Chevalley decomposition of $z$, then $ad_z=ad_s+ad_n$ is the Jordan-Chevalley decomposition of $ad_z$
\end{lemma}
\begin{proof}
(1)(3) can be proved by direct computation. For (2), $ad_n$ acts on $x$ by adding $n$ on either side of x. Acting enough times can make it vanish.
\end{proof}
\subsection{Killing form}
We begin with a lemma.
\begin{lemma}
Let $B(X,Y)$ be a symmetric nondegenerate bilinear form on $\mathbb{F}^{n\times n}$ with the associative property \[B(XY,Z)=B(X,YZ)\]
Then $\exists c\ne 0\in \mathbb{F} \text{ s.t. }B=c\text{tr}(XY)$
\end{lemma}
This lemma can be easily proved by direct computation, but here we make use of the general theory of bilinear form to exihibit an elegant proof.
\begin{proof}
Note that $\text{tr}(XY)$ is another nondegenerate bilinear form on $\mathbb{F}^{n\times n}$, so there exists an isomorphism $\varphi$ of $\mathbb{F}^{n\times n}$, such that\[B(X,Y)=\text{tr}(\varphi(X)Y)\]
By the property of $B(X,Y)$,\[\text{tr}(\varphi(X)Y)=\text{tr}
(\varphi(XY))=\text{tr}(X\varphi(Y))\]
Thus $\varphi(X)Y=\varphi(XY)=X\varphi(Y)$, so $\varphi$ is multiplication by a constant.
\end{proof}
\begin{remark}
An ideal in $\mathbb{F}^{n\times n}$ is either $\left \{ 0 \right \} $ or  $\mathbb{F}^{n\times n}$ itself, that is,  $\mathbb{F}^{n\times n}$ is a simple algebra.
\end{remark}
Now we can define the killing form on a finite-dimensional Lie algebra


\chapter{Matrix Analysis}
In this chapter, we examine some analysis properties of matrix. We consider matrices on field $\mathbb{C}$.

\section{Positive Matrices}
\begin{theorem}
    There are several characterization of positive matrices.\par

\end{theorem}

\section{Majorization}
\begin{definition}
Let $x=(x_1,\cdots,x_n)\in\mathbb{R}^n$. Define $x^\uparrow$ and $x^\downarrow$ be the vectors obtained by rearranging the coordinates of $x$ in the increasing and the decreasing orders respectively.
\end{definition}
\begin{remark}
$x_j^\uparrow=x_{n+1-j}^\downarrow$,    $1\le j\le n$
\end{remark}
\begin{definition}
We say that $x$ is majorized by $y$, in symbols $x\prec y$, if the following conditions are satisfied: \par
(i) $\sum_{j=1}^k x_j^\downarrow \le \sum_{j=1}^k y_j^\downarrow$, $\forall 1\le k \le n$\par
(ii) $\sum_{j=1}^n x_j^\downarrow = \sum_{j=1}^n y_j^\downarrow$
\end{definition}
\begin{remark}
The two conditions (i) and (ii) can be repalced by \par
(i') $\sum_{j=1}^k x_j^\uparrow \ge \sum_{j=1}^k y_j^\uparrow$, $\forall 1\le k \le n$\par
(ii') $\sum_{j=1}^n x_j^\uparrow = \sum_{j=1}^n y_j^\uparrow$
\end{remark}
\begin{definition}
\,\par
We say that $x$ is weakly submajorized by $y$, in symbols $x\prec_w y$ if condition (i) is fulfilled.\par
We say that $x$ is weakly supermajorized by $y$, in symbols $x\prec^w y$ if condition (i') is fulfilled.
\end{definition}


\section{Matrix Equations}\label{Matrix Equations}




\chapter{Operators on Hilbert Space}
Often, topics of Hilbert spaces is not contained in a book of linear algebra. But Hilbert Spaces is so much like the finite-dimensional Euclidean spaces, it is natural to begin our journey of functional analysis after the study of inner product spaces. Many of the definitions and theorems are modification of previously established results. 
\section{Hilbert Spaces}
\subsection{Examples}
\begin{example}
    Let $I$ be any set and $l^2(I)$ denotethe set of all functions 
\end{example}

\begin{example}
    Let $(X,\Omega,\mu)$ be a measure space consisting of a set $X$, a $\sigma$-algebra $\Omega$ of subsets of $X$,
    and a countably additive measure $\mu$ defined on $\Omega$ with values in the non-negative extended real numbers
\end{example}

Recall that an absolutely continuous function on the unit interval $[0,1]$ has a derivative a.e. on $[0,1]$.
\begin{example}
    Let $\mathcal{H}=$ the collection of all absolutely continous functions $f:[0,1]\to \mathbb{F}$ s.t. $f(0)=0$
    and $f'\in L^2(0,1)$. If $\left\langle f,g\right\rangle=\int_0^1 f'(t)g'(t)\mathrm{d}t$ for $f$ and $g$ in $\mathcal{H}$,
    then $\mathcal{H}$ is a Hilbert space.
\end{example}

\section{Operators}












\chapter{Miscellaneous Topics}
In this chapter, we discuss several advanced topics making use of linear algebra.
\section{Gelfand-Tsetlin Integrable System}
We begin by listing some facts which are useful to the following discussion.
\begin{theorem}[Cauchy's Interlace Theroem]
Let A be a Hermitian matrix of order n, and let B be a principal submatrix of A of order $n-1$. If $\lambda_n \le \lambda_{n-1} \le\cdots\le \lambda_2 \le \lambda_1$ lists the eigenvalues of A and $\mu_n \le \mu_{n-1} \leÂ·Â·Â·\le \mu_3 \le \mu_2$ the eigenvalues of B, then $\lambda_n \le \mu_n \le \lambda_{n-1} \le \mu_{n-1} \le\cdots\le \lambda_2 \le \mu_2 \le \lambda_1$
\end{theorem}
\begin{proof}[Proof 1]
This is an immediate consequence of the Courant-Fischer minimax theorem.
\end{proof}
\begin{proof}[Proof 2]
Compute the characteristic polynomial of the matrix in two different ways, one diagonal, the other submatrix diagonal and compare the coefficients. 
\[A\sim \begin{pmatrix}\mu_2 &  &  &  & a_2\\ & \mu_3 &  &  & a_3\\ &  &\ddots &  & \vdots\\&  &  & \mu_n &a_n \\\bar{a}_2& \bar{a}_3 & \cdots & \bar{a}_n &d
\end{pmatrix}\sim\begin{pmatrix}\lambda_1 &  &  &  & \\& \lambda_2 &  &  & \\ &  & \ddots &  & \\ &  &  & \lambda_{n-1} & \\&  &  &  &\lambda_n\end{pmatrix}\]
\[\prod_{i=1}^{n} (\lambda-\lambda_i)=(\lambda-d-\sum_{i=2}^{n}\frac{\left | a_i \right |^2}{\lambda-\mu_i})  \prod_{i=2}^{n}(\lambda-\mu_i) \]
\[\left | a_k \right | ^2=\frac{\prod _{i=1}^{n}(\mu_k-\lambda_i)}{\prod _{i\ne k,i=2}^n(\mu_k-\mu_i)}, k=2,\cdots,n\]
\end{proof}
\begin{definition}[Gelfand-Tsetlin Integrable System]
The phase space $M=\text{Hermite}(n\times n)$. Let $A=(a_{ij})$ where $a_{ij}$ be the coordinate function of $(i,j)$. Define a Poisson bracket on $M$ by $\left\{a_{ij},a_{kl}\right\}=\delta_{jk}a_{il}-\delta_{li}a_{kj}$. This is Gelfand-Tsetlin integrable system.
\end{definition}
\begin{lemma}
$\text{Tr}(A^k)$ is a Casimir function.
\end{lemma}
\begin{proof}
We want to show: $\left\{\text{Tr}(A^k),a_{ij}\right\}=\mathcal{L}_{X_{a_{ij}}}\text{Tr}(A^k)=0$. Noticing that Tr$(A^k)$ is a symmetric function of eigenvalue functions, we want to show $X_{a_{ij}}$ is a tangent vector of a similarity transformation.
\[X_{a_{ij}}(a_{kl})=\left\{a_{ij},a_{kl}\right\}=\delta_{jk}a_{il}-\delta_{li}a_{kj}\]
\[X_{a_{ij}}=(\delta_{jk}a_{il}-\delta_{li}a_{kj})\frac{\partial }{\partial a_{kl}} \]
\[\mathcal{L}_{X_{a_{ij}}}(A)=[E_{ij},A]\]
\[g^t_{X_{a_{ij}}}(A)=e^{tE_{ij}}Ae^{-tE_{ij}}\]
\end{proof}
\begin{theorem}
Denote $A^{(k)}$ the left-top $k\times k$ submatrix of $A\in\text{Hermite}(n\times n)$. Then $\left\{Tr(A^{(k)^j},Tr(A^{(s)^t})\right\}$=0.
\end{theorem}
\begin{proof}
We do Thimm's trick.
\[M_{1\times1}\subset M_{2\times2} \subset\cdots\subset M_{n\times n}\]
\[C^\infty(M_{n\times n})\longrightarrow C^\infty(M_{(n-1)\times (n-1)})\longrightarrow\cdots\longrightarrow C^\infty(M_{1\times 1})\text{ by restriction}\]
\end{proof}

\section{Unitary Matrices}
\begin{theorem}
    Suppose $U\in U(n)$ is unitary and symmetric, then $U=P^{-1}\Lambda P$, where $P\in O(n)$ and $\Lambda$ is unitary diagonal.
\end{theorem}
\begin{proof}
    The key is to separate the real and imaginary part of $U$, i.e. $U=A+iB$ with $A,B\in \mathbb{R}^{n\times n}$.\par
    Notice that $A$ and $B$ are symmetric, $U^*U=\bar{U}U=(A-iB)(A+iB)$, so that $A^2+B^2=I$ and $AB=BA$.
Therefore, $\exists P\in O(n)$, s.t. $A=P^{-1}\Lambda_1 P,B=P^{-1}\Lambda_2 P$ and $\Lambda_1,\Lambda_2$ are real diagonal.
Thus $U=P^{-1}(\Lambda_1+i\Lambda_2)P:=P^{-1}\Lambda P$, where $\Lambda$ is diagonal. $\Lambda$ is unitary because $U$ is unitary.
\end{proof}
\begin{corollary}
    Suppose $U\in U(n)$ is unitary and symmetric, then $U=B^tB$, where $B\in U(n)$.
\end{corollary}
\begin{theorem}
    Any unitary matrix $U\in U(n)$ can be written as $U=P_{1}\Lambda P_{2}$, where $P_1,P_2\in O(n)$ and $\Lambda$ is a diagonal matrix.
\end{theorem}
\begin{proof}
    Notice that $U^tU$ is a symmetric unitary matrix, therefore $U^tU=P^tDP$ where $P\in O(n)$ and $D$ is diagonal.
Let $\Lambda$ be a square root of $D$, that is, $\Lambda^2=D$, then $\Lambda$ is also a unitary diagonal matrix, and $U^tU=P^t\Lambda^t\Lambda P$.
Let $P_1=UP^{-1}\Lambda^{-1},P_{2}=P$, then $P_1^tP_1=I$. As $U,D,\Lambda$ are unitary, $P_1^*P_1=I$, so $P_1^t=P_1^*$. Therefore $P_1\in O(n)$. So $U=P_1\Lambda P_2$ satisfies the required conditions.
\end{proof}
\begin{corollary}[QS Decomposition]
    $\forall U\in U(n)$, $\exists Q\in O(n),S\in U(n)$ s.t. $U=QS$ and $S=f(U^tU),\text{ for some } f\in \mathbb{C}[x]$
\end{corollary}
\begin{proof}
    By the previous theorem, $U=P_1\Lambda P_2=P_1P_2 P_2^{-1}\Lambda P_2$. 
We know $\Lambda =f(D)$, so $P_2^{-1}\Lambda P_2=f(P_2^{-1}DP_2)=f(U^tU)$.
\end{proof}

\section{Matrix Decomposition}
In this section, we collect some results of matrix decomposition. 

\subsection{Positive Definite Matrices}
Throughout this subsection, $F=\mathbb{R}$ or $F=\mathbb{C}$.
\begin{lemma}
    If $P,Q\in \mathbb{C}^{n\times m}$, then 
    \[P^*P=Q^*Q\Longleftrightarrow \exists U\in U(n)\text{, s.t. }Q=UP\]
\end{lemma}
\begin{proof}
    Denote $A=P^*P=Q^*Q$, then $\ker A=\ker P=\ker Q$. Consider an inner product on $\mathbb{C}^m/\ker A$ defined by $\left\langle X,Y\right\rangle _A=Y^*AX$, and
an inner product on $\mathbb{C}^n$ defined by $\left\langle X,Y\right\rangle _0=Y^*X$. Then $P:\mathbb{C}^m/\ker A\to \text{Im}P$, $Q:\mathbb{C}^m/\ker A\to\text{Im}Q$ are both isometries.
So there exists a isometry $U:\text{Im}P\to\text{Im}Q$. Extending $U$ to an isometry of $\mathbb{C}^n$ gives the result.
\end{proof}
\begin{theorem}[Cholesky Decomposition]
    If $A$ is positive definite, then there exist an invertible matrix $P$ s.t. $A=P^*P$. Futhermore, if we impose the following restrictions:\par
    (1) $P$ is upper diagonal\par
    (2) the diagonal elements of $P$ are positive real numbers\par
    then $P$ is unique.
\end{theorem}
\begin{example}
    For $A\in\mathbb{R}^n$, TFAE:\par
    (1) $A$ is a product of two positive definite matrices\par
    (2) $A$ is diagonalizable and all the eigenvalues of $A$ are positive

\end{example}
\begin{proof}
    If $A=S_1S_2$, by Cholesky decomposition, $S_2=P^tP$, so $PAP^{-1}=PS_1P^t$ is still positive definite.\par
    The other direction is evident.
\end{proof}
\begin{remark}
    For semi-positive definite matrices, a similar result holds, but the matrix $P$ may not be invertible, and may not be unique.
\end{remark}

\subsection{}
\begin{theorem}[LU Decomposition]
    For $A\in GL_n(F)$, TFAE:
    (1) the ordered principal minors of $A$ are nonzero
    (2) $\exists L,U\in GL_n(F)$, $L$ is lower trianglular, $U$ is upper trianglular, and $A=LU$\par
    Futhermore, if all the diagonal elements of $L$(or $U$) are $1$, then the decomposition is unique.
    
\end{theorem}

\begin{theorem}[Bruhat Decomposition]

    
\end{theorem}



\end{document}