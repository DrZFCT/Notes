\chapter{Central Limit Theorems}
\section{Distributions}
\begin{definition}[distribution]
If $X$ is a random variable, then $X$ induces a probability measure on $\mathbb{R}$ called its \textbf{distribution}.
\end{definition}
\begin{remark}
The distribution of a random variable $X$ is usually described by giving its \textbf{distribution function} $F(x)=P(X\le x)$.
\end{remark}
\begin{theorem}[properties of distribution fucntions]\label{properties of distribution fucntions}
\,\par
(i) $F$ is nondecreasing\par
(ii) $\lim_{x\to-\infty}F(x)=0, \lim_{x\to\infty}F(x)=1$\par
(iii) $F$ is right continuous\par
(iv) If $F(x-)=\lim_{y\to x^-}$, then $F(x-)=P(X< x)$\par
(v) $P(X=x)=F(x)-F(x-)$
\end{theorem}
\begin{proof}
Directly follows from the definitions and the inclusion of sets.
\end{proof}
\begin{theorem}\label{r.v. constructed from distribution}
If $F$ satisfies (i),(ii),(iii) in \ref{properties of distribution fucntions}, then it is the distribution function of some random variable.
\end{theorem}
\begin{proof}
Let $\Omega=(0,1),\mathcal{F}$=the Borel sets, and $P$=Lebesgue measure. If $\omega\in(0,1)$, construct \[X(\omega)=\sup\left \{ y:F(y)<\omega \right \} \]
We need to show:\[\left \{ \omega:X(\omega)\le x \right \} =\left \{ \omega:\omega\le F(x) \right \} \]\par
For $\left \{ \omega:X(\omega)\le x \right \} \supseteq\left \{ \omega:\omega\le F(x) \right \}$, observe if $\omega \le F(x)$, then $X(\omega)\le x$.\par
For $\left \{ \omega:X(\omega)\le x \right \} \subseteq\left \{ \omega:\omega\le F(x) \right \}$, observe if $\omega>F(x)$, then since $F$ is right continuous, $\exists \epsilon>0$ s.t. $F(x+\epsilon)<\omega$. Therefore, $X(\omega)\ge x+\epsilon>x$.
\end{proof}

\section{weak convergence}
\begin{definition}[weak convergence: distribution functions]
A sequence of distribution functions $F_n$ is said to \textbf{converge weakly} to a limit $F$ if $F_n(y)\to F(y)$ for all $y$ that are continuity points of $F$. 
\end{definition}
\begin{remark}
Denoted by $F_n\Longrightarrow F$.
\end{remark}
\begin{definition}[weak convergence: random variable]
A sequence of random variables $X_n$ is said to \textbf{converge weakly (converge in distribution)} to a limit $X_\infty$ if their distribution functions converge weakly.
\end{definition}
\begin{remark}
Denoted by $X_n\Longrightarrow X_\infty$.
\end{remark}
\begin{theorem}[Skorokhod]\label{Skorokhod}
If $F_n\Longrightarrow F_\infty$, then $\exists \text{ r.v. } Y_n$ with distribution $F_n$ s.t. $Y_n\longrightarrow Y_\infty$ a.s.
\end{theorem}
\begin{proof}
As in the proof of \ref{r.v. constructed from distribution}, let $\Omega=(0,1),\mathcal{F}$=the Borel sets, and $P$=Lebesgue measure. If $\omega\in(0,1)$, construct \[Y_n(\omega)=\sup\left \{ y:F_n(y)<\omega \right \} \]
\par We want to show: \[Y_n(x)\longrightarrow Y_\infty(x)\] for all but a countable number of $x$.
\par We begin by identifying the exceptional set. Let $a_x=\sup\left \{ y:F_\infty(y)<x \right \}, b_x=\inf\left \{ y:F_\infty(y)>x \right \}$, and $\Omega_0=\left \{ x:(a_x,b_x)=\emptyset \right \}$. Then $\Omega-\Omega_0$ is countable. If $x\in \Omega_0$, then $F_\infty(y)<x$ for $y<Y_\infty(x)$ and $F_\infty(y)>x$ for $y> Y_\infty(x)$.\par
Now we show $\liminf_{n\to\infty} Y_n(x)\ge Y_\infty(x)$. Choose $y<Y_\infty(x)$ s.t. $F_\infty$ is continuous at $y$.  Then  $F_\infty(y)<x$ and $F_n(y)\longrightarrow F_\infty(y)$, so $F_n(y)< x$ for $n$ sufficient large, that is, $Y_n(x)\ge y$. This is true for all such $y$'s so the result follows.\par
The reverse inequality $\limsup_{n\to\infty} Y_n(x)\le Y_\infty(x)$ is true by symmetry.
\end{proof}
\begin{theorem}
$X_n\Longrightarrow X_\infty\Longleftrightarrow\forall\text{ bounded continuous function }g, Eg(X_n)\longrightarrow Eg(X_\infty)$
\end{theorem}
\begin{proof}
$\Longrightarrow$: By \ref{Skorokhod}, let $Y_n$ have the same distribution as $X_n$ and converge a.s. Since $g$ is continuous, $g(Y_n)\longrightarrow g(Y_\infty)$ a.s. so by the bounded convergence theorem $Eg(X_n)\longrightarrow Eg(X_\infty)$.\par
$\Longleftarrow$: construct a bounded and continuous function\[g_{x,\epsilon}(y)=\left\{\begin{matrix}
 1 &y\le x \\
 0 & y\ge x+\epsilon\\
 \text{linear} & x< y< x+\epsilon
\end{matrix}\right.\]
Therefore, $\limsup_{n\to\infty} P(X_n\le x)\le \limsup_{n\to\infty} Eg_{x,\epsilon}(X_n)=Eg_{x,\epsilon}(X_\infty)\le P(X_\infty\le x+\epsilon)$. Letting $\epsilon\to 0$ gives $\limsup_{n\to\infty}P(X_n\le x)\le P(X_\infty\le x)$. The reverse inequality can be proved in the same way.
\end{proof}
\begin{theorem}[continuous mapping theorem]

\end{theorem}

\begin{theorem}
TFAE:\par
(i) $X_n\Longrightarrow X_\infty$\par
(ii) For all open sets $G$, $\liminf_{n\to\infty}P(X_n\in G)\ge P(X_\infty\in G)$\par
(iii) For all closed sets $K$, $\limsup_{n\to\infty}P(X_n\in K)\le P(X_\infty\in K)$\par
(iv) For all Borel sets $A$ with $P(X_\infty\in\partial A)=0$, $\lim_{n\to\infty}P(X_n\in A)=P(X_\infty\in A)$
\end{theorem}

\begin{theorem}[Helly's selection theorem]
For every sequence $F_n$ of distribution functions, there is a subsequence $F_{n(k)}$ and a right continuous nondecreasing function $F$ s.t. $F_{n(k)}\Longrightarrow_v F$.
\end{theorem}
\begin{remark}
The limit may not be a distribution function. This type of convergence is called vague convergence.
\end{remark}
\begin{proof}
To construct the function $F$, we adopt the standard diagonal argument. Let $\left \{ q_i \right \} $ be an enumeration of the rationals. Since $F_m(q_k)\in [0,1]$ is bounded for all $m$, there is a subsubsequence $m_k(i)$ that is a subsequence of $m_{k-1}(i)$ s.t. $F_{m_k(i)}(q_k)\longrightarrow G(q_k)$. Select the diagonal sequence $n(k)=m_k(k)$, then by construction, $F_{n(k)}(q)\longrightarrow G(q)$ for all rational $q$.\par
Now we need to consruct $F$ from $G$. Let \[F(x)=\inf \left \{ G(q):q\in \mathbb{Q},q>x \right \} \]then $F(x)$ is right continuous and nondecreasing.\par
Let $x$ be a continuity point of $F$. Pick rational $s>x$ s.t. $F(x)\le F(s)< F(x)+\epsilon$, then as $F_{n(k)}(s)\longrightarrow G(s)\le F(s)$, for $k$ sufficient large, we have $F_{n(k)}(x)\le F_{n(k)}(s)<F(x)+\epsilon$. On the other hand, pick rational $r_1<r_2<x$ s.t. $F(x)-\epsilon<F(r_1)\le F(r_2)\le F(x)$, then as $F_{n(k)}(r_2)\longrightarrow G(r_2)\ge F(r_1)$, so $F_{n(k)}(x)\ge F_{n(k)}(r_2)>F(x)-\epsilon$ for $k$ sufficient large. Thus as $\epsilon\to 0$, we have the weak convergence.
\end{proof}
\begin{theorem}
Every subsequential limit is the distribution function of a probability measure $\Longleftrightarrow$ the sequence is \textbf{tight}, i.e. $\forall \epsilon>0,\exists M_\epsilon$ s.t.
\[\limsup_{n\to\infty} 1-F_n(M_\epsilon)+F_n(-M_\epsilon)\le \epsilon\]
\end{theorem}
\begin{proof}
First note that for vague convergence $0\le F(x)\le 1$.\par
$\Longleftarrow$: Suppose the sequence is tight and $F_{n(k)}\Longrightarrow_v F$. Let $r<-M_\epsilon,s>M_\epsilon$ be continuity points of $F$, then $1-F(s)+F(r)=\lim_{k\to\infty}1-F_{n(k)}(s)+F_{n(k)}(r)\le \limsup_{n\to\infty}1-F_n(M_\epsilon)+F_n(M_\epsilon)\le \epsilon$. Letting $r\to-\infty$ and $s\to\infty$ gives $\limsup_{n\to\infty}1-F(x)+F(-x)\le \epsilon$.\par
$\Longrightarrow$: Suppose $F_n$ is not tight. Then there is an $\epsilon>0$ and a subsequence $n(k)\to\infty$ s.t. \[1-F_{n(k)}(k)+F_{n(k)}(-k)\ge\epsilon\] for all $k$. By passing to a further subsequence $F_{n(k_j)}$ we can suppose $F_{n(k_j)}\Longrightarrow_v F$. Let $r<0<s$ be continuity points of $F$. Then $1-F(s)+F(r)=\lim_{j\to\infty} 1-F_{n(k_j)}(s)+F_{n(k_j)}(r)\ge\liminf_{j\to\infty}1-F_{n(k_j)}(k_j)+F_{n(k_j)}(-k_j)\ge\epsilon$. Letting $s\to\infty$ and $r\to-\infty$, we see that $F$ is not the distribution function of a probability measure.
\end{proof}
\begin{corollary}
If there is a $\varphi\ge0$ s.t. $\varphi(x)\to\infty$ as $\left | x \right | \to\infty$ and 
\[\sup_n\int\varphi(x)\mathrm{d}F_n(x)=C<\infty\] then $F_n$ is tight.
\end{corollary}
\begin{proof}
$C\ge\int\varphi(x)\mathrm{d}F_n(x)\ge\inf_{\left | x \right |\ge M}\varphi(x)(F_n(-M)+1-F_n(M))$
\end{proof}
\begin{lemma}
If $X_n\longrightarrow X$ in probability, then $X_n\Longrightarrow X$. Conversely, if $X_n\Longrightarrow c$ where $c$ is a constant, then $X_n\longrightarrow c$ in probability.
\end{lemma}
\begin{theorem}[slutsky]
If $X_n\Longrightarrow X$ and $Y_n\Longrightarrow c$, where $c$ is a constant, then:\par
(i) $X_n+Y_n\Longrightarrow X+c$\par
(ii) $X_nY_n\Longrightarrow cX$
\end{theorem}

\section{Characteristic Functions}
\begin{definition}[characteristic function]
    If $X$ is a random variable, we define its characteristic function by $\varphi(t)=Ee^{itX}$.
\end{definition}
\begin{theorem}[properties of ch.f.]
    All ch.f.s have the following properties:\newline 
    (i) $\varphi(0)=1$\newline 
    (ii) $\varphi(-t)=\overline{\varphi(t)}$\newline 
    (iii) $\left|\varphi(t)\right|\le 1$\newline 
    (iv) $\varphi(t)$ is uniformly continuous on $(-\infty,\infty)$\newline 
    (v) $Ee^{it(aX+b)}=e^{itb}\varphi(at)$
\end{theorem}
\begin{theorem}
    If $X_1$ and $X_2$ are independent and have ch.f.'s $\varphi_1$ and $\varphi_2$, then $X_1+X_2$ has ch.f. $\varphi_1(t)\varphi_2(t)$.
\end{theorem}
\begin{lemma}
    If $F_1,\cdots,F_n$ have ch.f. $\varphi_1,\cdots,\varphi_n$ and $\lambda_i\ge 0$ have $\lambda_1+\cdots+\lambda_n=1$, then $\sum_{i=1}^n\lambda_iF_i$ has ch.f. $\sum_{i=1}^n\lambda_i\varphi_i$.
\end{lemma}


\begin{theorem}[Continuity theorem]
    Let $\mu_n$, $1\le n\le \infty$ be probability measures with ch.f. $\varphi_n$.\newline 
    (i) If $\mu_n\Longrightarrow \mu_\infty$, then $\varphi_n(t)\to\varphi_\infty(t)$ for all $t$.\newline 
    (ii) If $\varphi_n(t)$ converges pointwise to a limit $\varphi(t)$ that is continuous at $0$,
    then the associated sequence of distributions $\mu_n$ is tight and converges weakly to the measure $\mu$ with characteristic function $\varphi$.
\end{theorem}

The next result is useful for constructing examples of ch.f.'s.
\begin{example}[Polya's distribution]
    \begin{align*}
        \text{Density}\quad& \frac{1-\cos(x)}{\pi x^2}   \\
        \text{Ch.f.}\quad& (1-\left|t\right|)^+
    \end{align*}

\end{example}
\begin{theorem}[Polya's criterion]
    Let $\varphi(t)$ be real nonnegative and have $\varphi(0)=1$, $\varphi(t)=\varphi(-t)$, and $\varphi$ is decreasing and convex on $(0,\infty)$ with 
    $\lim_{t\downarrow 0}\varphi(t)=1,\lim_{t\uparrow\infty}\varphi(t)=0$. Then there is a probability measure $\nu$ on $(0,\infty)$, so that
    \[\varphi(t)=\int_0^\infty (1-\left|\frac{t}{s}\right|^+)\nu(\mathrm{d}s)\] 
    and hence $\varphi$ is a characteristic function.
\end{theorem}

\section{The Moment Problem}
\begin{example}[Heyde(1963)]
    Consider the lognormal density \[f_0(x)=\frac{1}{\sqrt{(2\pi)}}\frac{1}{x}\exp^{-\frac{(\log x)^2}{2}}1_{x\ge 0}\] 
    and for $-1\le a\le 1$ let \[f_a(x)=f_0(x)(1+a\sin(2\pi\log x))\]
    We claim that $f_a$ is a density and has the same moment as $f_0$
\end{example}
\begin{example}
    
\end{example}
A usual sufficient condition for a distribution to be determined by its moments is:
\begin{theorem}
    If $\limsup_{n\to\infty}\frac{\mu_{2n}^{\frac{1}{2n}}}{2n}=r<\infty$, then there is at most one d.f. $F$ with
    $\mu_n=\int x^n\mathrm{d}F(x)$ for all positive integers $n$.
\end{theorem}
\begin{proof}
    First we explain why the condition only consider $2n$. Let $F$ be any d.f. with the moment $\mu_n$ and let $\nu_n=\int\left|x\right|^n\mathrm{d}F(x)$.
    The Cauchy-Schwarz inequality implies $\nu_{2n+1}^2\le \mu_{2n}\mu_{2n+2}$, so \[\limsup_{n\to\infty}\frac{\nu_{n}^{\frac{1}{n}}}{n}=r<\infty\]
    Next, we have \[\left|e^{i\theta X}(e^{itX}-\sum_{m=0}^{n-1}\frac{(itX)^m}{m!})\right|\le \frac{\left|tX\right|^n}{n!}\]
    Taking expected value, we have \[\left|\varphi(\theta+t)-\varphi(\theta)-t\varphi'(\theta)-\cdots-\frac{t^{n-1}}{(n-1)!}\varphi^{(n-1)}(\theta)\right|\le \frac{\left|t\right|^n}{n!}\nu_n\]
    So we see that for any $\theta$, \[\varphi(\theta+t)=\varphi(\theta)+\sum_{m=1}^\infty\frac{t^m}{m!}\varphi^{(m)}(\theta)\quad\forall\left|t\right|<\frac{1}{er}\]
    Let $G$ be another distribution with the given moments and $\psi$ its ch.f.. Since $\psi(0)=\varphi(0)=1$, it follows from the above equation and induction that $\psi(t)=\varphi(t)$ for $\left|t\right|\le \frac{k}{3r}$ for all $k$,
    so the two ch.f. coincide and the distributions are equal.
\end{proof}
Here is an application.
\begin{theorem}[Semi-Circle Law]
    
\end{theorem}

\section{Central Limit Theorems}
\begin{theorem}
    Let $X_1,X_2,\cdots$ be i.i.d. with $EX_i=\mu$, $\text{Var}(X_i)=\sigma^2\in(0,\infty)$. If $S_n=X_1+\cdots+X_n$, then
    \[\frac{S_n-n\mu}{\sigma \sqrt{n}}\Longrightarrow \mathcal{N}(0,1)\] 
\end{theorem}
\begin{proof}
    WLOG suppose $\mu=0$. $\varphi(t)=Ee^{itX_1}=1-\frac{\sigma^2t^2}{2}+o(t^2)$, so $Ee^{itS_n/\sigma n^{\frac{1}{2}}}=(1-\frac{t^2}{2n}+o(\frac{1}{n}))^n$.
    The last quantity $\to e^{-\frac{t^2}{2}}$ as $n\to\infty$, and the conclusion follows from the continuity theorem.
\end{proof}

\begin{theorem}[Lindeberg-Feller theorem]
    For each $n$, let $X_{n,m},1\le m\le n$ be independent random variables with $EX_{n,m}=0$. Suppose \newline 
    (i) $\sum_{m=1}^n EX_{n,m}^2\to \sigma^2>0$ \newline 
    (ii) $\forall \epsilon>0$, $\lim_{n\to\infty}\sum_{m=1}^nE(\left|X_{n,m}\right|^2;\left|X_{n,m}\right|>\epsilon)=0$ \newline 
    Then $S_n=X_{n,1}+\cdots+X_{n,n}\Longrightarrow \mathcal{N}(0,\sigma^2)$.
\end{theorem}
\begin{proof}
    
\end{proof}

\section{Local Limit Theorems}
Local limit theorems are subtly different from central limit theorems. The story is this:
\begin{example}
    
\end{example}
\begin{definition}[lattice distribution]
    A random variable has a lattice distribution if there are constant $b,h>0$ so that $P(X\in b+h\mathbb{Z})=1$.
    The largest $h$ for which the last statement holds is called the span of the distribution.
\end{definition}
\begin{theorem}
    Let $\varphi(t)=Ee^{itX}$. Regarding to the relationship between $\left|\varphi(t)\right|$ and $1$, there are only three possiblities.\newline 
    (i) $\left|\varphi(t)\right|<1$ for all $t\ne 0$.\newline 
    (ii) There is a $\lambda>0$ so that $\left|\varphi(\lambda)\right|=1$ and $\left|\varphi(t)\right|<1$ for $0<t<\lambda$. In this case,
    $X$ has a lattice distribution with span $\frac{2\pi}{\lambda}$.\newline 
    (iii) $\left|\varphi(t)\right|$ for all $t$. In this case, $X=b$ a.s. for some $b$.
\end{theorem}
\begin{proof}
    
\end{proof}

\begin{theorem}[LLT for the lattice case]
    Let $X_1,X_2,\cdots$ be i.i.d. with $EX_i=0,EX_i^2=\sigma^2\in(0,\infty)$, and having a common lattice distribution with span $h$.
    If $S_n=X_1+\cdots+X_n$ and $P(X_i\in b+h\mathbb{Z})=1$. We put
    \[p_n(x)=P(\frac{S_n}{\sqrt{n}}=x)\text{  for  }x\in\mathcal{L}_n=\left\{\frac{nb+hz}{\sqrt{n}}:z\in\mathbb{Z}\right\}\] 
    and \[n(x)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{x^2}{2\sigma^2}}\] 
    Then as $n\to\infty$,\[\sup_{x\in\mathcal{L}_n}\left|\frac{\sqrt{n}}{h}p_n(x)-n(x)\right|\to 0\]
\end{theorem}
\begin{proof}
    Recall the inversion formula for lattice r.v. $Y$ with $P(Y\in a+\theta \mathbb{Z})=1$ and $\psi(t)=Ee^{itY}$:
    \[P(Y=x)=\frac{\theta}{2\pi}\int_{-\frac{\pi}{\theta}}^{\frac{\pi}{\theta}}e^{-itx}\psi(t)\mathrm{d}t\]
    Use this formula for $\frac{S_n}{\sqrt{n}}$ gives
    \[\frac{\sqrt{n}}{h}p_n(x)=\frac{1}{2\pi}\int_{-\frac{\pi\sqrt{n}}{h}}^{\frac{\pi\sqrt{n}}{h}}e^{-itx}\varphi^n(\frac{t}{\sqrt{n}})\mathrm{d}t\]
    and we have \[n(x)=\frac{1}{2\pi}\int e^{itx}e^{-\frac{\sigma^2t^2}{2}}\]
    Substracting the last two equations and doing some estimation gives 
    \[\left|\frac{\sqrt{n}}{h}p_n(x)-n(x)\right|\le \int_{-\frac{\pi\sqrt{n}}{h}}^{\frac{\pi\sqrt{n}}{h}}\left|\varphi^n(\frac{t}{\sqrt{n}})-e^{-\frac{\sigma^2t^2}{2}}\right|\mathrm{d}t+\int_{\frac{\pi\sqrt{n}}{h}}^\infty e^{-\frac{\sigma^2t^2}{2}}\mathrm{d}t\]
    So we are left to estimate the integrals.
\end{proof}

\section{Poisson Convergence}
\begin{theorem}
    For each $n$ let $X_{n,m}$, $1\le m\le n$ be independent random variables with $P(X_{n,m}=1)=p_{n,m},P(X_{n,m}=0)=1-p_{n,m}$. Suppose \newline 
(i) $\sum_{m=1}^n p_{n,m} \to\lambda\in(0,\infty)$.\newline 
(ii) $\max_{1\le m\le n}\to 0$.\newline 
If $S_n=X_{n,1}+\cdots+X_{n,n}$, then $S_n\Longrightarrow \text{Poisson}(\lambda)$.
\end{theorem}

Here is a second proof of this theorem which provides new insight.
\begin{definition}[total variation distance]
    The total variation distance between two measures on a countable set $S$. $\left\|\mu-\nu\right\|=\frac{1}{2}\sum_z\left|\mu(z)-\nu(z)\right|$.
\end{definition} 

\begin{lemma}
    $\left\|\mu-\nu\right\|=\sup_{A\subset S}\left|\mu(A)-\nu(A)\right|$
\end{lemma}

\begin{lemma}
    $d(\mu,\nu)=\left\|\mu-\nu\right\|$ defines a metric on probability measures on $\mathbb{Z}$. furthermore
\end{lemma}

\begin{lemma}
    Consider measures on $\mathbb{Z}$. Then $\left\|\mu_1\times\mu_2-\nu_1\times\nu_2\right\|\le\left\|\mu_1-\nu_1\right\|+\left\|\mu_2-\nu_2\right\|$.
\end{lemma}
\begin{lemma}
    Consider measures on $\mathbb{Z}$.Then $\left\|\mu_1*\mu_2-\nu_1*\nu_2\right\|\le \left\|\mu_1\times\mu_2-\nu_1\times\nu_2\right\|$.\newline
    Here $*$ stands for the convolution.
\end{lemma}

\section{Stable Laws}
In this section, we will investigate the case $EX_1^2=\infty$ and give necessary and sufficient conditions for the existence of constants $a_n$ and $b_n$ so that
\[\frac{S_n-b_n}{a_n}\Longrightarrow Y\] 
where $Y$ is nondegenerate.

\begin{definition}[slowly varying]
    $L$ is said to be slowly varying if $\lim_{x\to\infty}\frac{L(tx)}{L(x)}=1$ $\forall t>0$.
\end{definition}

\begin{theorem}
    Suppose $X_1,X_2,\cdots$ are i.i.d. with a distribution that satisfies:\newline 
    (i) $\lim_{x\to\infty}\frac{P(X_1>x)}{P(\left|X_1\right|>x)}=\theta\in [0,1]$\newline 
    (ii) $P(\left|X_1\right|>x)=x^{-\alpha}L(x)$ where $\alpha<2$ and $L$ is slowly varying\newline 
    Let $S_n=X_1+\cdots+X_n$, $a_n=\inf\left\{x:P(\left|X_1\right|>x)\le \frac{1}{n}\right\}$ and $b_n=nE(X_11_{(\left|X_1\right|\le a_n)})$.
\end{theorem}

\begin{definition}
    The distributions whose ch.f are given by the following family with parameters $\kappa,\alpha,b,c$ are called stable laws.
    \[\exp{(itc-b\left|t\right|^\alpha(1+i\kappa\text{sgn}(t)w_\alpha(t)))}\] 
    where $\kappa\in [-1,1]$, $\alpha\in (0,2)$,
    \[w_\alpha(t) =\left\{\begin{matrix}
        \tan(\frac{\pi}{2}\alpha )  & \alpha \ne 1\\
         \frac{2}{\pi}\log\left|t\right| &\alpha =1
       \end{matrix}\right.\]
\end{definition}

\begin{theorem}
    $Y$ is the limit of $\frac{X_1+\cdots+X_k-b_k}{a_k}$ for some i.i.d. sequence $X_i$ if and only if $Y$ has a stable law.
\end{theorem}

\section{Infinitely Divisible Distributions}
\begin{definition}
    $Z$ has an infinitely divisible distribution if for each $n$ there is an i.i.d. sequence $Y_{n,1},\cdots,Y_{n,n}$ so that $Z=_d Y_{n,1}+\cdots+Y_{n,n}$.
\end{definition}
\begin{theorem}
    $Z$ is a limit of sums of type $Z=X_{n,1}+\cdots+X_{n,n}$ if and only if $Z$ has an infinitely divisible distribution.
\end{theorem}
\begin{proof}
    
\end{proof}

\begin{theorem}[Levy-Khinchin Theorem]
    $Z$ has an infinitely divisible distribution if and only if its characteristic function has
    \[\log \varphi(t)=ict-\frac{\sigma^2t^2}{2}+\int(e^{itx}-1-\frac{itx}{1+x^2})\mu(\mathrm{d}x)\] 
    where $\mu$ is a measure with $\mu(\left\{0\right\})=0$ and $\int\frac{x^2}{1+x^2}\mu(\mathrm{d}x)<\infty$.
\end{theorem}
The theory of infinitely divisible distributions is simpler in the case of finite variance. In this case, we have:
\begin{theorem}[Kolmogorov's Theorem]
    $Z$ has an infinitely divisible distribution with mean $0$ and finite variance if and only if its ch.f. has the form
    \[\log\varphi(t)=\int \frac{(e^{itx}-1-itx)}{x^2}\nu(\mathrm{d}x)\]
    $\nu$ is called the canonical measure, and $\text{Var}(Z)=\nu(\mathbb{R})$ .
\end{theorem}

\section{\texorpdfstring{Limit Theorems in $\mathbb{R}^d$}{Limit Theorems in Rd}}
\begin{theorem}[Convergence theorem]
    Let $X_n,1\le n\le\infty$ be random vectors with ch.f. $\varphi_n$. A necessary and sufficient condition for $X_n\Longrightarrow X_\infty$ is that $\varphi_n(t)\to\varphi_\infty(t)$.
\end{theorem}


\begin{theorem}[Cramer-Wold Device]
    A sufficient condition for $X_n\Longrightarrow X_\infty$ is that $\theta\cdot X_n\Longrightarrow \theta\cdot X_\infty$ for all $\theta\in\mathbb{R}^d$.
\end{theorem}


\section{Stein's method}
There is a lack of calculus in probability theory. We have only used Fourier transform, i.e. the characteristic function.
Stein invented a exotic way of using calculus to derive the convergence rate of CLT.
\begin{definition}
    
\end{definition}


Stein's method is related to Slepian's interpolation, which is in turn related to Lindeberg's telescopic interpolation.
