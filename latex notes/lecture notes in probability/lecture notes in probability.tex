\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper,total={6in,10in}]{geometry}
\usepackage{amsmath}


\usepackage{amsthm}

\newtheorem{Thm}{Theorem}[section]
\newtheorem{Cor}{Corollary}[Thm]
\newtheorem{Lem}{Lemma}[section]
\newtheorem{Eg}{Example}[section]
\newtheorem*{Rk}{Remark}

\theoremstyle{definition}
\newtheorem{Def}{Definition}[section]


\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{hyperref}



\title{Lecture Notes in Probability}
\author{Kaizhao Liu}
\date{April 2022}

\begin{document}
\maketitle
\tableofcontents
\section{Independence}
\subsection{Basic Definitions}
\begin{Def}[independence: events]
Two events $A,B$ are independent if $P(AB)=P(A)P(B)$.
\end{Def}
\begin{Def}[independence: random variables]
Two random variables $X,Y$ are independent if for all $C,D\in \mathcal{R}$, the events $A=\left \{ X\in C \right \} $ and $B=\left \{ Y\in D \right \} $ are independent.
\end{Def}
\begin{Def}[independence: $\sigma$-fields]
Two $\sigma$-fields $\mathcal{F}$ and $\mathcal{G}$ are independent if for all $A\in\mathcal{F}$ and $B\in\mathcal{G}$, the events $A$ and $B$ are independent.
\end{Def}
Actually, the first definition is a special case of the second, which is a special case of the third. This can be summarized in the following theorem.
\begin{Thm}
\,\par
(i) If $A$ and $B$ are independent, then so are $A^c$ and $B$ , $A$ and $B^c$, and $A^c$ and $B^c$.\par
(ii) Events $A$ and $B$ are independent if and only if $1_A$ and $1_B$ are independent.\par
(iii) If $X$ and $Y$ are independent then $\sigma(X)$ and $\sigma(Y)$ are.\par
(iv) If $\mathcal{F}$ and $\mathcal{G}$ are independent, $X\in\mathcal{F}$, and $Y\in\mathcal{G}$, then $X$ and $Y$ are independent.
\end{Thm}
We can extend this definition in an evident way for finitely many objects. Then, an infinite collection of objects is said to be independent if every finite subcollection is.
\subsection{Sufficient Conditions for Independence}
\begin{Thm}[$\pi$-$\lambda$ theorem]
If $\mathcal{P}$ is a $\pi$-system and $\mathcal{L}$ is a $\lambda$-system that contains $\mathcal{P}$, then $\sigma(\mathcal{P})\subset \mathcal{L}$.
\end{Thm}




\section{LLN}
\subsection{Stochastic Orders}
In calculus, two sequence of real numbers, $\left\{a_n\right\}$ and $\left\{b_n\right\}$, satisfy $a_n=O(b_n)$ if and only if $\left|a_n\right|\le c\left|b_n\right|$ for all $n$ and a constant $c$; and $a_n=o(b_n)$ if and only if $\frac{a_n}{b_n}\to 0$ as $n\to0$.
\begin{Def}
Let $X_1,X_2,\cdots$ be random vectors and $Y_1,Y_2,\cdots$ be random variables defined on a common probability space.\par
(i) $X_n=O(Y_n)$ a.s. if and only if $P(\left \| X_n \right \| =O(\left | Y_n \right | ))=1$.\par
(ii) $X_n=o(Y_n)$ a.s. if and only if $\frac{X_n}{Y_n}\to 0$ a.s..\par
(iii) $X_n=O_p(Y_n)$ if and only if for any $\epsilon>0$, there is a constant $C_\epsilon>0$ s.t. $\sup_nP(\left \| X_n \right \| \ge C_\epsilon \left | Y_n \right | )<\epsilon$.\par
(iv) $X_n=o_p(Y_n)$ if and only if $\frac{X_n}{Y_n}\to_p0$.
\end{Def}
\subsection{WLLN}
In this section we study convergence in probability and the laws of large numbers associated with this type of convergence.
\begin{Lem}[moments and tails]
    Let $\xi>0$ be an random variable with $E\xi\in (0,\infty)$. Then 
    \[(1-r)^2\frac{(E\xi)^2}{E\xi^2}\le P(\xi>rE\xi)\le \frac{1}{r},\quad r>0\]
\end{Lem}
\begin{Thm}[convergence in $L^p$ implies convergence in probability]
If $p>0$, then \[E\left | Z_n \right | ^p\to 0 \Longrightarrow Z_n\longrightarrow 0\text{ in probability.}\]
\end{Thm}
\begin{proof}
$P(\left | Z_n \right |\ge\epsilon)\le\frac{E\left | Z_n \right | ^p}{\epsilon^p}\to 0$
\end{proof}
\begin{Thm}[$L^2$ weak law]
Let $X_1,X_2,...,$ be uncorrelated random variables with $EX_i=\mu$ and $var(X_i)<C<\infty$. If $S_n=X_1+\cdots+X_n$, then as $n\to\infty$, $\frac{S_n}{n}\longrightarrow \mu$ in $L^2$.
\end{Thm}
\begin{proof}
$E(\frac{S_n}{n}-\mu)^2=\text{var}(\frac{S_n}{n})=\frac{1}{n^2}(\sum\text{var}(X_i))\le \frac{Cn}{n^2}\to 0$
\end{proof}

\subsection{Borel-Cantelli Lemmas}
Borel-Cantelli lemmas are the ladders from convergence in probability to a.s. convergence if the sequence of events are not decreasing.
If the sequence of events are decreasing, then convergence in probability is the same as a.s. convergence, and there is no need for Borel-Cantelli lemma.
\begin{Thm}[Borel-Cantelli lemma]
$\sum_{n=1}^\infty P(A_n)<\infty\Longrightarrow P(A_n\text{ i.o.})=0$.
\end{Thm}
\begin{proof}
Let $N=\sum_{n=1}^\infty 1_{A_n}$. $EN<\infty$ implies $N<\infty$ a.s.
\end{proof}
\begin{Thm}[The second Borel-Cantelli lemma]\label{The second Borel-Cantelli lemma}
If the events $A_n$ are independent, then \[\sum_{n=1}^\infty P(A_n)=\infty\Longrightarrow P(A_n\text{ i.o.})=1\]
\end{Thm}
\begin{proof}
Let $M<N<\infty$. $1-x\le e^{-x}$ and independence imply $P(\bigcap_{n=M}^{N}A_n^c)=\prod_{n=M}^N(1-P(A_n))\ge \text{exp}(-\sum_{n=M}^NP(A_n))\to 0$ as $N\to\infty$, so $P(\bigcup_{n=M}^{N}A_n)=1,\forall M$. Therefore $P(\limsup A_n)=1$.
\end{proof}
\begin{Thm}[Kochen-Stone lemma]
Suppose $\sum_{n=1}^\infty P(A_n)=\infty$. If 
\[\limsup_{n\to\infty}\frac{(\sum_{k=1}^nP(A_k))^2}{(\sum_{1\le i,j\le n} P(A_i\cap A_k))}=\alpha>0\] then $P(A_n\text{ i.o.})\ge\alpha$.
\end{Thm}
\begin{Rk}
This is a generalization of \ref{The second Borel-Cantelli lemma}.
\end{Rk}
\begin{Thm}
If $A_1,A_2,...$ are pairwise independent and $\sum_{n=1}^\infty P(A_n)<\infty$, then $\heartsuit$
\end{Thm}
\subsection{SLLN}
\subsection{0-1 Laws}
\begin{Thm}[Kolmogorov's 0-1 law]
    If $X_1,X_2,\cdots$ are independent and $A\in\mathcal{T}$, then $P(A)=0$ or $1$.
\end{Thm}
\begin{proof}
    The key point is to show that $A$ is independent of itself.\par
    To show this, we can procede by two limiting steps.
\end{proof}
\begin{Thm}[Hewitt-Savage 0-1 law]
    If $X_1,X_2,\cdots$ are i.i.d. and $A\in\mathcal{E}$, then $P(A)=0$ or $1$.
\end{Thm}
\begin{Lem}
    
\end{Lem}

\subsection{Convergence of Random Series}

\begin{Thm}[Kolmogorov's maximal inequality]
    Suppose $X_1,\cdots,X_n$ are independent with $EX_i=0$ and $\text{Var}(X_i)<\infty$. If $S_n=X_1+\cdots+X_n$, then
    \[P(\max_{1\le k\le n}\left|S_k\right|\ge x)\le \frac{\text{Var}(S_n)}{x^2}\] 
\end{Thm}
\begin{proof}
    There is a proof by Doob's inequality.
\end{proof}

\begin{Thm}
    Suppose $X_1,X_2,\cdots$ are independent and have $EX_n=0$. If \[\sum_{n=1}^\infty \text{Var}(X_n)<\infty\] 
    then with probability one $\sum_{n=1}^\infty X_n(\omega)$ converges.
\end{Thm}
\begin{proof}
    Let $S_N=\sum_{n=1}^N X_n$. From Kolmogorov's maximal inequality, we get 
    \[P(\max_{M\le m\le N}\left|S_m-S_M\right|>\epsilon)\le \epsilon^{-2}\text{Var}(S_N-S_M)=\epsilon^{-2}\sum_{n=M+1}^N\text{Var}(X_n)\] 
    Letting $N\to\infty$, we get \[P(\sup_{M\le m}\left|S_m-S_M\right|>\epsilon)\le \epsilon^{-2}\sum_{n=M+1}^\infty\text{Var}(X_n)\] 
    If we let $w_M=\sup_{m,n\ge M}\left|S_m-S_n\right|$, then \[P(w_M>2\epsilon)\le P(\sup_{M\le m}\left|S_m-S_M\right|>\epsilon)\to 0\quad\text{as }M\to\infty\]
    As $w_M$ decreases as $M$ increases, $w_M\downarrow 0$ a.s.. But $w_M(\omega)\downarrow 0$ implies $S_n(\omega)$ is a Cauchy sequence
    and hence $\lim_{n\to\infty}S_n(\omega)$ exists.
\end{proof}

\begin{Thm}[Kolmogorov's three series theorem]
    Let $X_1,X_2,\cdots$ be independent. Let $A>0$ and let $Y_i=X_i1_{\left|X_i\right|\le A}$. In order that $\sum_{n=1}^\infty X_n$ converges a.s.,
    it is necessary and sufficient that:\newline 
    (i) $\sum_{n=1}^\infty P(\left|X_n\right|>A)<\infty$\newline 
    (ii) $\sum_{n=1}^\infty EY_n$ converges \newline 
    (iii) $\sum_{n=1}^\infty \text{Var}(Y_n)<\infty$
\end{Thm}
\begin{proof}
    To prove sufficiency, let $\mu_n=EY_n$. By the above theorem, $\sum_{n=1}^\infty(Y_n-\mu_n)$ converges a.s..
    Using (ii), $\sum_{n=1}^\infty Y_n$ converges a.s.. (i) and Borel-Cantelli lemma imply $P(X_n\ne Y_n\, i.o.)=0$, so $\sum_{n=1}^\infty X_n$ converges a.s..
    \newline 
    For necessity, if the sum of (i) is infinite, $P(\left|X_n\right|>A\, i.o.)>0$ and $\lim_{m\to\infty} \sum_{n=1}^m X_n$ can not converge.
    Suppose next (i) is finite but the sum 
\end{proof}

One of the advantage of the random series proof is that it provides estimates on the rate of convergence.
\begin{Thm}
    Let $X_1,X_2,\cdots$ be i.i.d. random variables with $EX_i=0$ and $EX_i^2=\sigma^2<\infty$. Let $S_n=X_1+\cdots+X_n$. If $\epsilon>0$ then 
    \[\frac{S_n}{\sqrt{n(\log n)^{1+\epsilon}}}\to 0\text{ a.s. }\]
\end{Thm}

The next result, show that when $E\left|X_1\right|=\infty$, $\frac{S_n}{a_n}$ cannot converge almost surely to a nonzero limit.
\begin{Thm}
    Let $X_1,X_2,\cdots$ be i.i.d. with $E\left|X_1\right|=\infty$ and let $S_n=X_1+\cdots+X_n$.
    Let $a_n$ be a sequence of positive numbers with $\frac{a_n}{n}$ increasing
\end{Thm}

\subsection{Large Deviations}
Let $X_1,X_2,\cdots$ be i.i.d. and let $S_n=X_1+\cdots+X_n$. We will investigate the rate at which $P(S_n\ge na)\to 0$ for $x>\mu=EX_i$.
\begin{Lem}
If $\gamma_{m+n}\ge\gamma_m+\gamma_n$, then as $n\to\infty$, $\frac{\gamma_n}{n}\to\sup_m\frac{\gamma_m}{m}$.
\end{Lem}
\begin{Thm}
$\gamma(x)=\lim_{n\to\infty}\frac{\log P(S_n\ge nx)}{n}$ exists $\le 0$.
\end{Thm}
\begin{proof}
Let $\pi_n=P(S_n\ge nx)$, then $\pi_{m+n}\ge P(S_m\ge mx,S_{n+m}-S_m\ge nx)=\pi_m\pi_n$. Therefore, letting $\gamma_n=\log\pi_n$, from the lemma we conclude the existence of the limit.
\end{proof}

Next we want to determine the limit function $h(x)$. To do this, we need to introduce the cumulant-generating function of a random varaible $\xi$.
\[\phi(t)=\log E e^{t\xi},\quad t\in \mathbb{R}\]
and the Legendre transform of $\phi$, given by 
\[\phi^*(x)=\sup_{t\in \mathbb{R}}(tx-\phi(t)),\quad x\in\mathbb{R}\]
\begin{Lem}
    $\phi(t)$ and $\phi^*(x)$ are convex.
\end{Lem}
\begin{proof}
    The convexity of $\phi(t)$ comes from Holder's inequality, and the convexity for $\phi^*(x)$ is a property of Legendre transform.
\end{proof}
\section{CLT}
\subsection{Distributions}
\begin{Def}[distribution]
If $X$ is a random variable, then $X$ induces a probability measure on $\mathbb{R}$ called its \textbf{distribution}.
\end{Def}
\begin{Rk}
The distribution of a random variable $X$ is usually described by giving its \textbf{distribution function} $F(x)=P(X\le x)$.
\end{Rk}
\begin{Thm}[properties of distribution fucntions]\label{properties of distribution fucntions}
\,\par
(i) $F$ is nondecreasing\par
(ii) $\lim_{x\to-\infty}F(x)=0, \lim_{x\to\infty}F(x)=1$\par
(iii) $F$ is right continuous\par
(iv) If $F(x-)=\lim_{y\to x^-}$, then $F(x-)=P(X< x)$\par
(v) $P(X=x)=F(x)-F(x-)$
\end{Thm}
\begin{proof}
Directly follows from the definitions and the inclusion of sets.
\end{proof}
\begin{Thm}\label{r.v. constructed from distribution}
If $F$ satisfies (i),(ii),(iii) in \ref{properties of distribution fucntions}, then it is the distribution function of some random variable.
\end{Thm}
\begin{proof}
Let $\Omega=(0,1),\mathcal{F}$=the Borel sets, and $P$=Lebesgue measure. If $\omega\in(0,1)$, construct \[X(\omega)=\sup\left \{ y:F(y)<\omega \right \} \]
We need to show:\[\left \{ \omega:X(\omega)\le x \right \} =\left \{ \omega:\omega\le F(x) \right \} \]\par
For $\left \{ \omega:X(\omega)\le x \right \} \supseteq\left \{ \omega:\omega\le F(x) \right \}$, observe if $\omega \le F(x)$, then $X(\omega)\le x$.\par
For $\left \{ \omega:X(\omega)\le x \right \} \subseteq\left \{ \omega:\omega\le F(x) \right \}$, observe if $\omega>F(x)$, then since $F$ is right continuous, $\exists \epsilon>0$ s.t. $F(x+\epsilon)<\omega$. Therefore, $X(\omega)\ge x+\epsilon>x$.
\end{proof}

\subsection{weak convergence}
\begin{Def}[weak convergence: distribution functions]
A sequence of distribution functions $F_n$ is said to \textbf{converge weakly} to a limit $F$ if $F_n(y)\to F(y)$ for all $y$ that are continuity points of $F$. 
\end{Def}
\begin{Rk}
Denoted by $F_n\Longrightarrow F$.
\end{Rk}
\begin{Def}[weak convergence: random variable]
A sequence of random variables $X_n$ is said to \textbf{converge weakly (converge in distribution)} to a limit $X_\infty$ if their distribution functions converge weakly.
\end{Def}
\begin{Rk}
Denoted by $X_n\Longrightarrow X_\infty$.
\end{Rk}
\begin{Thm}[Skorokhod]\label{Skorokhod}
If $F_n\Longrightarrow F_\infty$, then $\exists \text{ r.v. } Y_n$ with distribution $F_n$ s.t. $Y_n\longrightarrow Y_\infty$ a.s.
\end{Thm}
\begin{proof}
As in the proof of \ref{r.v. constructed from distribution}, let $\Omega=(0,1),\mathcal{F}$=the Borel sets, and $P$=Lebesgue measure. If $\omega\in(0,1)$, construct \[Y_n(\omega)=\sup\left \{ y:F_n(y)<\omega \right \} \]
\par We want to show: \[Y_n(x)\longrightarrow Y_\infty(x)\] for all but a countable number of $x$.
\par We begin by identifying the exceptional set. Let $a_x=\sup\left \{ y:F_\infty(y)<x \right \}, b_x=\inf\left \{ y:F_\infty(y)>x \right \}$, and $\Omega_0=\left \{ x:(a_x,b_x)=\emptyset \right \}$. Then $\Omega-\Omega_0$ is countable. If $x\in \Omega_0$, then $F_\infty(y)<x$ for $y<Y_\infty(x)$ and $F_\infty(y)>x$ for $y> Y_\infty(x)$.\par
Now we show $\liminf_{n\to\infty} Y_n(x)\ge Y_\infty(x)$. Choose $y<Y_\infty(x)$ s.t. $F_\infty$ is continuous at $y$.  Then  $F_\infty(y)<x$ and $F_n(y)\longrightarrow F_\infty(y)$, so $F_n(y)< x$ for $n$ sufficient large, that is, $Y_n(x)\ge y$. This is true for all such $y$'s so the result follows.\par
The reverse inequality $\limsup_{n\to\infty} Y_n(x)\le Y_\infty(x)$ is true by symmetry.
\end{proof}
\begin{Thm}
$X_n\Longrightarrow X_\infty\Longleftrightarrow\forall\text{ bounded continuous function }g, Eg(X_n)\longrightarrow Eg(X_\infty)$
\end{Thm}
\begin{proof}
$\Longrightarrow$: By \ref{Skorokhod}, let $Y_n$ have the same distribution as $X_n$ and converge a.s. Since $g$ is continuous, $g(Y_n)\longrightarrow g(Y_\infty)$ a.s. so by the bounded convergence theorem $Eg(X_n)\longrightarrow Eg(X_\infty)$.\par
$\Longleftarrow$: construct a bounded and continuous function\[g_{x,\epsilon}(y)=\left\{\begin{matrix}
 1 &y\le x \\
 0 & y\ge x+\epsilon\\
 \text{linear} & x< y< x+\epsilon
\end{matrix}\right.\]
Therefore, $\limsup_{n\to\infty} P(X_n\le x)\le \limsup_{n\to\infty} Eg_{x,\epsilon}(X_n)=Eg_{x,\epsilon}(X_\infty)\le P(X_\infty\le x+\epsilon)$. Letting $\epsilon\to 0$ gives $\limsup_{n\to\infty}P(X_n\le x)\le P(X_\infty\le x)$. The reverse inequality can be proved in the same way.
\end{proof}
\begin{Thm}[continuous mapping theorem]

\end{Thm}

\begin{Thm}
TFAE:\par
(i) $X_n\Longrightarrow X_\infty$\par
(ii) For all open sets $G$, $\liminf_{n\to\infty}P(X_n\in G)\ge P(X_\infty\in G)$\par
(iii) For all closed sets $K$, $\limsup_{n\to\infty}P(X_n\in K)\le P(X_\infty\in K)$\par
(iv) For all Borel sets $A$ with $P(X_\infty\in\partial A)=0$, $\lim_{n\to\infty}P(X_n\in A)=P(X_\infty\in A)$
\end{Thm}

\begin{Thm}[Helly's selection theorem]
For every sequence $F_n$ of distribution functions, there is a subsequence $F_{n(k)}$ and a right continuous nondecreasing function $F$ s.t. $F_{n(k)}\Longrightarrow_v F$.
\end{Thm}
\begin{Rk}
The limit may not be a distribution function. This type of convergence is called vague convergence.
\end{Rk}
\begin{proof}
To construct the function $F$, we adopt the standard diagonal argument. Let $\left \{ q_i \right \} $ be an enumeration of the rationals. Since $F_m(q_k)\in [0,1]$ is bounded for all $m$, there is a subsubsequence $m_k(i)$ that is a subsequence of $m_{k-1}(i)$ s.t. $F_{m_k(i)}(q_k)\longrightarrow G(q_k)$. Select the diagonal sequence $n(k)=m_k(k)$, then by construction, $F_{n(k)}(q)\longrightarrow G(q)$ for all rational $q$.\par
Now we need to consruct $F$ from $G$. Let \[F(x)=\inf \left \{ G(q):q\in \mathbb{Q},q>x \right \} \]then $F(x)$ is right continuous and nondecreasing.\par
Let $x$ be a continuity point of $F$. Pick rational $s>x$ s.t. $F(x)\le F(s)< F(x)+\epsilon$, then as $F_{n(k)}(s)\longrightarrow G(s)\le F(s)$, for $k$ sufficient large, we have $F_{n(k)}(x)\le F_{n(k)}(s)<F(x)+\epsilon$. On the other hand, pick rational $r_1<r_2<x$ s.t. $F(x)-\epsilon<F(r_1)\le F(r_2)\le F(x)$, then as $F_{n(k)}(r_2)\longrightarrow G(r_2)\ge F(r_1)$, so $F_{n(k)}(x)\ge F_{n(k)}(r_2)>F(x)-\epsilon$ for $k$ sufficient large. Thus as $\epsilon\to 0$, we have the weak convergence.
\end{proof}
\begin{Thm}
Every subsequential limit is the distribution function of a probability measure $\Longleftrightarrow$ the sequence is \textbf{tight}, i.e. $\forall \epsilon>0,\exists M_\epsilon$ s.t.
\[\limsup_{n\to\infty} 1-F_n(M_\epsilon)+F_n(-M_\epsilon)\le \epsilon\]
\end{Thm}
\begin{proof}
First note that for vague convergence $0\le F(x)\le 1$.\par
$\Longleftarrow$: Suppose the sequence is tight and $F_{n(k)}\Longrightarrow_v F$. Let $r<-M_\epsilon,s>M_\epsilon$ be continuity points of $F$, then $1-F(s)+F(r)=\lim_{k\to\infty}1-F_{n(k)}(s)+F_{n(k)}(r)\le \limsup_{n\to\infty}1-F_n(M_\epsilon)+F_n(M_\epsilon)\le \epsilon$. Letting $r\to-\infty$ and $s\to\infty$ gives $\limsup_{n\to\infty}1-F(x)+F(-x)\le \epsilon$.\par
$\Longrightarrow$: Suppose $F_n$ is not tight. Then there is an $\epsilon>0$ and a subsequence $n(k)\to\infty$ s.t. \[1-F_{n(k)}(k)+F_{n(k)}(-k)\ge\epsilon\] for all $k$. By passing to a further subsequence $F_{n(k_j)}$ we can suppose $F_{n(k_j)}\Longrightarrow_v F$. Let $r<0<s$ be continuity points of $F$. Then $1-F(s)+F(r)=\lim_{j\to\infty} 1-F_{n(k_j)}(s)+F_{n(k_j)}(r)\ge\liminf_{j\to\infty}1-F_{n(k_j)}(k_j)+F_{n(k_j)}(-k_j)\ge\epsilon$. Letting $s\to\infty$ and $r\to-\infty$, we see that $F$ is not the distribution function of a probability measure.
\end{proof}
\begin{Cor}
If there is a $\varphi\ge0$ s.t. $\varphi(x)\to\infty$ as $\left | x \right | \to\infty$ and 
\[\sup_n\int\varphi(x)\mathrm{d}F_n(x)=C<\infty\] then $F_n$ is tight.
\end{Cor}
\begin{proof}
$C\ge\int\varphi(x)\mathrm{d}F_n(x)\ge\inf_{\left | x \right |\ge M}\varphi(x)(F_n(-M)+1-F_n(M))$
\end{proof}
\begin{Lem}
If $X_n\longrightarrow X$ in probability, then $X_n\Longrightarrow X$. Conversely, if $X_n\Longrightarrow c$ where $c$ is a constant, then $X_n\longrightarrow c$ in probability.
\end{Lem}
\begin{Thm}[slutsky]
If $X_n\Longrightarrow X$ and $Y_n\Longrightarrow c$, where $c$ is a constant, then:\par
(i) $X_n+Y_n\Longrightarrow X+c$\par
(ii) $X_nY_n\Longrightarrow cX$
\end{Thm}

\subsection{Characteristic Functions}
\begin{Def}[characteristic function]
    If $X$ is a random variable, we define its characteristic function by $\varphi(t)=Ee^{itX}$.
\end{Def}
\begin{Thm}[properties of ch.f.]
    All ch.f.s have the following properties:\newline 
    (i) $\varphi(0)=1$\newline 
    (ii) $\varphi(-t)=\overline{\varphi(t)}$\newline 
    (iii) $\left|\varphi(t)\right|\le 1$\newline 
    (iv) $\varphi(t)$ is uniformly continuous on $(-\infty,\infty)$\newline 
    (v) $Ee^{it(aX+b)}=e^{itb}\varphi(at)$
\end{Thm}
\begin{Thm}
    If $X_1$ and $X_2$ are independent and have ch.f.'s $\varphi_1$ and $\varphi_2$, then $X_1+X_2$ has ch.f. $\varphi_1(t)\varphi_2(t)$.
\end{Thm}
\begin{Lem}
    If $F_1,\cdots,F_n$ have ch.f. $\varphi_1,\cdots,\varphi_n$ and $\lambda_i\ge 0$ have $\lambda_1+\cdots+\lambda_n=1$, then $\sum_{i=1}^n\lambda_iF_i$ has ch.f. $\sum_{i=1}^n\lambda_i\varphi_i$.
\end{Lem}


\begin{Thm}[Continuity theorem]
    Let $\mu_n$, $1\le n\le \infty$ be probability measures with ch.f. $\varphi_n$.\newline 
    (i) If $\mu_n\Longrightarrow \mu_\infty$, then $\varphi_n(t)\to\varphi_\infty(t)$ for all $t$.\newline 
    (ii) If $\varphi_n(t)$ converges pointwise to a limit $\varphi(t)$ that is continuous at $0$,
    then the associated sequence of distributions $\mu_n$ is tight and converges weakly to the measure $\mu$ with characteristic function $\varphi$.
\end{Thm}

The next result is useful for constructing examples of ch.f.'s.
\begin{Eg}[Polya's distribution]
    \begin{align*}
        \text{Density}\quad& \frac{1-\cos(x)}{\pi x^2}   \\
        \text{Ch.f.}\quad& (1-\left|t\right|)^+
    \end{align*}

\end{Eg}
\begin{Thm}[Polya's criterion]
    Let $\varphi(t)$ be real nonnegative and have $\varphi(0)=1$, $\varphi(t)=\varphi(-t)$, and $\varphi$ is decreasing and convex on $(0,\infty)$ with 
    $\lim_{t\downarrow 0}\varphi(t)=1,\lim_{t\uparrow\infty}\varphi(t)=0$. Then there is a probability measure $\nu$ on $(0,\infty)$, so that
    \[\varphi(t)=\int_0^\infty (1-\left|\frac{t}{s}\right|^+)\nu(\mathrm{d}s)\] 
    and hence $\varphi$ is a characteristic function.
\end{Thm}

\subsection{The Moment Problem}
\begin{Eg}[Heyde(1963)]
    Consider the lognormal density \[f_0(x)=\frac{1}{\sqrt{(2\pi)}}\frac{1}{x}\exp^{-\frac{(\log x)^2}{2}}1_{x\ge 0}\] 
    and for $-1\le a\le 1$ let \[f_a(x)=f_0(x)(1+a\sin(2\pi\log x))\]
    We claim that $f_a$ is a density and has the same moment as $f_0$
\end{Eg}
\begin{Eg}
    
\end{Eg}
A usual sufficient condition for a distribution to be determined by its moments is:
\begin{Thm}
    If $\limsup_{n\to\infty}\frac{\mu_{2n}^{\frac{1}{2n}}}{2n}=r<\infty$, then there is at most one d.f. $F$ with
    $\mu_n=\int x^n\mathrm{d}F(x)$ for all positive integers $n$.
\end{Thm}
\begin{proof}
    First we explain why the condition only consider $2n$. Let $F$ be any d.f. with the moment $\mu_n$ and let $\nu_n=\int\left|x\right|^n\mathrm{d}F(x)$.
    The Cauchy-Schwarz inequality implies $\nu_{2n+1}^2\le \mu_{2n}\mu_{2n+2}$, so \[\limsup_{n\to\infty}\frac{\nu_{n}^{\frac{1}{n}}}{n}=r<\infty\]
    Next, we have \[\left|e^{i\theta X}(e^{itX}-\sum_{m=0}^{n-1}\frac{(itX)^m}{m!})\right|\le \frac{\left|tX\right|^n}{n!}\]
    Taking expected value, we have \[\left|\varphi(\theta+t)-\varphi(\theta)-t\varphi'(\theta)-\cdots-\frac{t^{n-1}}{(n-1)!}\varphi^{(n-1)}(\theta)\right|\le \frac{\left|t\right|^n}{n!}\nu_n\]
    So we see that for any $\theta$, \[\varphi(\theta+t)=\varphi(\theta)+\sum_{m=1}^\infty\frac{t^m}{m!}\varphi^{(m)}(\theta)\quad\forall\left|t\right|<\frac{1}{er}\]
    Let $G$ be another distribution with the given moments and $\psi$ its ch.f.. Since $\psi(0)=\varphi(0)=1$, it follows from the above equation and induction that $\psi(t)=\varphi(t)$ for $\left|t\right|\le \frac{k}{3r}$ for all $k$,
    so the two ch.f. coincide and the distributions are equal.
\end{proof}
Here is an application.
\begin{Thm}[Semi-Circle Law]
    
\end{Thm}

\subsection{Central Limit Theorems}
\begin{Thm}
    Let $X_1,X_2,\cdots$ be i.i.d. with $EX_i=\mu$, $\text{Var}(X_i)=\sigma^2\in(0,\infty)$. If $S_n=X_1+\cdots+X_n$, then
    \[\frac{S_n-n\mu}{\sigma \sqrt{n}}\Longrightarrow \mathcal{N}(0,1)\] 
\end{Thm}
\begin{proof}
    WLOG suppose $\mu=0$. $\varphi(t)=Ee^{itX_1}=1-\frac{\sigma^2t^2}{2}+o(t^2)$, so $Ee^{itS_n/\sigma n^{\frac{1}{2}}}=(1-\frac{t^2}{2n}+o(\frac{1}{n}))^n$.
    The last quantity $\to e^{-\frac{t^2}{2}}$ as $n\to\infty$, and the conclusion follows from the continuity theorem.
\end{proof}

\begin{Thm}[Lindeberg-Feller theorem]
    For each $n$, let $X_{n,m},1\le m\le n$ be independent random variables with $EX_{n,m}=0$. Suppose \newline 
    (i) $\sum_{m=1}^n EX_{n,m}^2\to \sigma^2>0$ \newline 
    (ii) $\forall \epsilon>0$, $\lim_{n\to\infty}\sum_{m=1}^nE(\left|X_{n,m}\right|^2;\left|X_{n,m}\right|>\epsilon)=0$ \newline 
    Then $S_n=X_{n,1}+\cdots+X_{n,n}\Longrightarrow \mathcal{N}(0,\sigma^2)$.
\end{Thm}
\begin{proof}
    
\end{proof}

\subsection{Local Limit Theorems}
Local limit theorems are subtly different from central limit theorems. The story is this:
\begin{Eg}
    
\end{Eg}
\begin{Def}[lattice distribution]
    A random variable has a lattice distribution if there are constant $b,h>0$ so that $P(X\in b+h\mathbb{Z})=1$.
    The largest $h$ for which the last statement holds is called the span of the distribution.
\end{Def}
\begin{Thm}
    Let $\varphi(t)=Ee^{itX}$. Regarding to the relationship between $\left|\varphi(t)\right|$ and $1$, there are only three possiblities.\newline 
    (i) $\left|\varphi(t)\right|<1$ for all $t\ne 0$.\newline 
    (ii) There is a $\lambda>0$ so that $\left|\varphi(\lambda)\right|=1$ and $\left|\varphi(t)\right|<1$ for $0<t<\lambda$. In this case,
    $X$ has a lattice distribution with span $\frac{2\pi}{\lambda}$.\newline 
    (iii) $\left|\varphi(t)\right|$ for all $t$. In this case, $X=b$ a.s. for some $b$.
\end{Thm}
\begin{proof}
    
\end{proof}

\begin{Thm}[LLT for the lattice case]
    Let $X_1,X_2,\cdots$ be i.i.d. with $EX_i=0,EX_i^2=\sigma^2\in(0,\infty)$, and having a common lattice distribution with span $h$.
    If $S_n=X_1+\cdots+X_n$ and $P(X_i\in b+h\mathbb{Z})=1$. We put
    \[p_n(x)=P(\frac{S_n}{\sqrt{n}}=x)\text{  for  }x\in\mathcal{L}_n=\left\{\frac{nb+hz}{\sqrt{n}}:z\in\mathbb{Z}\right\}\] 
    and \[n(x)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{x^2}{2\sigma^2}}\] 
    Then as $n\to\infty$,\[\sup_{x\in\mathcal{L}_n}\left|\frac{\sqrt{n}}{h}p_n(x)-n(x)\right|\to 0\]
\end{Thm}
\begin{proof}
    Recall the inversion formula for lattice r.v. $Y$ with $P(Y\in a+\theta \mathbb{Z})=1$ and $\psi(t)=Ee^{itY}$:
    \[P(Y=x)=\frac{\theta}{2\pi}\int_{-\frac{\pi}{\theta}}^{\frac{\pi}{\theta}}e^{-itx}\psi(t)\mathrm{d}t\]
    Use this formula for $\frac{S_n}{\sqrt{n}}$ gives
    \[\frac{\sqrt{n}}{h}p_n(x)=\frac{1}{2\pi}\int_{-\frac{\pi\sqrt{n}}{h}}^{\frac{\pi\sqrt{n}}{h}}e^{-itx}\varphi^n(\frac{t}{\sqrt{n}})\mathrm{d}t\]
    and we have \[n(x)=\frac{1}{2\pi}\int e^{itx}e^{-\frac{\sigma^2t^2}{2}}\]
    Substracting the last two equations and doing some estimation gives 
    \[\left|\frac{\sqrt{n}}{h}p_n(x)-n(x)\right|\le \int_{-\frac{\pi\sqrt{n}}{h}}^{\frac{\pi\sqrt{n}}{h}}\left|\varphi^n(\frac{t}{\sqrt{n}})-e^{-\frac{\sigma^2t^2}{2}}\right|\mathrm{d}t+\int_{\frac{\pi\sqrt{n}}{h}}^\infty e^{-\frac{\sigma^2t^2}{2}}\mathrm{d}t\]
    So we are left to estimate the integrals.
\end{proof}

\subsection{Poisson Convergence}
\begin{Thm}
    For each $n$ let $X_{n,m}$, $1\le m\le n$ be independent random variables with $P(X_{n,m}=1)=p_{n,m},P(X_{n,m}=0)=1-p_{n,m}$. Suppose \newline 
(i) $\sum_{m=1}^n p_{n,m} \to\lambda\in(0,\infty)$.\newline 
(ii) $\max_{1\le m\le n}\to 0$.\newline 
If $S_n=X_{n,1}+\cdots+X_{n,n}$, then $S_n\Longrightarrow \text{Poisson}(\lambda)$.
\end{Thm}

Here is a second proof of this theorem which provides new insight.
\begin{Def}[total variation distance]
    The total variation distance between two measures on a countable set $S$. $\left\|\mu-\nu\right\|=\frac{1}{2}\sum_z\left|\mu(z)-\nu(z)\right|$.
\end{Def} 

\begin{Lem}
    $\left\|\mu-\nu\right\|=\sup_{A\subset S}\left|\mu(A)-\nu(A)\right|$
\end{Lem}

\begin{Lem}
    $d(\mu,\nu)=\left\|\mu-\nu\right\|$ defines a metric on probability measures on $\mathbb{Z}$. furthermore
\end{Lem}

\begin{Lem}
    Consider measures on $\mathbb{Z}$. Then $\left\|\mu_1\times\mu_2-\nu_1\times\nu_2\right\|\le\left\|\mu_1-\nu_1\right\|+\left\|\mu_2-\nu_2\right\|$.
\end{Lem}
\begin{Lem}
    Consider measures on $\mathbb{Z}$.Then $\left\|\mu_1*\mu_2-\nu_1*\nu_2\right\|\le \left\|\mu_1\times\mu_2-\nu_1\times\nu_2\right\|$.\newline
    Here $*$ stands for the convolution.
\end{Lem}

\subsection{Stable Laws}
In this section, we will investigate the case $EX_1^2=\infty$ and give necessary and sufficient conditions for the existence of constants $a_n$ and $b_n$ so that
\[\frac{S_n-b_n}{a_n}\Longrightarrow Y\] 
where $Y$ is nondegenerate.

\begin{Def}[slowly varying]
    $L$ is said to be slowly varying if $\lim_{x\to\infty}\frac{L(tx)}{L(x)}=1$ $\forall t>0$.
\end{Def}

\begin{Thm}
    Suppose $X_1,X_2,\cdots$ are i.i.d. with a distribution that satisfies:\newline 
    (i) $\lim_{x\to\infty}\frac{P(X_1>x)}{P(\left|X_1\right|>x)}=\theta\in [0,1]$\newline 
    (ii) $P(\left|X_1\right|>x)=x^{-\alpha}L(x)$ where $\alpha<2$ and $L$ is slowly varying\newline 
    Let $S_n=X_1+\cdots+X_n$, $a_n=\inf\left\{x:P(\left|X_1\right|>x)\le \frac{1}{n}\right\}$ and $b_n=nE(X_11_{(\left|X_1\right|\le a_n)})$.
\end{Thm}

\begin{Def}
    The distributions whose ch.f are given by the following family with parameters $\kappa,\alpha,b,c$ are called stable laws.
    \[\exp{(itc-b\left|t\right|^\alpha(1+i\kappa\text{sgn}(t)w_\alpha(t)))}\] 
    where $\kappa\in [-1,1]$, $\alpha\in (0,2)$,
    \[w_\alpha(t) =\left\{\begin{matrix}
        \tan(\frac{\pi}{2}\alpha )  & \alpha \ne 1\\
         \frac{2}{\pi}\log\left|t\right| &\alpha =1
       \end{matrix}\right.\]
\end{Def}

\begin{Thm}
    $Y$ is the limit of $\frac{X_1+\cdots+X_k-b_k}{a_k}$ for some i.i.d. sequence $X_i$ if and only if $Y$ has a stable law.
\end{Thm}

\subsection{Infinitely Divisible Distributions}
\begin{Def}
    $Z$ has an infinitely divisible distribution if for each $n$ there is an i.i.d. sequence $Y_{n,1},\cdots,Y_{n,n}$ so that $Z=_d Y_{n,1}+\cdots+Y_{n,n}$.
\end{Def}
\begin{Thm}
    $Z$ is a limit of sums of type $Z=X_{n,1}+\cdots+X_{n,n}$ if and only if $Z$ has an infinitely divisible distribution.
\end{Thm}
\begin{proof}
    
\end{proof}

\begin{Thm}[Levy-Khinchin Theorem]
    $Z$ has an infinitely divisible distribution if and only if its characteristic function has
    \[\log \varphi(t)=ict-\frac{\sigma^2t^2}{2}+\int(e^{itx}-1-\frac{itx}{1+x^2})\mu(\mathrm{d}x)\] 
    where $\mu$ is a measure with $\mu(\left\{0\right\})=0$ and $\int\frac{x^2}{1+x^2}\mu(\mathrm{d}x)<\infty$.
\end{Thm}
The theory of infinitely divisible distributions is simpler in the case of finite variance. In this case, we have:
\begin{Thm}[Kolmogorov's Theorem]
    $Z$ has an infinitely divisible distribution with mean $0$ and finite variance if and only if its ch.f. has the form
    \[\log\varphi(t)=\int \frac{(e^{itx}-1-itx)}{x^2}\nu(\mathrm{d}x)\]
    $\nu$ is called the canonical measure, and $\text{Var}(Z)=\nu(\mathbb{R})$ .
\end{Thm}

\subsection{Limit Theorems in $\mathbb{R}^d$}
\begin{Thm}[Convergence theorem]
    Let $X_n,1\le n\le\infty$ be random vectors with ch.f. $\varphi_n$. A necessary and sufficient condition for $X_n\Longrightarrow X_\infty$ is that $\varphi_n(t)\to\varphi_\infty(t)$.
\end{Thm}


\begin{Thm}[Cramer-Wold Device]
    A sufficient condition for $X_n\Longrightarrow X_\infty$ is that $\theta\cdot X_n\Longrightarrow \theta\cdot X_\infty$ for all $\theta\in\mathbb{R}^d$.
\end{Thm}


\subsection{Stein's method}
There is a lack of calculus in probability theory. We have only used Fourier transform, i.e. the characteristic function.
Stein invented a exotic way of using calculus to derive the convergence rate of CLT.
\begin{Def}
    
\end{Def}


Stein's method is related to Slepian's interpolation, which is in turn related to Lindeberg's telescopic interpolation.


\section{HDP}
The goal of HDP is to quantify the convergence rate of limit theorems such as CLT. So it is of non-asymptotic nature.
\subsection{Concentration with Independence}
\begin{Thm}[Hoeffding's inequality]
    Let $X_1,\cdots,X_n$ be i.i.d. symmetric Bernoulli random variable. Then 
\end{Thm}

\begin{Def}[sub-gaussian random variable]
    
\end{Def}


\begin{Def}[sub-exponential random variable]
    
\end{Def}


\begin{Lem}[Hoeffding's lemma]
    Assume $a\le X\le b$. Then $\phi(t)\le \frac{t^2(b-a)^2}{8}$.
\end{Lem}
\begin{proof}
    WLOG assume $EX=0$. Recall that $\phi(t)=\log E(e^{tX})$. Then 
    \[\phi'(t)=\frac{E(Xe^{tX})}{E(e^{tX})},\quad \phi''(t)=\frac{E(X^2e^{tX})}{E(e^{tX})}-(\frac{E(Xe^{tX})}{E(e^{tX})})^2\]
    Let $Q$ denote the distribution with $\frac{\mathrm{d} Q}{\mathrm{d} P} = \frac{e^{tX}}{Ee^{tX}} $, where $P$ is the distribution of $X$.
    Then $\phi''(t)=\text{Var}_Q(X)\le \frac{(b-a)^2}{4}$.
\end{proof}

\begin{Lem}[maximal inequality]
    Assume that $X_1,\cdots,X_n$ be $n$ sub-gaussian random variables
    \[E\max_{i\in[n]}X_i\le \sqrt{2\log n}\]
\end{Lem}
\begin{proof}
    LogSumExp Trick.
\end{proof}
\begin{Rk}
    The bound is sharp even though we do not assume independence.
\end{Rk}
\begin{Eg}
    Let $X_1,\cdots,X_n$ be independent $N(0,1)$ random variables. Then 
\end{Eg}

\subsection{Concentration without Independence}
The approach to concentration inequality we developed so far relies crucially on independence of random variables.
We now pursue some alternative approaches to concentration, which are not based on independence.


\subsection{Kernel Trick}
\begin{Thm}[Grothendick's inequality]
    Consider an $m\times m$ matrix $(a_{ij})$ of real numbers. Assume that for numbers $x_i,y_j\in\left\{0,1\right\}$,
    we have \[\left|\sum_{i,j}a_{ij}x_iy_j\right|\le 1\] 
    Then, for any Hilbert space $H$ and any vector $u_i,v_j\in H$ satisfying $\left\|u_i\right\|=\left\|v_j\right\|=1$,
    we have \[\left|\sum_{i,j}a_{ij}x_iy_j\right|\le K\] 
    where $K\le 1.783$ is an absolute constant.
\end{Thm}

\subsection{Decoupling}
In the beginning of HDP, we studied independent random variables of the type $\sum_{i=1}^n a_iX_i$.
Now we want to study quadratic forms of the type $\sum_{i,j=1}^na_{ij}X_iX_j$ where $X_i$'s are independent. Such a quadratic form is called a chaos in probability theory.
It is harder to establish a concentration of a chaos. The main difficulty is that the terms of the sum are not independent.
This difficulty can be overcome by the decoupling technique.\par 
The purpose of decoupling is to replace the quadratic form with the bilinear form $\sum_{i,j=1}^na_{ij}X_iX'_j$ where $X'$ is an independent copy of $X$.
\begin{Thm}[decoupling]
    Let $A$ be an $n\times n$ diagonal-free matrix. Let $X$ be a random vector with independent mean zero coordinates $X_i$.
    Then for every convex function $F:\mathbb{R}\to \mathbb{R}$, 
    \[EF(X^TAX)\le EF(4X^TAX')\] 
    where $X'$ is an independent copy of $X$.
\end{Thm}

\begin{Lem}
    Let $Y$ and $Z$ be independent random variables s.t. $EZ=0$. Then for every convex function $F:\mathbb{R}\to\mathbb{R}$,
    \[EF(Y)\le EF(Y+Z)\]
\end{Lem}
\begin{proof}
    First condition on $Y$ and use Jensen's inequality. Then take expecation w.r.t $Y$.
\end{proof}
\begin{Rk}
    Intuitively, this lemma tells us, under some conditions, adding a mean zero disturbance increases the value.
\end{Rk}


\subsection{Symmetrization}
In this section we develop the simple and useful technique of symmetrization.
It allows one to reduce problems about arbitary distributions to symmetric distributions.
\begin{Def}[Rademacher random variable]
    
\end{Def}
Throuhout this section, we denote by $\xi_1,\xi_2,\cdots$ a sequence of independent Rademacher random variables.
We assume that they are independent not only with each other, but also of any other random variable in question.
\begin{Lem}[symmetrization]
    Let $X_1,\cdots,X_N$ be independent, mean zero random vectors in a normed space. Then 
    \[\frac{1}{2}E\left\|\sum_{i=1}^N\xi_iX_i\right\|\le E\left\|\sum_{i=1}^NX_i\right\|\le 2E\left\|\sum_{i=1}^N\xi_iX_i\right\|\]
\end{Lem}
\begin{proof}
    The proof relies on introducing an independent copy $X'_i$'s of $X_i$ to symmetrize the expression,
    and noticing that $X=^d \xi X$ if $X$ is symmetric.
    \begin{align*}
        E\left\|\sum_{i=1}^NX_i\right\|&\le E\left\|\sum_{i=1}^N(X_i-X'_i)\right\|\\
        &=E\left\|\sum_{i=1}^N\xi_i(X_i-X'_i)\right\|\\
        &=2 E\left\|\sum_{i=1}^N\xi_iX_i\right\|
    \end{align*}
    \begin{align*}
        E\left\|\sum_{i=1}^N\xi_iX_i\right\|&\le E\left\|\sum_{i=1}^N\xi_i(X_i-X'_i)\right\|\\
        &=E\left\|\sum_{i=1}^N(X_i-X'_i)\right\|\\
        &\le 2E\left\|\sum_{i=1}^NX_i\right\|
    \end{align*}
        
\end{proof}


\subsection{Chaining}
\textbf{This section should be moved to stochastic analysis notes because it considers a continuum of random variables.}
Chaining is a multi-scale version of the $\epsilon$-net argument.
\begin{Lem}[discrete Dudley's inequality]
    Let $(X_t)_{t\in T}$ be a mean zero random process on a metric space $(T,d)$ with sub-gaussian increments.
    Then \[E\sup_{t\in T}X_t\le CK\sum_{k\in\mathbb{Z}}2^{-k}\sqrt{\log \mathcal{N}(T,d,2^{-k})}\]
\end{Lem}
In the single-scale $\epsilon$-net argument, we discretize $T$ by choosing an $\epsilon$-net $\mathcal{N}$ of $T$.
Then every point $t\in T$ can be approximated by a closest point from the net $\pi(t)\in\mathcal{N}$ with accuracy $\epsilon$.
The increment condition yields $\left\|X_t-X_{\pi(t)}\right\|_{\phi_2}\le K\epsilon$. This gives $E\sup_{t\in T}X_t\le E\sup_{t\in T}X_{\pi(t)}+E\sup_{t\in T}(X_t-X_{\pi(t)})$.
The first term can be controlled by a union bound over $\left|\mathcal{N}\right|=\mathcal{N}(T,d,\epsilon)$ points $\pi(t)$.
But for the second term, it is not clear how to control the supremum over $t\in T$.



\section{Martingales}
\subsection{Conditional Expectation}
\begin{Def}[conditional expectation]
Given a probability space $(\Omega,\mathcal{F}_o,P)$, a $\sigma$-field $\mathcal{F}\subset\mathcal{F}_o$, and a random varaible $X\in\mathcal{F}_o$ with $E\left|X\right|<\infty$. The conditional expectation of $X$ given $\mathcal{F}$ is any random variable $Y$ that satisfies:\par
(i) $Y\in\mathcal{F}$\par
(ii) $\forall A\in\mathcal{F},\int_AX\mathrm{d}P=\int_A\mathrm{d}P$.
\end{Def}
\begin{Lem}
If $Y$ satisfies (i)\&(ii), then it is integrable.
\end{Lem}
\begin{proof}
Let $A=\left \{ Y>0 \right \} \in \mathcal{F}$. We have $\int_AY\mathrm{d}P=\int_AX\mathrm{d}P\le\int_A\left | X \right | \mathrm{d}P$ and $\int_{A^c}-Y\mathrm{d}P=\int_{A^c}-X\mathrm{d}P\le \int_{A^c}\left | X \right | \mathrm{d}P$, therefore we have $E\left |Y \right | \le\left | X \right | $.
\end{proof}
\begin{Thm}[uniqueness of conditional expectation]
The conditional expecation of $X$ given $\mathcal{F}$ is unique, denoted by $E(X|\mathcal{F})$.
\end{Thm}
\begin{proof}
Suppose $Y'$ also satisfies (i)\&(ii). Taking $A=\left \{ Y-Y'\ge\epsilon>0 \right \} $, we see $0=\int_AX-X\mathrm{d}P=\int_AY-Y'\mathrm{d}P\ge\epsilon P(A)$ so $P(A)=0$. Since this holds for all $\epsilon$, we have $Y\le Y'$ a.s., and switching the role of $Y$\&$Y'$ gives the desiered result.
\end{proof}
\begin{Thm}[existence of conditional expectation]
$E(X|\mathcal{F})$ exists.
\end{Thm}
\begin{proof}
The proof is based on Radon-Nikodym Theorem. Suppose first that $X\ge 0$. Construct  a measure $\nu(A)=\int_AX\mathrm{d}P$ for $A\in\mathcal{F}$. Then $\nu\ll P$, so by Radon-Nikodym Theorem, there exists $Y\in\mathcal{F}$ satisfying $\nu(A)=\int_AY\mathrm{d}P$.\par
To treat the general case, write $X=X^+-X^-$, let $Y_1=E(X^+|\mathcal{F})$ and $Y_2=E(X^-|\mathcal{F})$, then verify condition (i)\&(ii).
\end{proof}
Now we investigate the properties of conditional expectation.
\begin{Thm}

\end{Thm}
\begin{Thm}
If $\varphi$ is convex and $E\left|X\right|,E\left|\varphi(X)\right|<\infty$, then \[\varphi(E(X|\mathcal{F}))\le E(\varphi(X)|\mathcal{F})\]
\end{Thm}
\begin{proof}

\end{proof}
\begin{Cor}
Conditional expectation is a contraction in $L^p$, $p\ge 1$.
\end{Cor}
\begin{Thm}
If $\mathcal{F}_1\subset\mathcal{F}_2$, then:\par
(i) $E(E(X|\mathcal{F}_1)|\mathcal{F}_2)=E(X|\mathcal{F}_1)$\par
(ii) $E(E(X|\mathcal{F}_2)|\mathcal{F}_1)=E(X|\mathcal{F}_1)$
\end{Thm}
\begin{proof}
Directly follows from the definition.
\end{proof}
\begin{Rk}
This theorem shows that whatever the order of conditioning is, the result is always conditioning on the smallest $\sigma$-field.
\end{Rk}
\begin{Thm}
If $X\in\mathcal{F}$ and $E\left|Y\right|,E\left|XY\right|<\infty$, then \[E(XY|\mathcal{F})=XE(Y|\mathcal{F})\]
\end{Thm}
\begin{proof}
Approximate $X$ by the standard process as in the construction of Lebesgue integral.
\end{proof}
\begin{Thm}[LSE]
Suppose $EX^2<\infty$. $E(X|\mathcal{F})$ is the variable $Y\in\mathcal{F}$ that minimizes $E(X-Y)^2$.
\end{Thm}

\subsection{Martingales}
\begin{Def}[filtration]
An increasing sequence of $\sigma$-fields is called a filtration.
\end{Def}
\begin{Def}[adapted]
A sequence $X_n$ is said to be adapted to $\mathcal{F}_n$ if $X_n\in\mathcal{F}_n$ for all $n$.
\end{Def}
\begin{Def}[martingale]
If $X_n$ is a sequence with:\par
(i) $E\left|X_n\right|<\infty$\par
(ii) $X_n$ is adapted to $\mathcal{F}_n$\par
(iii) $E(X_{n+1}|\mathcal{F}_n)=X_n$ for all $n$\par
then $X$ is said to be a martingale.
\end{Def}
\begin{Rk}
If in (iii) is replaced by $\le$ or $\ge$, then $X$ is said to be a supermartingale or submartingale respectively.
\end{Rk}
\begin{Thm}
If $X_n$ is a supermartingale, then for $n>m$, $E(X_n|\mathcal{F}_m)\le X_m$.\par
If $X_n$ is a submartingale, then for $n>m$, $E(X_n|\mathcal{F}_m)\ge X_m$.\par
If $X_n$ is a martingale, then for $n>m$, $E(X_n|\mathcal{F}_m)= X_m$.
\end{Thm}
\begin{proof}
By definition and induction.
\end{proof}
\begin{Thm}
If $X_n$ is a supermartingale w.r.t. $\mathcal{F}_n$ and $\varphi$ is an increasing  concave function with $E\left|\varphi(X_n)\right|<\infty$ for all $n$, then $\varphi(X_n)$ is a supermartingale w.r.t $\mathcal{F}_n$. \par
If $X_n$ is a submartingale w.r.t. $\mathcal{F}_n$ and $\varphi$ is an increasing  convex function with $E\left|\varphi(X_n)\right|<\infty$ for all $n$, then $\varphi(X_n)$ is a submartingale w.r.t $\mathcal{F}_n$. \par
If $X_n$ is a martingale w.r.t. $\mathcal{F}_n$ and $\varphi$ is a convex function with $E\left|\varphi(X_n)\right|<\infty$ for all $n$, then $\varphi(X_n)$ is a submartingale w.r.t $\mathcal{F}_n$. 
\end{Thm}
\begin{proof}
Directly follows from the definition of martingale and Jensen's inequality.
\end{proof}
\begin{Cor}
If $X_n$ is a submartingale, then $(X_n-a)^+$ is a submartingale.
\end{Cor}
\begin{Cor}
If $X_n$ is a supermartingale, then $X_n\wedge a$ is a supermartingale.
\end{Cor}
\begin{Def}[predictable]
Let $\mathcal{F}_n$, $n\ge 0$ be a filtration. $H_n$, $n\ge 1$ is said to be a predictable sequence if $H_n\in\mathcal{F}_{n-1}$ for all $n\ge 1$.
\end{Def}
\begin{Eg}

\end{Eg}
\begin{Thm}
Let $X_n$, $n\ge 0$, be a supermartingale. If $H_n\ge0$ is predictable and each $H_n$ is bounded, then $(H\cdot X)_n=\sum_{m=1}^nH_m(X_m-X_{m-1})$ is a supermartingale. \par
The same fact is true for submartingales and for martingales, while in the latter case we can relax the restriction $H_n\ge0$.
\end{Thm}


\begin{Thm}[Doob's decomposition]
Any submartingale $X_n,n\ge 0$, can be written in a unique way as $X_n=M_n+A_n$, where $M_n$ is a martingale and $A_n$ is a predictable increasing sequence with $A_0=0$.
\end{Thm}
\begin{proof}
We want $X_n=M_n+A_n,E(M_n|\mathcal{F}_{n-1})=M_{n-1}$, and $A_n\in\mathcal{F}_{n-1}$. So we must have 
\begin{align*}
E(X_n|\mathcal{F}_{n-1})&=E(M_n|\mathcal{F}_{n-1})+E(A_n|\mathcal{F}_{n-1})\\
&=M_{n-1}+A_n\\
&=X_{n-1}-A_{n-1}+A_n
\end{align*}
So $A_n-A_{n-1}=E(X_n|\mathcal{F}_{n-1})-X_{n-1}$. Since $A_0=0$, we have \[A_n=\sum_{m=1}^nE(X_n-X_{n-1}|\mathcal{F}_{n-1})\]
The last step is to check what we have constructed above indeed satisfies the desired properties.
\end{proof}

\subsection{Stopping Times}
\begin{Def}[stopping time]
    A random variable $N$ is said to be a stopping time if $\left\{N=n\right\}\in\mathcal{F}_n$ for all $n$.
\end{Def}
\begin{Cor}
    If $N$ is a stopping time and $X_n$ is a supermartingale, then $X_{N\wedge n}$ is a supermartingale.
\end{Cor}
\begin{proof}
    Let $H_n=1_{N\ge n}$. Verify that $H_n$ is predictable. It follows from the theorem that $(H\cdot X)_n=X_{N\wedge n}-X_0$ is a supermartingale. Thus $X_{N\wedge n}$ is a supermartingale as a sum of two supermartingale.
\end{proof}


\subsection{Almost Sure Convergence}
Suppose $X_n$, $n\ge0$, is a submartingale. Let $a<b$ and $N_0=-1$, and for $k\ge 1$ let \[ N_l=\left\{\begin{matrix}
\inf\left \{ m>N_{2k-2}:X_m\le a \right \},   &l=2k-1\\
\inf\left \{ m>N_{2k-1}:X_m\ge b \right \},&l=2k
\end{matrix}\right.\]
The $N_j$ are stopping times and $\left \{ N_{2k-1}<m\le N_{2k} \right \} =\left \{ N_{2k-1}\le m-1\right \} \cap\left \{ N_{2k}\le m-1 \right \}^c \in\mathcal{F}_{m-1}$, so \[H_m=\left\{\begin{matrix}
 1 & \text{ if }N_{2k-1}<m\le N_{2k} \text{ for some }k\\
 0& \text{otehrwise}
\end{matrix}\right.\]
defines a predictable sequence.\par
Note that $X_{N_{2k-1} }\le a$ and $X_{N_{2k}}\ge b$. We can regard $H_m$ as a gambling system taking advantage of these upcrossings. In stock market terms, we buy when $X_m\le a$ and sell when $X_m\ge b$, so every time an upcrossing is completed, we make a profit of $\ge (b-a)$. \par
Finally, let $U_n=\sup\left\{k:N_{2k}\le n\right\}$ be the number of upcrossings completed by time $n$.
\begin{Thm}[upcrossing inequality]
If $X_m$, $m\ge0$, is a submartingale, then \[(b-a)EU_n\le E(X_n-a)^+-E(X_0-a)^+\]
\end{Thm}
\begin{proof}
Let we introduce $Y_n=a+(X_n-a)^+$ to fix the final incomplete upcrossing, then $Y_n$ is a submartingale that upcrosses $[a,b]$ the same number of times that $X_m$ does. Each upcross results in a profit $\ge(b-a)$ and a final incomplete upcrossing of $Y_n$ (instead of $X_n$) results in a nonnegative profit, therefore we have $(b-a)U_n\le (H\cdot Y)_n$.\par
Let $K_m=1-H_m$, then $Y_n-Y_0=(H\cdot Y)_n+(K\cdot Y)_n$. $(K\cdot Y)_n$ is a submartingale as well, so $E(K\cdot Y)_n\ge E(K\cdot Y)_0=0$. Therefore $E(H\cdot Y)_n\le E(Y_n-Y_0)$.
\end{proof}
\begin{Thm}[martingale convergence theorem]
If $X_n$ is a submartingale with $\sup EX_n^+<\infty$, then as $n\to\infty$, $X_n$ converges a.s. to a limit $X$ with $E\left|X\right|<\infty$.
\end{Thm}
\begin{proof}
Since $(X-a)^+\le X^++\left|a\right|$, upcrossing inequality implies that \[EU_n\le \frac{ EX^+_n+\left|a\right|}{b-a}\]
As $n\uparrow\infty$, $U_n\uparrow U$, where $U$ is the number of upcrossings of $[a,b]$ by the whole sequence, so if $\sup EX_n^+<\infty$, then $EU<\infty$ and hence $U\le\infty$ a.s..\par
Since the last conclusion holds for all rational $a$ and $b$,\[P(\bigcup_{a,b\in\mathbb{Q}}\left\{\liminf X_n<a<b<\limsup X_n\right\})=0\]
and hence $\lim X_n$ exists a.s..\par
Fatou's lemma guarantees $EX^+\le\liminf EX_n^+<\infty$. For $EX^-$, we observe that $EX_n^-=EX_n^+-EX_n\le EX_n^+-EX_0$ since $X_n$ is a submartingale, so another application of Fatou's lemma shows $EX^-\le\liminf EX^-_n\le \sup EX_n^+-EX_0$ and completes the proof.
\end{proof}
\subsection{Convergence in $L^p$}
\begin{Lem}[bounded optional stopping]\label{Bounded Optional Stopping}
If $X_n$ is a submartingale and $N$ is a stopping time with $P(N\le k)=1$, then \[EX_0\le EX_N\le EX_k\]
\end{Lem}
\begin{proof}
$X_{N\wedge n}$ is a submartingale, so $EX_0=EX_{N\wedge n}\le EX_{N\wedge k}=EX_N$.\par
To prove the other inequality, let $K_n=1_{\left\{N<n\right\}}=1_{\left\{N\le n-1\right\}}$. $K_n$ is predictable, so $(K\cdot X)_n=X_n-X_{n\wedge N}$ is a submartingale, and it follows that $EX_k-EX_N=E(K\cdot X)_k\ge E(K\cdot X)_0=0$.
\end{proof}
\begin{Lem}
    If $X_n$ is a submartingale and $M\le N$ are stopping times with $P(N\le k)=1$,
    then $EX_M\le EX_N$.
\end{Lem}
\begin{proof}
    Let $K_n=1_{\{M<n\le N\}}$ and modify the above proof.
\end{proof}
\begin{Lem}[bounded Doob's stopping]
    If $X_n$ is a submartingale and $M\le N$ are stopping times with $P(N\le k)=1$,
    then $X_M\le E(X_N|\mathcal{F}_M)$.
\end{Lem}
\begin{proof}
    Let $A\in\mathcal{F}_M$. Define a random time $L=M 1_A +N 1_{A^c}$. Actually, this is a stopping time.
    So $EX_M\le EX_L\le EX_N$ by the above lemma. Thus $E(X_M 1_A)\le E(X_N 1_A)$ and the result follows.
\end{proof}

\begin{Cor}
    An adapted and integrable process $X_t$ is a martingale if and only if 
    \[E(X_M)=E(X_N)\] 
    for every such pair of stopping times.
\end{Cor}
\begin{proof}
    Let $A\in\mathcal{F}_{n-1}$ and $L=(n-1)1_A+n1_{A^c}$. By $EX_L=EX_n$, we have $EX_n1_A=EX_{n-1}1_A$,
    so $E(X_n|\mathcal{F}_{n-1})=X_{n-1}$.
\end{proof}


\begin{Thm}[Doob's inequality]
Let $X_m$ be a submartingale, $\lambda>0$, and $A=\left\{ \max_{0\le m\le n}X_m^+  \ge\lambda\right\}$, then \[\lambda P(A)\le EX_n1_A\le EX_n^+\]
\end{Thm}
\begin{proof}
Let $N=\inf\left\{m:X_m\ge\lambda\right\}\wedge n$, then $X_N\ge\lambda$ on $A$. Therefore $\lambda P(A)\le EX_N1_A\le EX_n1_A$, where the second inequality follows from the lemma above.\par
The other inequality is obvious.
\end{proof}
\begin{Eg}[Kolmogorov's maximal inequality]
If we let $S_n=\xi_1+\cdots+\xi_n$, where the $\xi_m$ is independent and have $E\xi_m=0$, $\sigma_m^2=E\xi_m^2<\infty$. $S_n$ is a martingale, so $S_n^2$ is a submartingale. If we let $\lambda=x^2$ and apply Doob's inequality, we get Kolmogorov's maximal inequality: \[P(\max_{1\le m\le n}\left|S_m\right|\ge x)\le x^{-2}\text{var}(S_n)\]
\end{Eg}
\begin{Thm}[$L^p$ maximum inequality]
If $X_n$ is a submartingale, and $\bar{X}_n= \max_{0\le m\le n}X_m^+$, then for $1<p<\infty$, \[E(\bar{X}_n^p)\le (\frac{p}{p-1})^pE(X_n^+)^p\]
\end{Thm}
\begin{proof}
The ingredients are  Doob's inequality and H\"older's inequality. 
To avoid dividing infinity, we will work with $\bar{X}_n\wedge M$ rather than $\bar{X}_n$. This does not change the application of Doob's inequality.\par
\begin{align*}
E((\bar{X}_n\wedge M)^p)&=\int_0^\infty p\lambda^{p-1}P(\bar{X}_n\wedge M\ge \lambda)\mathrm{d}\lambda\\
&\le \int_0^\infty p\lambda^{p-1}(\lambda^{-1}\int X_n^+1_{\left \{ \bar{X}_n\wedge M\ge\lambda \right \} }\mathrm{d}P)\mathrm{d}\lambda\\
&=\int X_n^+\int_0^{\bar{X}_n\wedge M}p\lambda^{p-2}\mathrm{d}\lambda\mathrm{d}P\\
&=\frac{p}{p-1}\int X_n^+(\bar{X}_n\wedge M)^{p-1}\mathrm{d}P
\end{align*}
If we let $q=\frac{p}{p-1}$ be the conjugate to $p$ and apply H\"older's inequality, we see that \[\le (\frac{p}{p-1})(E\left|X_n^+\right|^p)^{1/p}(E\left|\bar{X}_n\wedge M\right|^p)^{1/q}\]
If we divide both sides of the last inequality by $(E\left|\bar{X}_n\wedge M\right|^p)^{1/q}$, which is finite thanks to $\wedge M$, then take the $p$th power of each side, and letting $M\to\infty$ and using the monotone convergence theorem gives the desired result.
\end{proof}
\begin{Thm}[$L^p$ convergence theorem]
If $X_n$ is a martingale with $\sup E\left|X_n\right|^p<\infty$ where $p>1$, then $X_n\to X$ a.s. and in $L^p$.
\end{Thm}
\begin{proof}
$(EX_n^+)^p\le (E\left|X_n\right|)^p\le E\left|X_n\right|^p$, so it follows from the martingale convergence theorem that $X_n\to X$ a.s..\par
Applying $L^p$ maximum inequality to $\left|X_n\right|$ implies \[E(\sup_{0\le m\le n}\left|X_n\right|)^p\le (\frac{p}{p-1})^pE(\left|X_n\right|)^p\]
Letting $n\to\infty$ and using the monotone convergence theorem implies $\sup\left|X_n\right|\in L^p$. Since $\left|X_n-X\right|^p\le (2\sup\left|X_n\right|)^p$, it follows from the dominated convergence theorem that $E\left|X_n-X\right|^p\to 0$.
\end{proof}

\subsection{Square Integrable Martingales}
In this section, we will suppose \[X_n \text{ is a martingale with }X_0=0 \text{ and } EX_n^2<\infty \text{ for all } n\]
Thus, $X_n^2$ is a submartingale. It follows from Doob's decomposition that we can write $X_n^2=M_n+A_n$, where $M_n$ is a martingale, and \[A_n=\sum_{m=1}^nE(X_n^2-X_{n-1}^2|\mathcal{F}_{n-1})=\sum_{m=1}^nE((X_n-X_{n-1})^2|\mathcal{F}_{n-1})\]
$A_n$ is called the increasing process associated with $X_n$.
\begin{Thm}
$\lim_{n\to\infty}X_n$ exists and is finite a.s. on $\left\{A_\infty<\infty\right\}$.
\end{Thm}
\begin{Lem}
$E(\sup_m\left|X_m\right|^2)\le 4EA_\infty$
\end{Lem}
\begin{proof}
By $L^2$ maximum inequality, $E(sup_{0\le m\le n}\left|X_m\right|^2)\le 4EX_n^2=4EA_n+4EM_n=4EA_n+4EM_0=4EA_n+4EX_0^2=4EA_n$. Using the monotone convergence theorem now gives the desired result.
\end{proof}
\begin{proof}
Let $a>0$. Since $A_{n+1}\in \mathcal{F}_n$, $N=\inf\left\{n:A_{n+1}>a^2\right\}$ is a stopping time. Applying the lemma to $X_{N\wedge n}$ and noticing $A_{N\wedge n}\ge a^2$ gives $E(\sup_n\left|X_{N\wedge n}\right|^2)\le 4a^2$, so the $L^2$ convergence theorem implies that $\lim X_{N\wedge n}$ exists and is finite a.s.. Since $a$ is arbitary, the desired result follows.
\end{proof}
\begin{Thm}
Let $f\ge 1$ be increasing with $\int_0^\infty f(t)^{-2}\mathrm{d}t<\infty$. Then $\frac{X_n}{f(A_n)}\to 0$ a.s. on $\left\{A_\infty=\infty\right\}$.
\end{Thm}

\subsection{Convergence in $L^1$}
Now we seek the necessary and sufficient conditions for a martingale to converge in $L^1$. This leads to the definition of uniformly integrability.
\begin{Def}[uniformly integrablity]
A collection of random variables $X_i,i\in I$ is said to be uniformly integrable if 
\[\lim_{M\to\infty}(\sup_{i\in I}E(\left|X_i\right|;\left|X_i\right|>M))=0\]
\end{Def}
\begin{Eg}
    A collection of random variables that are dominated by an integrable random variable is uniformly integrable.
\end{Eg}
\begin{Eg}
    A collection of bounded random variables is uniformly integrable.
\end{Eg}
Below we give an interesting example of a uniformly integrable family.
\begin{Thm}
Given a probability space $(\Omega,\mathcal{F}_o.P)$ and an $X\in L^1$, then $\left\{E(X|\mathcal{F}):\mathcal{F}\text{ is a }\sigma\text{-field}\subset\mathcal{F}_o\right\}$ is uniformly integrable.
\end{Thm}

A common way to check uniform integrability is to use:
\begin{Lem}
Let $\varphi\ge 0$ be any function with $\frac{\varphi(x)}{x}\to\infty$ as $x\to\infty$. If $E\varphi(\left|X_i\right|)\le C$ for all $i\in I$, then $X_i,i\in I$ is uniformly integrable.
\end{Lem}
\begin{proof}
    Write $E(\left|X_i\right|;\left|X_i\right|>M)=E(\frac{\left|X_i\right|}{\varphi(\left|X_i\right|)}\varphi(\left|X_i\right|);\left|X_i\right|>M)$ and notice that $\frac{\left|X_i\right|}{\varphi(\left|X_i\right|)}\to 0$.
\end{proof}



\begin{Lem}
If integrable random variables $X_n\to X$ in $L^1$, then $E(X_n;A)\to E(X;A)$.
\end{Lem}
\begin{proof}
The difference is smaller than $E\left|X_n-X\right|$.
\end{proof}
\begin{Lem}
If a martingale $X_n\to X$ in $L^1$, then $X_n=E(X|\mathcal{F}_n)$.
\end{Lem}
\begin{proof}
$E(X_m|\mathcal{F}_n)$ for $m>n$, so if $A\in\mathcal{F}_n$, $E(X_m;A)=E(X_n;A)$. By the lemma above, we have $E(X_n;A)=E(X;A)$ for all $A\in\mathcal{F}_n$. By the definition of condition expectation, $X_n=E(X|\mathcal{F}_n)$.
\end{proof}
\begin{Thm}
    For a martingale, TFAE:\newline
    (i) It is uniformly integrable.\newline 
    (ii) It converges a.s. and in $L^1$.\newline 
    (iii) It converges in $L^1$.\newline 
    (iv) There is an integrable random variable $X$ s.t. $X_n=E(X|\mathcal{F}_n)$.
\end{Thm}



\subsection{Backwards Martingales}
\begin{Def}[backwards martingale]
A backwards martingale is a martingale indexed by the negative integers, i.e., $X_n,n\le 0$, adapted to an increasing sequence of $\sigma$-fields $\mathcal{F}_n$ with $E(X_{n+1}|\mathcal{F}_n)=X_n$ for $n\le -1$.
\end{Def}
\begin{Thm}
$X_{-\infty}=\lim_{n\to-\infty}X_n$ exists a.s. and in $L^1$.
\end{Thm}
\begin{proof}
 Let $U_n$ be the number of upcrossings of $[a,b]$ by $X_{-n},\cdots,X_0$.
 The upcrossing inequality implies $(b-a)EU_n\le E(X_0-a)^+$. Letting $n\to\infty$ and using the monotone convergence theorem,
 we have $EU_\infty<\infty$, so $X_{-\infty}=\lim_{n\to-\infty}X_n$ exists a.s..\newline 
 The martingale property implies $X_n=E(X_0|\mathcal{F}_n)$, so $X_n$ is uniformly integrable and the convergence occurs in $L_1$.

\end{proof}
Now we identify the limit.
\begin{Thm}
If $X_{-\infty}=\lim_{n\to-\infty}X_n$ and $\mathcal{F}_{-\infty}=\bigcap_n\mathcal{F}_n$, then $X_{-\infty}=E(X_0|\mathcal{F}_{-\infty})$.
\end{Thm}
\begin{proof}
Clearly $X_{-\infty}\in\mathcal{F}_{-\infty}$. $X_n=E(X_0|\mathcal{F}_n)$, so if $A\in\mathcal{F}_{-\infty}\subset\mathcal{F}_n$, then $\int_AX_n\mathrm{d}P=\int_AX_0\mathrm{d}P$, so $\int_AX_{-\infty}\mathrm{d}P=\int_AX_0\mathrm{d}P$ for all $A\in\mathcal{F}_{-\infty}$, proving the desired conclusion.
\end{proof}
\begin{Thm}
If $\mathcal{F}_n\downarrow\mathcal{F}_{-\infty}$ as $n\downarrow -\infty$, then $E(Y|\mathcal{F}_n)\to E(Y|\mathcal{F}_{-\infty})$ a.s. and in $L^1$.
\end{Thm}
\begin{proof}
$X_n=E(Y|\mathcal{F}_n)$ is a backwards martingale, so $X_n\to X_{-\infty}$ a.s. and in $L^1$, where $X_{-\infty}=E(X_0|\mathcal{F}_{-\infty})=E(Y|\mathcal{F}_{-\infty})$.
\end{proof}

\subsection{Optional Stopping Theorems}
Recall that in Lemma \ref{Bounded Optional Stopping}, we have already established optional stopping theorem for bounded stopping times,
so in this section we mainly focus on unbounded stopping time. 


\begin{Thm}
    If $X_n$ is a uniformly integrable submartingale, them for any stopping time $N$,
    $X_{n\wedge N}$ is uniformly integrable.
\end{Thm}
\begin{proof}
    As $X_n^+$ is a submartingale, so by bounded optional stopping $EX_{N\wedge n}^+ \le EX_n^+$.
    Since $X_n^+$ is uniformly integrable, $\sup_n X_{N\wedge n}^+ \le \sup_n EX_n^+ <\infty$. So by martingale convergence, $X_{N\wedge n}\to X_N$ a.s. and $E|X_N|<\infty$.
    Now
    \begin{equation*}
        E(|X_{N\wedge n}|;|X_{N\wedge n}|>K) = E(|X_{N\wedge n}|;|X_{N\wedge n}|>K,N\le n)+E(|X_{N\wedge n}|;|X_{N\wedge n}|>K,N>n)
    \end{equation*}
    Since $E|X_N|<\infty$ and $X_n$ is uniformly integrable, if $K$ is large, then each term is controlled.

\end{proof}
\begin{Rk}
    From the last computation above, we actually get that if $E|X_N|<\infty$ and $X_n1_{\{N>n\}}$ is uniformly integrable,
    then $X_{N\wedge n}$ is uniformly integrable. And this is the requirement of the optional sampling theorem we usually see.
\end{Rk}

\begin{Thm}
    If $X_n$ is a uniformly integrable submartingale, then for any stopping time $N\le \infty$,
    we have $EX_0\le EX_N\le EX_\infty$. 
\end{Thm}
\begin{proof}
    By bounded optional stopping, $EX_0\le EX_{N\wedge n}\le EX_n$. Let $n\to\infty$ and use the $L1$ convergence of uniformly integrable submartingale.
\end{proof}

\begin{Thm}
    If $X_n$ is a uniformly integrable submartingale and $L\le M$ are stopping times and $Y_{M\wedge n}$ is uniformly integrable submartingale,
    then we have $Y_L\le E(Y_M|\mathcal{F}_L)$. 
\end{Thm}

For a nonegative supermartingale, we do not require uniform integrability.
\begin{Thm}
    If $X_n$ is a nonnegative supermartingale and $N\le\infty$ is a stopping time,
    then $EX_0\ge EX_N$
\end{Thm}
\begin{proof}
    
\end{proof}

\begin{Thm}
    Suppose $X_n$ is a submartingale and $E(|X_{n+1}-X_n||\mathcal{F}_n)\le B$ a.s..
    If $N$ is a stopping time with $EN<\infty$, then $X_{N\wedge n}$ is uniformly integrable
    and hence $EX_N\ge EX_0$.
\end{Thm}
\begin{proof}
    We begin by observing that \[|X_{N\wedge n}|\le |X_0|+\sum_{m=0}^\infty|X_{m+1}-X_m|1_{(N>m)}\] 
    To prove uniform integrability, it suffices to show that the right-hand side has finite expectation for then $|X_{N\wedge n}|$ is dominated by an integrable r.v..\newline 
    \[ E(|X_{m+1}-X_m|1_{\{N>m\}})=E(E(|X_{m+1}-X_m||\mathcal{F}_m)1_{\{N>m\}})\le BP(N>m)\] 
    and hence the expectation of RHS$\le E|X_0|+BEN$.
\end{proof}

Now we come to Doob's stopping theorem.
\begin{Thm}
    If $X_n$ is a uniformly integrable submartingale, then 
\end{Thm}


\subsubsection{Applications}
\begin{Eg}
    
\end{Eg}

\begin{Eg}
    Let $S_n$ be symmetric random walk with $S_0=0$ and let $T_1=\min\{n:S_n=1\}$.
    Find $P(T_1=2n-1)$.
\end{Eg}
\begin{proof}
    First $P(T_1<\infty)=1$
    Use the exponential martingale $X_n=\frac{\exp{\theta S_n}}{E\exp{\theta S_n}}$. $E\exp{\theta S_n}=\frac{e^\theta+e^{-\theta}}{2}$.
\end{proof}

\begin{Eg}
    Let $S_n$ be a symmetric random walk starting at $0$, and let $T=\inf\{n:S_n\notin (-a,a)\}$,
    where $a$ is an integer. Compute $ET^2$.
\end{Eg}
\begin{proof}
    
\end{proof}

\begin{Eg}
    Consider a favorable game in which the payoffs are -1,1,2 with probability $\frac{1}{3}$ each. Compute the probability 
    we ever go broke when we start with $i>0$.
\end{Eg}
\begin{Lem}
    Let $S_n=\xi_1+\cdots+\xi_n$ be a random walk. Suppose  
\end{Lem}
\begin{proof}
    
\end{proof}
\begin{proof}
    The original problem is the case where $\theta_0=\ln (\sqrt{2}-1)$. So the probability is $(\sqrt{2}-1)^i$.
\end{proof}

\section{The Probabilistic Method}
\subsection{The Method}



\subsection{123 Theorem}
As a sort of 'inverse' of the probabilistic method, combinatorial techniques can also apply to probabilistic statement.


\subsection{The Local Lemma}

\begin{Def}[dependency digraph]
    Let $A_i$($i\in [n])$ be $n$ events. A directed graph $D=(V,E)$ on the set of vertices 
    $V=\{1,\cdots,n\}$ is called a dependency digraph for the events if 
    for each $i$ the event is mutually independent of all the events $\{A_j:(i,j)\notin E\}$.
\end{Def}
\begin{Thm}[the local lemma]
    Let $A_i$($i\in [n])$ be $n$ events. Suppose $D=(V,E)$ is a dependency digraph for the above events and 
    suppose there are real numbers $x_1,\cdots,x_n$ s.t. $1\le x_i<0$ and $P(A_i)\le x_i\prod_{(i,j)\in E}(1-x_j)$
    for all $1\le i\le n$. Then 
    \[ P(\bigcap_{i=1}^n\overline{A_i})\ge \prod_{i=1}^n (1-x_i)\] 
\end{Thm}
\begin{Rk}
    In particular, with positive probability, no event $A_i$ holds.
\end{Rk}

\begin{Cor}[the local lemma: symmetric case]
    
\end{Cor}
\begin{Rk}
    The constant $e$ is the best possible constant.
\end{Rk}


\subsection{Correlation Inequalities}
\begin{Thm}[the four functions theorem]
    
\end{Thm}



\begin{Thm}[FKG inequality]
    Let $L$ be a finite distributive lattice, and let $\mu:L\to \mathbb{R}^+$ be a log-supermodular function.
    Then for any two increasing functions $f,g$, we have
    \[E(fg)\ge (Ef) (Eg)\] where the expectation is taken w.r.t. 
\end{Thm}
\begin{Rk}
    If both $f$ and $g$ are decreasing, the the result still holds. If one is increasing and one is decreasing, then the inequality is reversed.
\end{Rk}

\begin{Lem}[Kleitman's lemma]
    Let 
\end{Lem}

\begin{Thm}
    
\end{Thm}

\begin{Def}[linear extension]
    
\end{Def}

\begin{Thm}[XYZ theorem]
    Let 
\end{Thm}


\section{Random Matrices}
The goal of this section is to extend previous results for random variables to random matrices.
\end{document}