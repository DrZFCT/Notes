\chapter{High Dimensional Probability}
The goal of HDP is to quantify the convergence rate of limit theorems such as CLT (Now I think this should be considered as large deviation theory instead). So it is of non-asymptotic nature.

High-Dimensional track the dependence on the dimension, overcome curse of dimensionality. Thus tensorization is of vital importance.
\section{Concentration with Independence}
\begin{theorem}[Hoeffding's inequality]
    Let $X_1,\cdots,X_n$ be i.i.d. symmetric Bernoulli random variable. Then 
\end{theorem}

\begin{definition}[sub-gaussian random variable]
    
\end{definition}


\begin{definition}[sub-exponential random variable]
    
\end{definition}


\begin{lemma}[Hoeffding's lemma]
    Assume $a\le X\le b$. Then $\phi(t)\le \frac{t^2(b-a)^2}{8}$.
\end{lemma}
\begin{proof}
    WLOG assume $EX=0$. Recall that $\phi(t)=\log E(e^{tX})$. Then 
    \[\phi'(t)=\frac{E(Xe^{tX})}{E(e^{tX})},\quad \phi''(t)=\frac{E(X^2e^{tX})}{E(e^{tX})}-(\frac{E(Xe^{tX})}{E(e^{tX})})^2\]
    Let $Q$ denote the distribution with $\frac{\mathrm{d} Q}{\mathrm{d} P} = \frac{e^{tX}}{Ee^{tX}} $, where $P$ is the distribution of $X$.
    Then $\phi''(t)=\text{Var}_Q(X)\le \frac{(b-a)^2}{4}$.
\end{proof}

\begin{lemma}[maximal inequality]
    Assume that $X_1,\cdots,X_n$ be $n$ sub-gaussian random variables
    \[E\max_{i\in[n]}X_i\le \sqrt{2\log n}\]
\end{lemma}
\begin{proof}
    LogSumExp Trick.
\end{proof}
\begin{remark}
    The bound is sharp even though we do not assume independence.
\end{remark}
\begin{example}
    Let $X_1,\cdots,X_n$ be independent $N(0,1)$ random variables. Then 
\end{example}

\section{Concentration without Independence}
The approach to concentration inequality we developed so far relies crucially on independence of random variables.
We now pursue some alternative approaches to concentration, which are not based on independence.


\section{Subgaussian Concentration}

\subsection{The Martingale Method}

\begin{lemma}[Azuma]
    
\end{lemma}



\subsection{The Entropy Method}

First we define the entropy of a nonnegative random variable. 
\begin{definition}
    For a random variable $Z\geq 0$, its entropy is defined as the following:
    \begin{equation*}
        \Ent(Z):=\EE [Z\log Z]-\EE [Z]\log \EE[Z].
    \end{equation*}
\end{definition}
\begin{remark}
    When $Z(\omega)=\frac{\d{\mu}(\omega)}{\d{\nu}(\omega)}$ and $\omega$ with measure $\nu$, this is the KL divergence (or relative entropy).
    However, note that this definition is different from entropy (or differential entropy) for a probability distribution.
\end{remark}3

One can immediately see by Jensen's ineqaulity that 
\[\Ent(Z)\leq \cov(Z,\log Z)\]

\begin{lemma}[Herbst]
    Suppose that 
    \begin{equation*}
        \Ent[e^{\lambda X}]\leq \frac{\lambda^2\sigma^2}{2}\EE[e^{\lambda X}],\quad\forall \lambda\geq 0.
    \end{equation*}
    Then 
    \[\phi(\lambda):=\log \EE[e^{\lambda(X-\EE X)}]\leq \frac{\lambda^2\sigma^2}{2},\quad\forall \lambda\geq 0.\]
\end{lemma}
\begin{proof}
    The key observation is the following calculation for $\phi(\lambda)$.
    As $\phi(\lambda)=\log \EE[e^{\lambda X}]-\lambda \EE X$,
    we can divide by $\lambda$ and make the following calculation to get rid of $\EE X$
    \begin{equation*}
        \frac{\d}{\d{\lambda}}\frac{\phi(\lambda)}{\lambda}=\frac{1}{\lambda^2}\frac{\Ent[e^{\lambda X}]}{\EE[e^{\lambda X}]}
    \end{equation*}
\end{proof}
Here we just derive another equivalent formulation of Gaussian 

The key is 
\begin{theorem}
    If $X_1,\dots,X_n$ are independent, then
    \begin{equation*}
          \Ent f(X_1,\dots,X_n)\leq \EE \sum_{i=1}^{n} \Ent_i f(X_1,\dots,X_n).  
    \end{equation*}
\end{theorem}
\begin{proof}
      
\end{proof}

Analogous to Donsker-Varadhan, we have a variational 

\section{Lipschitz Concentration}

Isoperimetirc inequality

transport inequality, Bobkov-Gotze
\begin{theorem}[Bobkov-Gotze]
    Let 
\end{theorem}
\begin{proof}
    Use variational representation of Wasserstein distance and KL divergence (Gibbs variational principle)
\end{proof}

Using the transportation  and the chain rule for KL divergence, we have the following tool for tensorization
\begin{theorem}[Marton]
    Let $\varphi:\RR^+\to\RR^+$ be a convex function, and let $w_i:\XX_i\times\XX_i\to\RR^+$ be positive weight function. Suppose that for $i=1,\dots,d$
\end{theorem}

But the RHS is not a Wasserstein distance itself.
weighted $\ell_1$-metric
\begin{equation*}
    d_c(x,y):=\sum_{i=1}^{d}c_id_i(x_i,y_i),
\end{equation*}


\subsection{Talagrand's Concentrartion Inequality}
One-Sided Lipschitz property

\section{Kernel Trick}
\begin{theorem}[Grothendick's inequality]
    Consider an $m\times m$ matrix $(a_{ij})$ of real numbers. Assume that for numbers $x_i,y_j\in\left\{0,1\right\}$,
    we have \[\left|\sum_{i,j}a_{ij}x_iy_j\right|\le 1\] 
    Then, for any Hilbert space $H$ and any vector $u_i,v_j\in H$ satisfying $\left\|u_i\right\|=\left\|v_j\right\|=1$,
    we have \[\left|\sum_{i,j}a_{ij}x_iy_j\right|\le K\] 
    where $K\le 1.783$ is an absolute constant.
\end{theorem}

\section{Decoupling}
In the beginning of HDP, we studied independent random variables of the type $\sum_{i=1}^n a_iX_i$.
Now we want to study quadratic forms of the type $\sum_{i,j=1}^na_{ij}X_iX_j$ where $X_i$'s are independent. Such a quadratic form is called a chaos in probability theory.
It is harder to establish a concentration of a chaos. The main difficulty is that the terms of the sum are not independent.
This difficulty can be overcome by the decoupling technique.\par 
The purpose of decoupling is to replace the quadratic form with the bilinear form $\sum_{i,j=1}^na_{ij}X_iX'_j$ where $X'$ is an independent copy of $X$.
\begin{theorem}[decoupling]
    Let $A$ be an $n\times n$ diagonal-free matrix. Let $X$ be a random vector with independent mean zero coordinates $X_i$.
    Then for every convex function $F:\mathbb{R}\to \mathbb{R}$, 
    \[EF(X^TAX)\le EF(4X^TAX')\] 
    where $X'$ is an independent copy of $X$.
\end{theorem}

\begin{lemma}
    Let $Y$ and $Z$ be independent random variables s.t. $EZ=0$. Then for every convex function $F:\mathbb{R}\to\mathbb{R}$,
    \[EF(Y)\le EF(Y+Z)\]
\end{lemma}
\begin{proof}
    First condition on $Y$ and use Jensen's inequality. Then take expecation w.r.t $Y$.
\end{proof}
\begin{remark}
    Intuitively, this lemma tells us, under some conditions, adding a mean zero disturbance increases the value.
\end{remark}

Now we can prove the Hason-Wright inequality.
\begin{theorem}
    Let $X=(X_1,\cdots,X_n)\in \mathbb{R}^n$ be a random vector with independent, 
    mean zero, sub-gaussian coordinates.Let $A$ be an $n\times n$ matrix. Then, for every $t\geq 0$, we have 
    \[ \]
\end{theorem}

\section{Symmetrization}
In this section we develop the simple and useful technique of symmetrization.
It allows one to reduce problems about arbitary distributions to symmetric distributions.
\begin{definition}[Rademacher random variable]
    
\end{definition}
Throuhout this section, we denote by $\xi_1,\xi_2,\cdots$ a sequence of independent Rademacher random variables.
We assume that they are independent not only with each other, but also of any other random variable in question.
\begin{lemma}[symmetrization]
    Let $X_1,\cdots,X_N$ be independent, mean zero random vectors in a normed space. Then 
    \[\frac{1}{2}E\left\|\sum_{i=1}^N\xi_iX_i\right\|\le E\left\|\sum_{i=1}^NX_i\right\|\le 2E\left\|\sum_{i=1}^N\xi_iX_i\right\|\]
\end{lemma}
\begin{proof}
    The proof relies on introducing an independent copy $X'_i$'s of $X_i$ to symmetrize the expression,
    and noticing that $X=^d \xi X$ if $X$ is symmetric.
    \begin{align*}
        E\left\|\sum_{i=1}^NX_i\right\|&\le E\left\|\sum_{i=1}^N(X_i-X'_i)\right\|\\
        &=E\left\|\sum_{i=1}^N\xi_i(X_i-X'_i)\right\|\\
        &=2 E\left\|\sum_{i=1}^N\xi_iX_i\right\|
    \end{align*}
    \begin{align*}
        E\left\|\sum_{i=1}^N\xi_iX_i\right\|&\le E\left\|\sum_{i=1}^N\xi_i(X_i-X'_i)\right\|\\
        &=E\left\|\sum_{i=1}^N(X_i-X'_i)\right\|\\
        &\le 2E\left\|\sum_{i=1}^NX_i\right\|
    \end{align*}
        
\end{proof}


\section{Chaining}
\textbf{This section should be moved to stochastic analysis notes because it considers a continuum of random variables.}
Chaining is a multi-scale version of the $\epsilon$-net argument.
\begin{lemma}[discrete Dudley's inequality]
    Let $(X_t)_{t\in T}$ be a mean zero random process on a metric space $(T,d)$ with sub-gaussian increments.
    Then \[E\sup_{t\in T}X_t\le CK\sum_{k\in\mathbb{Z}}2^{-k}\sqrt{\log \mathcal{N}(T,d,2^{-k})}\]
\end{lemma}
In the single-scale $\epsilon$-net argument, we discretize $T$ by choosing an $\epsilon$-net $\mathcal{N}$ of $T$.
Then every point $t\in T$ can be approximated by a closest point from the net $\pi(t)\in\mathcal{N}$ with accuracy $\epsilon$.
The increment condition yields $\left\|X_t-X_{\pi(t)}\right\|_{\phi_2}\le K\epsilon$. This gives $E\sup_{t\in T}X_t\le E\sup_{t\in T}X_{\pi(t)}+E\sup_{t\in T}(X_t-X_{\pi(t)})$.
The first term can be controlled by a union bound over $\left|\mathcal{N}\right|=\mathcal{N}(T,d,\epsilon)$ points $\pi(t)$.
But for the second term, it is not clear how to control the supremum over $t\in T$.


