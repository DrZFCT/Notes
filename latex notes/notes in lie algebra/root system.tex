\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper,total={6in,10in}]{geometry}
\usepackage{amsmath}
\usepackage[all,cmtip]{xy}

\usepackage{amsthm}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{example}{Example}[section]
\newtheorem*{remark}{Remark}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]


\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{hyperref}



\title{Root Systems}
\author{Kaizhao Liu}
\date{Faburary 2023}

\begin{document}
\maketitle
\tableofcontents

\section{Root}
\begin{definition}[Root System]
    A root system is a finite-dimensional real vector space $V$ with an inner product $\langle\cdot,\cdot\rangle$,
    together with a finite collection $R$ of nonzero vectors in $V$ satisfying the following properties:\newline 
    (i) The vectors in $R$ span $V$.\newline 
    (ii) $\alpha\in R\Longrightarrow -\alpha\in R$.\newline
    (iii) If $\alpha\in R$, then the only multiples of $\alpha$ in $R$ are $\alpha$ and $-\alpha$.\newline 
    (iv) $\alpha,\beta\in R\Longrightarrow w_\alpha\cdot \beta\in R$, where 
    \[w_\alpha\cdot \beta=\beta-2\frac{\langle\beta,\alpha\rangle}{\langle\alpha,\alpha\rangle}\alpha\]
    (v) $\forall \alpha,\beta\in R$, $2\frac{\langle\beta,\alpha\rangle}{\langle\alpha,\alpha\rangle}$ is an integer.

    $\dim V$ is called the rank of the root system and the elements of $R$ is called roots.
\end{definition}
There are some redundancy in this definition since $w_\alpha(\alpha)=-\alpha$, but our definition is more intuitive.

\begin{theorem}[Weyl Group]
    Let $(V,R)$ be a root system, then the Weyl group $W$ of $R$ is the subgroup of the orthogonal group of $V$, 
    generated by $w_\alpha$ where $\alpha\in R$.
\end{theorem}

\begin{theorem}[Direct Sum]
    Suppose $(V,R)$ and $(W,S)$ are root systems. Consider the vector space $V\oplus W$, with the natural inner product.
    Then $R\cup S$ is a root system in $V\oplus W$, called the direct sum of $V$ and $W$. Here we assume the natural identification.
\end{theorem}

\begin{definition}[Irreduciblility]
    A root system $(V,R)$ is called reducible if there exists an orthogonal decomposition $V=V_1\oplus V_2$ with $\dim V_1>0$ and $\dim V_2>0$
    s.t. every element of $R$ is either in $V_1$ or in $V_2$. If no such decomposition exists, $(V,R)$ is called irreducible.
\end{definition}

\begin{definition}[Equivalence]
    Two root systems $(V_1,R_1)$ and $(V_2,R_2)$ are said to be equivalent if $\exists T:V_1\to V_2$ which is an invertible linear transformation
    s.t. $T$ maps $R_1$ onto $R_2$ and s.t. $\forall \alpha\in R_1,\beta\in V_1$, we have 
    \[ T(w_\alpha\cdot \beta)=w_{T\alpha}\cdot T\beta.\] 
    A map $T$ with this property is called an equivalence.
\end{definition}

What are the root systems?
\begin{theorem}\label{4 cases}
    Suppose $\alpha,\beta$ are noncollinear roots and $\langle\alpha\rangle\ge\langle\beta\rangle$. Then one of the following holds:\newline 
    (i) $\langle \alpha,\beta\rangle=0$ \newline 
    (ii) $\langle \alpha,\alpha\rangle=\langle \beta,\beta\rangle$ and the angle between $\alpha$ and $\beta$ is $\frac{\pi}{3}$ or $\frac{2\pi}{3}$\newline 
    (iii) $\langle \alpha,\alpha\rangle=2\langle \beta,\beta\rangle$ and the angle between $\alpha$ and $\beta$ is $\frac{\pi}{4}$ or $\frac{3\pi}{4}$\newline 
    (iv) $\langle \alpha,\alpha\rangle=3\langle \beta,\beta\rangle$ and the angle between $\alpha$ and $\beta$ is $\frac{\pi}{6}$ or $\frac{5\pi}{6}$
\end{theorem}
\begin{proof}
    Let $\theta$ be the angle between $\alpha$ and $\beta$.
    \[4\frac{\langle\beta,\alpha\rangle}{\langle\alpha,\alpha\rangle}\frac{\langle\beta,\alpha\rangle}{\langle\beta,\beta\rangle}=4\cos^2\theta\in \mathbb{Z}\]
\end{proof}
\begin{corollary}\label{a+b}
    Suppose $\alpha,\beta$ are roots. \newline
    (i) If the angle between $\alpha$ and $\beta$ is strictly obtuse, then $\alpha+\beta$ is a root.\newline 
    (ii) If the angle between $\alpha$ and $\beta$ is strictly acute, then $\alpha-\beta$ is a root.
\end{corollary}
\begin{proof}
    Discuss case by case.
\end{proof}

Now we discuss the dual of a root system.
\begin{definition}[Co-Root]
    If $(V,R)$ is a root system, then for each root $\alpha\in R$, define the co-root $H_\alpha$ by \[H_\alpha=2\frac{\alpha}{\langle\alpha,\alpha\rangle}\] 
    The set of all co-roots is denoted $R^\vee$ and is called the dual root system to $R$.
\end{definition}
This is actually an inversion with respect to the ball centered at the origin with radius $\sqrt{2}$. 
\begin{theorem}[Duality]
    $(R^\vee)^\vee=R$
\end{theorem}
\begin{proof}
    The dual system can be regarded as an inversion with respect to the ball centered at the origin with radius $\sqrt{2}$,
    and inversion is a kind of involution.
\end{proof}
\begin{theorem}[Dual Root System]
    $R^\vee$ is a root system and its Weyl group is the same as the that of $R$. 
\end{theorem}

Next we construct the base if a root system.
\begin{definition}[Base]
    A subset $\Delta$ of $R$ is called a base for $R$ if the following conditions hold:\newline 
    (i) $\Delta$ is a basis for $V$ as a vector space.\newline 
    (ii) Each root can be expressed as a linear combination of elements of $\Delta$ with integer coefficients and 
    in such a way that the coefficients are either all non-negative or all nonpositive.

    The roots for which the coefficients are non-negative are called positive roots and the others are called negative roots.
    The set of positive roots relative to a fixed base $\Delta$ is denoted $R^+$. The elements of $\Delta$ are called simple positive roots.
\end{definition}
\begin{lemma}
    If $\alpha,\beta$ are distinct elements of a base, then $\langle\alpha,\beta\rangle\le 0$. 
\end{lemma}
\begin{proof}
    Use Corollary \ref{a+b}.
\end{proof}
\begin{lemma}
    If $V$ is a finite-dimensional real vector space and $R$ is a finite subset of $V$ not containing $0$,
    then there exists a hyperplane $M$ that does not contain any element of $R$.
\end{lemma}
\begin{proof}
    The union of finite collection of hyperplanes can not be all of $V$.
\end{proof}
\begin{definition}[Indecomposability]
    
\end{definition}
\begin{theorem}\label{indecomposable base}
    Suppose $(R,V)$ is a root system, $W$ is a hyperplane not containing any element of $R$, and $R^+$ is the set of roots lying on the
    fixed side of $V$. Then the set of indecomposable elements of $R^+$ is a base for $R$.
\end{theorem}

\begin{theorem}
    Given any base $\Delta$ for $R$, there exists a hyperplane $V$ s.t. $\Delta$ arises as in Theorem \ref{indecomposable base}.
\end{theorem}

\begin{theorem}[Base for Dual Root System]\label{Base for Dual Root System}
    If $\Delta$ is a base for $R$, then the set of all co-roots $H_\alpha$, $\alpha\in\Delta$, is a base for the dual root system $R^\vee$.
\end{theorem}

Finally we explore the integral elements.
\begin{definition}[Integral Element]
    An element $v\in V$ is called an integral element if for all $\alpha\in R$, the quantity 
    \[2\frac{\langle\mu,\alpha\rangle}{\langle\alpha,\alpha\rangle}\] 
    is an integer.
\end{definition}
\begin{definition}
    If $\Delta$ is a base for $R$, then an integral element $\mu$ is called dominant integral if 
    \[ 2\frac{\langle\mu,\alpha\rangle}{\langle\alpha,\alpha\rangle}\ge 0,\quad\forall \alpha\in\Delta\] 
    It is called strictly dominant if the inequality is strict.
\end{definition}
\begin{remark}
    An integral element is dominant if and only if it is contained in closed fundamental Weyl chamber,
    and strictly dominant if and only if contained in the open fundamental Weyl chamber.
\end{remark}
\begin{lemma}
    If $v\in V$ has the property that \[2\frac{\langle\mu,\alpha\rangle}{\langle\alpha,\alpha\rangle}\] 
    is an integer for all $\alpha\in \Delta$, then $v$ us an integral element.
\end{lemma}
\begin{proof}
    Use Theorem \ref{Base for Dual Root System}.
\end{proof}

\begin{definition}[Fundamental Weights]
    
\end{definition}


\begin{definition}[Higher and Lower]
    
\end{definition}
\section{Example: Rank 2}





\section{Example: Rank 3}





\section{Additional Properties}




\section{Application: Classical Lie Algebras}



\section{Dynkin Diagrams}
The classification of root systems is given in terms of an object called the Dynkin diagram.

Suppose $\Delta=\{\alpha_1,\cdots,\alpha_r\}$ is a base for a root system $R$.
Then the Dynkin diagram for $R$ is a graph having vertices $v_1,\cdots,v_r$.
Between any two vertices, we place either no edge, one edge, two edge, or three edge corresponding to the four cases in Theorem \ref{4 cases}.


\begin{theorem}
    Every irreducible root system is isomorphic to precisely one root system from the following list:\newline 
    (i) $A_n$, $n\ge 1$ \newline 
    (ii) $B_n$, $n\ge 2$\newline 
    (iii) $C_n$, $n\ge 3$\newline 
    (iv) $D_n$, $n\ge 4$\newline 
    (v) $G_2$, $F_4$, $E_6$, $E_7$, $E_8$
\end{theorem}

\section{The Root Lattice}



\end{document}